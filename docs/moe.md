# Mixture of experts

## Components of Moe

1. Sparse MoE Layers instead of Feed-forward (FFN)

2. Gate network or router - determines which tokens are sent to which expert.


Qwen3 pipeline closely mirrors DeepSeek R1
That means DeepSeek R1 paper is all you need

Question:
- In training what is verifiable rewards ?