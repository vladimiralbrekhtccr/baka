
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>Glossary - Ohayou</title>
      
    
    
      <link rel="stylesheet" href="../../../../../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../../assets/extra.css">
    
    <script>__md_scope=new URL("../../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#glosario" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../../.." title="Ohayou" class="md-header__button md-logo" aria-label="Ohayou" data-md-component="logo">
      
  <img src="../../../../../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Ohayou
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Glossary
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../ohayou/" class="md-tabs__link">
        
  
  
    
  
  Ohayou

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../../vllm/open_ai_vllm_example_a_v_t/" class="md-tabs__link">
          
  
  
    
  
  vLLM

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../../llm/speculative_decoding/" class="md-tabs__link">
          
  
  
    
  
  LLM

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../../vlm/qwen3_vl_4B_object_detection/" class="md-tabs__link">
          
  
  
    
  
  VLM

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../md_format_helpers/" class="md-tabs__link">
        
  
  
    
  
  MD format helpers

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../docker/" class="md-tabs__link">
        
  
  
    
  
  Docker

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../linux/" class="md-tabs__link">
        
  
  
    
  
  Linux

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../moe/" class="md-tabs__link">
        
  
  
    
  
  Mixture of Experts

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../slurm/" class="md-tabs__link">
        
  
  
    
  
  Slurm

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../../japanese-phrases/" class="md-tabs__link">
          
  
  
    
  
  Japanese Phrases

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../hackathon/index.md" class="md-tabs__link">
        
  
  
    
  
  Hack

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../../.." title="Ohayou" class="md-nav__button md-logo" aria-label="Ohayou" data-md-component="logo">
      
  <img src="../../../../../../assets/logo.png" alt="logo">

    </a>
    Ohayou
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../ohayou/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Ohayou
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    vLLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            vLLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../vllm/open_ai_vllm_example_a_v_t/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Single Request
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../vllm/bash_vllm_serve/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Bash online serve
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../vllm/benchmarks/performance_eval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Benchmarks
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    LLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            LLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../llm/speculative_decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speculative Decoding
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    VLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            VLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../vlm/qwen3_vl_4B_object_detection/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Qwen3VL
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../md_format_helpers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MD format helpers
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Docker
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../linux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linux
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../moe/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mixture of Experts
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../slurm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Slurm
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../../japanese-phrases/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Japanese Phrases
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11" id="__nav_11_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            Japanese Phrases
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../../japanese-phrases/daily-life/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Daily Life
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11_2" id="__nav_11_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_11_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11_2">
            <span class="md-nav__icon md-icon"></span>
            Daily Life
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../japanese-phrases/daily-life/shopping/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shopping
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../../japanese-phrases/greetings/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Greetings
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11_3" id="__nav_11_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_11_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11_3">
            <span class="md-nav__icon md-icon"></span>
            Greetings
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../japanese-phrases/greetings/casual/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Casual
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../japanese-phrases/emotions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Emotions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../japanese-phrases/anime-manga/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Anime/Manga
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../hackathon/index.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hack
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#a" class="md-nav__link">
    <span class="md-ellipsis">
      A
    </span>
  </a>
  
    <nav class="md-nav" aria-label="A">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#attention-mask" class="md-nav__link">
    <span class="md-ellipsis">
      attention mask
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#autoencoding-models" class="md-nav__link">
    <span class="md-ellipsis">
      autoencoding models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#autoregressive-models" class="md-nav__link">
    <span class="md-ellipsis">
      autoregressive models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#b" class="md-nav__link">
    <span class="md-ellipsis">
      B
    </span>
  </a>
  
    <nav class="md-nav" aria-label="B">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#backbone" class="md-nav__link">
    <span class="md-ellipsis">
      backbone
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#c" class="md-nav__link">
    <span class="md-ellipsis">
      C
    </span>
  </a>
  
    <nav class="md-nav" aria-label="C">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#causal-language-modeling" class="md-nav__link">
    <span class="md-ellipsis">
      causal language modeling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#channel" class="md-nav__link">
    <span class="md-ellipsis">
      channel
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#connectionist-temporal-classification-ctc" class="md-nav__link">
    <span class="md-ellipsis">
      connectionist temporal classification (CTC)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#convolution" class="md-nav__link">
    <span class="md-ellipsis">
      convolution
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#d" class="md-nav__link">
    <span class="md-ellipsis">
      D
    </span>
  </a>
  
    <nav class="md-nav" aria-label="D">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#dataparallel-dp" class="md-nav__link">
    <span class="md-ellipsis">
      DataParallel (DP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decoder-input-ids" class="md-nav__link">
    <span class="md-ellipsis">
      decoder input IDs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#decoder-models" class="md-nav__link">
    <span class="md-ellipsis">
      decoder models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deep-learning-dl" class="md-nav__link">
    <span class="md-ellipsis">
      deep learning (DL)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#e" class="md-nav__link">
    <span class="md-ellipsis">
      E
    </span>
  </a>
  
    <nav class="md-nav" aria-label="E">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#encoder-models" class="md-nav__link">
    <span class="md-ellipsis">
      encoder models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#f" class="md-nav__link">
    <span class="md-ellipsis">
      F
    </span>
  </a>
  
    <nav class="md-nav" aria-label="F">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#feature-extraction" class="md-nav__link">
    <span class="md-ellipsis">
      feature extraction
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#feed-forward-chunking" class="md-nav__link">
    <span class="md-ellipsis">
      feed forward chunking
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#finetuned-models" class="md-nav__link">
    <span class="md-ellipsis">
      finetuned models
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#h" class="md-nav__link">
    <span class="md-ellipsis">
      H
    </span>
  </a>
  
    <nav class="md-nav" aria-label="H">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#head" class="md-nav__link">
    <span class="md-ellipsis">
      head
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#i" class="md-nav__link">
    <span class="md-ellipsis">
      I
    </span>
  </a>
  
    <nav class="md-nav" aria-label="I">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#image-patch" class="md-nav__link">
    <span class="md-ellipsis">
      image patch
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inference" class="md-nav__link">
    <span class="md-ellipsis">
      inference
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#input-ids" class="md-nav__link">
    <span class="md-ellipsis">
      input IDs
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#l" class="md-nav__link">
    <span class="md-ellipsis">
      L
    </span>
  </a>
  
    <nav class="md-nav" aria-label="L">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#labels" class="md-nav__link">
    <span class="md-ellipsis">
      labels
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#large-language-models-llm" class="md-nav__link">
    <span class="md-ellipsis">
      large language models (LLM)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#m" class="md-nav__link">
    <span class="md-ellipsis">
      M
    </span>
  </a>
  
    <nav class="md-nav" aria-label="M">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#masked-language-modeling-mlm" class="md-nav__link">
    <span class="md-ellipsis">
      masked language modeling (MLM)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multimodal" class="md-nav__link">
    <span class="md-ellipsis">
      multimodal
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#n" class="md-nav__link">
    <span class="md-ellipsis">
      N
    </span>
  </a>
  
    <nav class="md-nav" aria-label="N">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#natural-language-generation-nlg" class="md-nav__link">
    <span class="md-ellipsis">
      Natural language generation (NLG)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#natural-language-processing-nlp" class="md-nav__link">
    <span class="md-ellipsis">
      Natural language processing (NLP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#natural-language-understanding-nlu" class="md-nav__link">
    <span class="md-ellipsis">
      Natural language understanding (NLU)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#p" class="md-nav__link">
    <span class="md-ellipsis">
      P
    </span>
  </a>
  
    <nav class="md-nav" aria-label="P">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pipeline" class="md-nav__link">
    <span class="md-ellipsis">
      Pipeline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pipelineparallel-pp" class="md-nav__link">
    <span class="md-ellipsis">
      PipelineParallel (PP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pixel-values" class="md-nav__link">
    <span class="md-ellipsis">
      pixel values
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pooling" class="md-nav__link">
    <span class="md-ellipsis">
      pooling
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#position-ids" class="md-nav__link">
    <span class="md-ellipsis">
      position IDs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#preprocessing" class="md-nav__link">
    <span class="md-ellipsis">
      preprocessing
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pretrained-model" class="md-nav__link">
    <span class="md-ellipsis">
      pretrained model
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#r" class="md-nav__link">
    <span class="md-ellipsis">
      R
    </span>
  </a>
  
    <nav class="md-nav" aria-label="R">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#recurrent-neural-network-rnn" class="md-nav__link">
    <span class="md-ellipsis">
      recurrent neural network (RNN)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#representation-learning" class="md-nav__link">
    <span class="md-ellipsis">
      representation learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#s" class="md-nav__link">
    <span class="md-ellipsis">
      S
    </span>
  </a>
  
    <nav class="md-nav" aria-label="S">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sampling-rate" class="md-nav__link">
    <span class="md-ellipsis">
      sampling rate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-attention" class="md-nav__link">
    <span class="md-ellipsis">
      self-attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#self-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      self-supervised learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#semi-supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      semi-supervised learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sequence-to-sequence-seq2seq" class="md-nav__link">
    <span class="md-ellipsis">
      sequence-to-sequence (seq2seq)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sharded-ddp" class="md-nav__link">
    <span class="md-ellipsis">
      Sharded DDP
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stride" class="md-nav__link">
    <span class="md-ellipsis">
      stride
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#supervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      supervised learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#t" class="md-nav__link">
    <span class="md-ellipsis">
      T
    </span>
  </a>
  
    <nav class="md-nav" aria-label="T">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensor-parallelism-tp" class="md-nav__link">
    <span class="md-ellipsis">
      Tensor Parallelism (TP)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#token" class="md-nav__link">
    <span class="md-ellipsis">
      token
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#token-type-ids" class="md-nav__link">
    <span class="md-ellipsis">
      token Type IDs
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transfer-learning" class="md-nav__link">
    <span class="md-ellipsis">
      transfer learning
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#transformer" class="md-nav__link">
    <span class="md-ellipsis">
      transformer
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#u" class="md-nav__link">
    <span class="md-ellipsis">
      U
    </span>
  </a>
  
    <nav class="md-nav" aria-label="U">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#unsupervised-learning" class="md-nav__link">
    <span class="md-ellipsis">
      unsupervised learning
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#z" class="md-nav__link">
    <span class="md-ellipsis">
      Z
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Z">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#zero-redundancy-optimizer-zero" class="md-nav__link">
    <span class="md-ellipsis">
      Zero Redundancy Optimizer (ZeRO)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<!--Copyright 2020 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

<h1 id="glosario">Glosario</h1>
<p>Este glosario define t√©rminos generales de aprendizaje autom√°tico y t√©rminos relacionados con ü§ó Transformers para ayudarte a comprender mejor la documentaci√≥n.</p>
<h2 id="a">A</h2>
<h3 id="attention-mask">attention mask</h3>
<p>La m√°scara de atenci√≥n es un argumento opcional utilizado al agrupar secuencias.</p>
<p><Youtube id="M6adb1j2jPI"/></p>
<p>Este argumento indica al modelo qu√© tokens deben recibir atenci√≥n y cu√°les no.</p>
<p>Por ejemplo, considera estas dos secuencias:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BertTokenizer</span>
</span><span id="__span-0-2"><a id="__codelineno-0-2" name="__codelineno-0-2" href="#__codelineno-0-2"></a>
</span><span id="__span-0-3"><a id="__codelineno-0-3" name="__codelineno-0-3" href="#__codelineno-0-3"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-cased&quot;</span><span class="p">)</span>
</span><span id="__span-0-4"><a id="__codelineno-0-4" name="__codelineno-0-4" href="#__codelineno-0-4"></a>
</span><span id="__span-0-5"><a id="__codelineno-0-5" name="__codelineno-0-5" href="#__codelineno-0-5"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">sequence_a</span> <span class="o">=</span> <span class="s2">&quot;This is a short sequence.&quot;</span>
</span><span id="__span-0-6"><a id="__codelineno-0-6" name="__codelineno-0-6" href="#__codelineno-0-6"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">sequence_b</span> <span class="o">=</span> <span class="s2">&quot;This is a rather long sequence. It is at least longer than the sequence A.&quot;</span>
</span><span id="__span-0-7"><a id="__codelineno-0-7" name="__codelineno-0-7" href="#__codelineno-0-7"></a>
</span><span id="__span-0-8"><a id="__codelineno-0-8" name="__codelineno-0-8" href="#__codelineno-0-8"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">encoded_sequence_a</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sequence_a</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
</span><span id="__span-0-9"><a id="__codelineno-0-9" name="__codelineno-0-9" href="#__codelineno-0-9"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">encoded_sequence_b</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sequence_b</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
</span></code></pre></div>
<p>Las versiones codificadas tienen longitudes diferentes:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_sequence_a</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">encoded_sequence_b</span><span class="p">)</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a><span class="p">(</span><span class="mi">8</span><span class="p">,</span> <span class="mi">19</span><span class="p">)</span>
</span></code></pre></div>
<p>Por lo tanto, no podemos colocarlas juntas en el mismo tensor tal cual. La primera secuencia necesita ser rellenada hasta la longitud de la segunda, o la segunda necesita ser truncada hasta la longitud de la primera.</p>
<p>En el primer caso, la lista de IDs se extender√° con los √≠ndices de relleno. Podemos pasar una lista al tokenizador y pedirle que realice el relleno de esta manera:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">padded_sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">([</span><span class="n">sequence_a</span><span class="p">,</span> <span class="n">sequence_b</span><span class="p">],</span> <span class="n">padding</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></code></pre></div>
<p>Podemos ver que se han agregado ceros a la derecha de la primera oraci√≥n para que tenga la misma longitud que la segunda:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">padded_sequences</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a><span class="p">[[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">1188</span><span class="p">,</span> <span class="mi">1110</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mi">1603</span><span class="p">,</span> <span class="mi">4954</span><span class="p">,</span> <span class="mi">119</span><span class="p">,</span> <span class="mi">102</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">1188</span><span class="p">,</span> <span class="mi">1110</span><span class="p">,</span> <span class="mi">170</span><span class="p">,</span> <span class="mi">1897</span><span class="p">,</span> <span class="mi">1263</span><span class="p">,</span> <span class="mi">4954</span><span class="p">,</span> <span class="mi">119</span><span class="p">,</span> <span class="mi">1135</span><span class="p">,</span> <span class="mi">1110</span><span class="p">,</span> <span class="mi">1120</span><span class="p">,</span> <span class="mi">1655</span><span class="p">,</span> <span class="mi">2039</span><span class="p">,</span> <span class="mi">1190</span><span class="p">,</span> <span class="mi">1103</span><span class="p">,</span> <span class="mi">4954</span><span class="p">,</span> <span class="mi">138</span><span class="p">,</span> <span class="mi">119</span><span class="p">,</span> <span class="mi">102</span><span class="p">]]</span>
</span></code></pre></div>
<p>Esto luego se puede convertir en un tensor en PyTorch o TensorFlow. La m√°scara de atenci√≥n es un tensor binario que indica la posici√≥n de los √≠ndices de relleno para que el modelo no los tenga en cuenta. Para el [<code>BertTokenizer</code>], <code>1</code> indica un valor al que se debe prestar atenci√≥n, mientras que <code>0</code> indica un valor de relleno. Esta m√°scara de atenci√≥n est√° en el diccionario devuelto por el tokenizador bajo la clave "attention_mask":</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">padded_sequences</span><span class="p">[</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">]</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
</span></code></pre></div>
<h3 id="autoencoding-models">autoencoding models</h3>
<p>Consulta <a href="#encoder-models">modelos de codificaci√≥n</a> y <a href="#masked-language-modeling-mlm">modelado de lenguaje enmascarado</a></p>
<h3 id="autoregressive-models">autoregressive models</h3>
<p>Consulta <a href="#causal-language-modeling">modelado de lenguaje causal</a> y <a href="#decoder-models">modelos de decodificaci√≥n</a></p>
<h2 id="b">B</h2>
<h3 id="backbone">backbone</h3>
<p>La columna vertebral, backbone en ingl√©s, es la red (embeddings y layers) que produce los estados ocultos o caracter√≠sticas crudas. Normalmente, est√° conectado a una <a href="#head">cabecera</a>, que acepta las caracter√≠sticas como entrada para hacer una predicci√≥n. Por ejemplo, [<code>ViTModel</code>] es una columna vertebral sin una cabecera espec√≠fica encima. Otros modelos tambi√©n pueden usar [<code>VitModel</code>] como columna vertebral, como por ejemplo <a href="model_doc/dpt">DPT</a>.</p>
<h2 id="c">C</h2>
<h3 id="causal-language-modeling">causal language modeling</h3>
<p>Una tarea de preentrenamiento donde el modelo lee los textos en orden y tiene que predecir la siguiente palabra. Generalmente, se realiza leyendo toda la oraci√≥n, pero utilizando una m√°scara dentro del modelo para ocultar los tokens futuros en un cierto paso de tiempo.</p>
<h3 id="channel">channel</h3>
<p>Las im√°genes a color est√°n compuestas por alguna combinaci√≥n de valores en tres canales: rojo, verde y azul (RGB), y las im√°genes en escala de grises solo tienen un canal. En ü§ó Transformers, el canal puede ser la primera o √∫ltima dimensi√≥n del tensor de una imagen: [<code>n_channels</code>, <code>height</code>, <code>width</code>] o [<code>height</code>, <code>width</code>, <code>n_channels</code>].</p>
<h3 id="connectionist-temporal-classification-ctc">connectionist temporal classification (CTC)</h3>
<p>Un algoritmo que permite que un modelo aprenda sin saber exactamente c√≥mo est√°n alineadas la entrada y la salida; CTC calcula la distribuci√≥n de todas las salidas posibles para una entrada dada y elige la salida m√°s probable de ella. CTC se utiliza com√∫nmente en tareas de reconocimiento de voz porque el habla no siempre se alinea perfectamente con la transcripci√≥n debido a diversas razones, como las diferentes velocidades de habla de los oradores.</p>
<h3 id="convolution">convolution</h3>
<p>Un tipo de capa en una red neuronal donde la matriz de entrada se multiplica elemento por elemento por una matriz m√°s peque√±a (n√∫cleo o filtro) y los valores se suman en una nueva matriz. Esto se conoce como una operaci√≥n de convoluci√≥n que se repite sobre toda la matriz de entrada. Cada operaci√≥n se aplica a un segmento diferente de la matriz de entrada. Las redes neuronales convolucionales (CNN) se utilizan com√∫nmente en visi√≥n por computadora.</p>
<h2 id="d">D</h2>
<h3 id="dataparallel-dp">DataParallel (DP)</h3>
<p>T√©cnica de paralelismo para entrenamiento en m√∫ltiples GPUs donde se replica la misma configuraci√≥n varias veces, con cada instancia recibiendo una porci√≥n de datos √∫nica. El procesamiento se realiza en paralelo y todas las configuraciones se sincronizan al final de cada paso de entrenamiento.</p>
<p>Obt√©n m√°s informaci√≥n sobre c√≥mo funciona el DataParallel <a href="perf_train_gpu_many#dataparallel-vs-distributeddataparallel">aqu√≠</a>.</p>
<h3 id="decoder-input-ids">decoder input IDs</h3>
<p>Esta entrada es espec√≠fica para modelos codificador-decodificador y contiene los IDs de entrada que se enviar√°n al decodificador. Estas entradas deben usarse para tareas de secuencia a secuencia, como traducci√≥n o resumen, y generalmente se construyen de una manera espec√≠fica para cada modelo.</p>
<p>La mayor√≠a de los modelos codificador-decodificador (BART, T5) crean sus <code>decoder_input_ids</code> por s√≠ mismos a partir de las <code>labels</code>. En tales modelos, pasar las <code>labels</code> es la forma preferida de manejar el entrenamiento.</p>
<p>Consulta la documentaci√≥n de cada modelo para ver c√≥mo manejan estos IDs de entrada para el entrenamiento de secuencia a secuencia.</p>
<h3 id="decoder-models">decoder models</h3>
<p>Tambi√©n conocidos como modelos autorregresivos, los modelos decodificadores involucran una tarea de preentrenamiento (llamada modelado de lenguaje causal) donde el modelo lee los textos en orden y tiene que predecir la siguiente palabra. Generalmente, se realiza leyendo la oraci√≥n completa con una m√°scara para ocultar los tokens futuros en un cierto paso de tiempo.</p>
<p><Youtube id="d_ixlCubqQw"/></p>
<h3 id="deep-learning-dl">deep learning (DL)</h3>
<p>Algoritmos de aprendizaje autom√°tico que utilizan redes neuronales con varias capas.</p>
<h2 id="e">E</h2>
<h3 id="encoder-models">encoder models</h3>
<p>Tambi√©n conocidos como modelos de codificaci√≥n autom√°tica (autoencoding models), los modelos codificadores toman una entrada (como texto o im√°genes) y las transforman en una representaci√≥n num√©rica condensada llamada embedding. A menudo, los modelos codificadores se entrenan previamente utilizando t√©cnicas como el <a href="#masked-language-modeling-mlm">modelado de lenguaje enmascarado</a>, que enmascara partes de la secuencia de entrada y obliga al modelo a crear representaciones m√°s significativas.</p>
<p><Youtube id="H39Z_720T5s"/></p>
<h2 id="f">F</h2>
<h3 id="feature-extraction">feature extraction</h3>
<p>El proceso de seleccionar y transformar datos crudos en un conjunto de caracter√≠sticas m√°s informativas y √∫tiles para algoritmos de aprendizaje autom√°tico. Algunos ejemplos de extracci√≥n de caracter√≠sticas incluyen transformar texto crudo en embeddings de palabras y extraer caracter√≠sticas importantes como bordes o formas de datos de im√°genes/videos.</p>
<h3 id="feed-forward-chunking">feed forward chunking</h3>
<p>En cada bloque de atenci√≥n residual en los transformadores, la capa de autoatenci√≥n suele ir seguida de 2 capas de avance. El tama√±o de embedding intermedio de las capas de avance suele ser mayor que el tama√±o oculto del modelo (por ejemplo, para <code>google-bert/bert-base-uncased</code>).</p>
<p>Para una entrada de tama√±o <code>[batch_size, sequence_length]</code>, la memoria requerida para almacenar los embeddings intermedios de avance <code>[batch_size, sequence_length, config.intermediate_size]</code> puede representar una gran fracci√≥n del uso de memoria. Los autores de <a href="https://huggingface.co/papers/2001.04451">Reformer: The Efficient Transformer</a> observaron que, dado que el c√°lculo es independiente de la dimensi√≥n <code>sequence_length</code>, es matem√°ticamente equivalente calcular los embeddings de salida de ambas capas de avance  <code>[batch_size, config.hidden_size]_0, ..., [batch_size, config.hidden_size]_n</code> individualmente y concatenarlos despu√©s a <code>[batch_size, sequence_length, config.hidden_size]</code> con <code>n = sequence_length</code>, lo que intercambia el aumento del tiempo de c√°lculo por una reducci√≥n en el uso de memoria, pero produce un resultado matem√°ticamente <strong>equivalente</strong>.</p>
<p>Para modelos que utilizan la funci√≥n [<code>apply_chunking_to_forward</code>], el <code>chunk_size</code> define el n√∫mero de embeddings de salida que se calculan en paralelo y, por lo tanto, define el equilibrio entre la complejidad de memoria y tiempo. Si <code>chunk_size</code> se establece en 0, no se realiza ninguna fragmentaci√≥n de avance.</p>
<h3 id="finetuned-models">finetuned models</h3>
<p>El ajuste fino es una forma de transferencia de aprendizaje que implica tomar un modelo entrenado previamente, congelar sus pesos y reemplazar la capa de salida con una nueva <a href="#head">cabecera de modelo</a> reci√©n a√±adida. La cabecera del modelo se entrena en tu conjunto de datos objetivo.</p>
<p>Consulta el tutorial <a href="https://huggingface.co/docs/transformers/training">Ajustar finamente un modelo pre-entrenado</a> para obtener m√°s detalles y aprende c√≥mo ajustar finamente modelos con ü§ó Transformers.</p>
<h2 id="h">H</h2>
<h3 id="head">head</h3>
<p>La cabecera del modelo se refiere a la √∫ltima capa de una red neuronal que acepta los estados ocultos crudos y los proyecta en una dimensi√≥n diferente. Hay una cabecera de modelo diferente para cada tarea. Por ejemplo:</p>
<ul>
<li>[<code>GPT2ForSequenceClassification</code>] es una cabecera de clasificaci√≥n de secuencias, es decir, una capa lineal, encima del modelo base [<code>GPT2Model</code>].</li>
<li>[<code>ViTForImageClassification</code>] es una cabecera de clasificaci√≥n de im√°genes, es decir, una capa lineal encima del estado oculto final del token <code>CLS</code>, encima del modelo base [<code>ViTModel</code>].</li>
<li>[<code>Wav2Vec2ForCTC</code>] es una cabecera de modelado de lenguaje con <a href="#connectionist-temporal-classification-ctc">CTC</a> encima del modelo base [<code>Wav2Vec2Model</code>].</li>
</ul>
<h2 id="i">I</h2>
<h3 id="image-patch">image patch</h3>
<p>Los modelos de Transformers basados en visi√≥n dividen una imagen en parches m√°s peque√±os que se incorporan linealmente y luego se pasan como una secuencia al modelo. Puedes encontrar el <code>patch_size</code> (o resoluci√≥n del modelo) en su configuraci√≥n.</p>
<h3 id="inference">inference</h3>
<p>La inferencia es el proceso de evaluar un modelo en nuevos datos despu√©s de completar el entrenamiento. Consulta el tutorial <a href="https://huggingface.co/docs/transformers/pipeline_tutorial">Pipeline for inference</a> para aprender c√≥mo realizar inferencias con ü§ó Transformers.</p>
<h3 id="input-ids">input IDs</h3>
<p>Los IDs de entrada a menudo son los √∫nicos par√°metros necesarios que se deben pasar al modelo como entrada. Son √≠ndices de tokens, representaciones num√©ricas de tokens que construyen las secuencias que se utilizar√°n como entrada por el modelo.</p>
<p><Youtube id="VFp38yj8h3A"/></p>
<p>Cada tokenizador funciona de manera diferente, pero el mecanismo subyacente sigue siendo el mismo. Aqu√≠ tienes un ejemplo utilizando el tokenizador BERT, que es un tokenizador <a href="https://huggingface.co/papers/1609.08144">WordPiece</a>:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BertTokenizer</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>
</span><span id="__span-5-3"><a id="__codelineno-5-3" name="__codelineno-5-3" href="#__codelineno-5-3"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-cased&quot;</span><span class="p">)</span>
</span><span id="__span-5-4"><a id="__codelineno-5-4" name="__codelineno-5-4" href="#__codelineno-5-4"></a>
</span><span id="__span-5-5"><a id="__codelineno-5-5" name="__codelineno-5-5" href="#__codelineno-5-5"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">sequence</span> <span class="o">=</span> <span class="s2">&quot;A Titan RTX has 24GB of VRAM&quot;</span>
</span></code></pre></div>
<p>El tokenizador se encarga de dividir la secuencia en tokens disponibles en el vocabulario del tokenizador.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenized_sequence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">tokenize</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
</span></code></pre></div>
<p>Los tokens son palabras o sub palabras. Por ejemplo, "VRAM" no estaba en el vocabulario del modelo, as√≠ que se dividi√≥
en "V", "RA" y "M". Para indicar que estos tokens no son palabras separadas sino partes de la misma palabra, se a√±ade un prefijo de doble almohadilla para "RA" y "M":</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">tokenized_sequence</span><span class="p">)</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="p">[</span><span class="s1">&#39;A&#39;</span><span class="p">,</span> <span class="s1">&#39;Titan&#39;</span><span class="p">,</span> <span class="s1">&#39;R&#39;</span><span class="p">,</span> <span class="s1">&#39;##T&#39;</span><span class="p">,</span> <span class="s1">&#39;##X&#39;</span><span class="p">,</span> <span class="s1">&#39;has&#39;</span><span class="p">,</span> <span class="s1">&#39;24&#39;</span><span class="p">,</span> <span class="s1">&#39;##GB&#39;</span><span class="p">,</span> <span class="s1">&#39;of&#39;</span><span class="p">,</span> <span class="s1">&#39;V&#39;</span><span class="p">,</span> <span class="s1">&#39;##RA&#39;</span><span class="p">,</span> <span class="s1">&#39;##M&#39;</span><span class="p">]</span>
</span></code></pre></div>
<p>Estos tokens luego se pueden convertir en IDs que son comprensibles por el modelo. Esto se puede hacer alimentando directamente la oraci√≥n al tokenizador, que aprovecha la implementaci√≥n en Rust de <a href="https://github.com/huggingface/tokenizers">ü§ó Tokenizers</a> para obtener un rendimiento √≥ptimo.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>
</span></code></pre></div>
<p>El tokenizador devuelve un diccionario con todos los argumentos necesarios para que su modelo correspondiente funcione correctamente. Los √≠ndices de los tokens est√°n bajo la clave <code>input_ids</code>:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">encoded_sequence</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span>
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">encoded_sequence</span><span class="p">)</span>
</span><span id="__span-9-3"><a id="__codelineno-9-3" name="__codelineno-9-3" href="#__codelineno-9-3"></a><span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">138</span><span class="p">,</span> <span class="mi">18696</span><span class="p">,</span> <span class="mi">155</span><span class="p">,</span> <span class="mi">1942</span><span class="p">,</span> <span class="mi">3190</span><span class="p">,</span> <span class="mi">1144</span><span class="p">,</span> <span class="mi">1572</span><span class="p">,</span> <span class="mi">13745</span><span class="p">,</span> <span class="mi">1104</span><span class="p">,</span> <span class="mi">159</span><span class="p">,</span> <span class="mi">9664</span><span class="p">,</span> <span class="mi">2107</span><span class="p">,</span> <span class="mi">102</span><span class="p">]</span>
</span></code></pre></div>
<p>Ten en cuenta que el tokenizador a√±ade autom√°ticamente "tokens especiales" (si el modelo asociado depende de ellos), que son IDs especiales que el modelo utiliza en ocasiones.</p>
<p>Si descodificamos la secuencia anterior de IDs,</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">decoded_sequence</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_sequence</span><span class="p">)</span>
</span></code></pre></div>
<p>Veremos</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">decoded_sequence</span><span class="p">)</span>
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a><span class="p">[</span><span class="n">CLS</span><span class="p">]</span> <span class="n">A</span> <span class="n">Titan</span> <span class="n">RTX</span> <span class="n">has</span> <span class="mi">24</span><span class="n">GB</span> <span class="n">of</span> <span class="n">VRAM</span> <span class="p">[</span><span class="n">SEP</span><span class="p">]</span>
</span></code></pre></div>
<p>Porque esta es la forma en que un [<code>BertModel</code>] espera sus entradas.</p>
<h2 id="l">L</h2>
<h3 id="labels">labels</h3>
<p>Las etiquetas son un argumento opcional que se puede pasar para que el modelo calcule la p√©rdida por s√≠ mismo. Estas etiquetas deber√≠an ser la predicci√≥n esperada del modelo: usar√° la p√©rdida est√°ndar para calcular la p√©rdida entre sus
predicciones y el valor esperado (la etiqueta).</p>
<p>Estas etiquetas son diferentes seg√∫n la cabecera del modelo, por ejemplo:</p>
<ul>
<li>Para modelos de clasificaci√≥n de secuencias ([<code>BertForSequenceClassification</code>]), el modelo espera un tensor de dimensi√≥n
  <code>(batch_size)</code> con cada valor del lote correspondiente a la etiqueta esperada de toda la secuencia.</li>
<li>Para modelos de clasificaci√≥n de tokens ([<code>BertForTokenClassification</code>]), el modelo espera un tensor de dimensi√≥n
  <code>(batch_size, seq_length)</code> con cada valor correspondiente a la etiqueta esperada de cada token individual.</li>
<li>Para el modelado de lenguaje enmascarado ([<code>BertForMaskedLM</code>]), el modelo espera un tensor de dimensi√≥n <code>(batch_size, seq_length)</code> con cada valor correspondiente a la etiqueta esperada de cada token individual: las etiquetas son el ID del token enmascarado y los valores deben ignorarse para el resto (generalmente -100).</li>
<li>Para tareas de secuencia a secuencia ([<code>BartForConditionalGeneration</code>], [<code>MBartForConditionalGeneration</code>]), el modelo
  espera un tensor de dimensi√≥n <code>(batch_size, tgt_seq_length)</code> con cada valor correspondiente a las secuencias objetivo asociadas con cada secuencia de entrada. Durante el entrenamiento, tanto BART como T5 generar√°n internamente los <code>decoder_input_ids</code> y las m√°scaras de atenci√≥n del decodificador. Por lo general, no es necesario suministrarlos. Esto no se aplica a los modelos que aprovechan el marco codificador-decodificador.</li>
<li>Para modelos de clasificaci√≥n de im√°genes ([<code>ViTForImageClassification</code>]), el modelo espera un tensor de dimensi√≥n
  <code>(batch_size)</code> con cada valor del lote correspondiente a la etiqueta esperada de cada imagen individual.</li>
<li>Para modelos de segmentaci√≥n sem√°ntica ([<code>SegformerForSemanticSegmentation</code>]), el modelo espera un tensor de dimensi√≥n
  <code>(batch_size, height, width)</code> con cada valor del lote correspondiente a la etiqueta esperada de cada p√≠xel individual.</li>
<li>Para modelos de detecci√≥n de objetos ([<code>DetrForObjectDetection</code>]), el modelo espera una lista de diccionarios con claves <code>class_labels</code> y <code>boxes</code> donde cada valor del lote corresponde a la etiqueta esperada y el n√∫mero de cajas delimitadoras de cada imagen individual.</li>
<li>Para modelos de reconocimiento autom√°tico de voz ([<code>Wav2Vec2ForCTC</code>]), el modelo espera un tensor de dimensi√≥n <code>(batch_size, target_length)</code> con cada valor correspondiente a la etiqueta esperada de cada token individual.</li>
</ul>
<p><Tip></p>
<p>Las etiquetas de cada modelo pueden ser diferentes, as√≠ que aseg√∫rate siempre de revisar la documentaci√≥n de cada modelo para obtener m√°s informaci√≥n sobre sus etiquetas espec√≠ficas.</p>
<p></Tip></p>
<p>Los modelos base ([<code>BertModel</code>]) no aceptan etiquetas, ya que estos son los modelos base de transformadores, que simplemente generan caracter√≠sticas.</p>
<h3 id="large-language-models-llm">large language models (LLM)</h3>
<p>Un t√©rmino gen√©rico que se refiere a modelos de lenguaje de transformadores (GPT-3, BLOOM, OPT) que fueron entrenados con una gran cantidad de datos. Estos modelos tambi√©n tienden a tener un gran n√∫mero de par√°metros que se pueden aprender (por ejemplo, 175 mil millones para GPT-3).</p>
<h2 id="m">M</h2>
<h3 id="masked-language-modeling-mlm">masked language modeling (MLM)</h3>
<p>Una tarea de preentrenamiento en la que el modelo ve una versi√≥n corrupta de los textos, generalmente hecha
al enmascarar algunos tokens al azar, y tiene que predecir el texto original.</p>
<h3 id="multimodal">multimodal</h3>
<p>Una tarea que combina textos con otro tipo de entradas (por ejemplo: im√°genes).</p>
<h2 id="n">N</h2>
<h3 id="natural-language-generation-nlg">Natural language generation (NLG)</h3>
<p>Todas las tareas relacionadas con la generaci√≥n de texto (por ejemplo: <a href="https://transformer.huggingface.co/">Escribe con Transformers</a> o traducci√≥n).</p>
<h3 id="natural-language-processing-nlp">Natural language processing (NLP)</h3>
<p>Una forma gen√©rica de decir "trabajar con textos".</p>
<h3 id="natural-language-understanding-nlu">Natural language understanding (NLU)</h3>
<p>Todas las tareas relacionadas con entender lo que hay en un texto (por ejemplo: clasificar el
texto completo o palabras individuales).</p>
<h2 id="p">P</h2>
<h3 id="pipeline">Pipeline</h3>
<p>Un pipeline en ü§ó Transformers es una abstracci√≥n que se refiere a una serie de pasos que se ejecutan en un orden espec√≠fico para preprocesar y transformar datos y devolver una predicci√≥n de un modelo. Algunas etapas de ejemplo que se encuentran en un pipeline pueden ser el preprocesamiento de datos, la extracci√≥n de caracter√≠sticas y la normalizaci√≥n.</p>
<p>Para obtener m√°s detalles, consulta <a href="https://huggingface.co/docs/transformers/pipeline_tutorial">Pipelines para inferencia</a>.</p>
<h3 id="pipelineparallel-pp">PipelineParallel (PP)</h3>
<p>T√©cnica de paralelismo en la que el modelo se divide verticalmente (a nivel de capa) en varios GPU, de modo que solo una o varias capas del modelo se colocan en un solo GPU. Cada GPU procesa en paralelo diferentes etapas del pipeline y trabaja en un peque√±o fragmento del lote. Obt√©n m√°s informaci√≥n sobre c√≥mo funciona PipelineParallel <a href="perf_train_gpu_many#from-naive-model-parallelism-to-pipeline-parallelism">aqu√≠</a>.</p>
<h3 id="pixel-values">pixel values</h3>
<p>Un tensor de las representaciones num√©ricas de una imagen que se pasa a un modelo. Los valores de p√≠xeles tienen una forma de [<code>batch_size</code>, <code>num_channels</code>, <code>height</code>, <code>width</code>], y se generan a partir de un procesador de im√°genes.</p>
<h3 id="pooling">pooling</h3>
<p>Una operaci√≥n que reduce una matriz a una matriz m√°s peque√±a, ya sea tomando el m√°ximo o el promedio de la dimensi√≥n (o dimensiones) agrupada(s). Las capas de agrupaci√≥n se encuentran com√∫nmente entre capas convolucionales para reducir la representaci√≥n de caracter√≠sticas.</p>
<h3 id="position-ids">position IDs</h3>
<p>A diferencia de las RNN que tienen la posici√≥n de cada token incrustada en ellas, los transformers no son conscientes de la posici√≥n de cada token. Por lo tanto, se utilizan los IDs de posici√≥n (<code>position_ids</code>) para que el modelo identifique la posici√≥n de cada token en la lista de tokens.</p>
<p>Son un par√°metro opcional. Si no se pasan <code>position_ids</code> al modelo, los IDs se crean autom√°ticamente como embeddings de posici√≥n absolutas.</p>
<p>Los embeddings de posici√≥n absolutas se seleccionan en el rango <code>[0, config.max_position_embeddings - 1]</code>. Algunos modelos utilizan otros tipos de embeddings de posici√≥n, como embeddings de posici√≥n sinusoidales o embeddings de posici√≥n relativas.</p>
<h3 id="preprocessing">preprocessing</h3>
<p>La tarea de preparar datos crudos en un formato que pueda ser f√°cilmente consumido por modelos de aprendizaje autom√°tico. Por ejemplo, el texto se preprocesa t√≠picamente mediante la tokenizaci√≥n. Para tener una mejor idea de c√≥mo es el preprocesamiento para otros tipos de entrada, consulta el tutorial <a href="https://huggingface.co/docs/transformers/preprocessing">Pre-procesar</a>.</p>
<h3 id="pretrained-model">pretrained model</h3>
<p>Un modelo que ha sido pre-entrenado en algunos datos (por ejemplo, toda Wikipedia). Los m√©todos de preentrenamiento involucran un objetivo auto-supervisado, que puede ser leer el texto e intentar predecir la siguiente palabra (ver <a href="#causal-language-modeling">modelado de lenguaje causal</a>) o enmascarar algunas palabras e intentar predecirlas (ver <a href="#masked-language-modeling-mlm">modelado de lenguaje enmascarado</a>).</p>
<p>Los modelos de habla y visi√≥n tienen sus propios objetivos de pre-entrenamiento. Por ejemplo, Wav2Vec2 es un modelo de habla pre-entrenado en una tarea contrastiva que requiere que el modelo identifique la representaci√≥n de habla "verdadera" de un conjunto de representaciones de habla "falsas". Por otro lado, BEiT es un modelo de visi√≥n pre-entrenado en una tarea de modelado de im√°genes enmascaradas que enmascara algunos de los parches de la imagen y requiere que el modelo prediga los parches enmascarados (similar al objetivo de modelado de lenguaje enmascarado).</p>
<h2 id="r">R</h2>
<h3 id="recurrent-neural-network-rnn">recurrent neural network (RNN)</h3>
<p>Un tipo de modelo que utiliza un bucle sobre una capa para procesar textos.</p>
<h3 id="representation-learning">representation learning</h3>
<p>Un subcampo del aprendizaje autom√°tico que se centra en aprender representaciones significativas de datos en bruto. Algunos ejemplos de t√©cnicas de aprendizaje de representaciones incluyen embeddings de palabras, auto-encoders y Redes Generativas Adversarias (Generative Adversarial Networks, GANs).</p>
<h2 id="s">S</h2>
<h3 id="sampling-rate">sampling rate</h3>
<p>Una medida en hercios del n√∫mero de muestras (la se√±al de audio) tomadas por segundo. La tasa de muestreo es el resultado de aproximar una se√±al continua como el habla.</p>
<h3 id="self-attention">self-attention</h3>
<p>Cada elemento de la entrada averigua a cu√°les otros elementos de la entrada debe prestar atenci√≥n.</p>
<h3 id="self-supervised-learning">self-supervised learning</h3>
<p>Una categor√≠a de t√©cnicas de aprendizaje autom√°tico en la que un modelo crea su propio objetivo de aprendizaje a partir de datos no etiquetados. Difiere del <a href="#unsupervised-learning">aprendizaje no supervisado</a> y del <a href="#supervised-learning">aprendizaje supervisado</a> en que el proceso de aprendizaje est√° supervisado, pero no expl√≠citamente por el usuario.</p>
<p>Un ejemplo de aprendizaje auto-supervisado es el <a href="#masked-language-modeling-mlm">modelado de lenguaje enmascarado</a>, donde un modelo recibe oraciones con una proporci√≥n de sus tokens eliminados y aprende a predecir los tokens faltantes.</p>
<h3 id="semi-supervised-learning">semi-supervised learning</h3>
<p>Una amplia categor√≠a de t√©cnicas de entrenamiento de aprendizaje autom√°tico que aprovecha una peque√±a cantidad de datos etiquetados con una mayor cantidad de datos no etiquetados para mejorar la precisi√≥n de un modelo, a diferencia del <a href="#supervised-learning">aprendizaje supervisado</a> y del <a href="#unsupervised-learning">aprendizaje no supervisado</a>.</p>
<p>Un ejemplo de un enfoque de aprendizaje semi-supervisado es "auto-entrenamiento", en el que un modelo se entrena con datos etiquetados y luego se utiliza para hacer predicciones sobre los datos no etiquetados. La porci√≥n de datos no etiquetados que el modelo predice con mayor confianza se agrega al conjunto de datos etiquetados y se utiliza para volver a entrenar el modelo.</p>
<h3 id="sequence-to-sequence-seq2seq">sequence-to-sequence (seq2seq)</h3>
<p>Modelos que generan una nueva secuencia a partir de una entrada, como modelos de traducci√≥n o modelos de resumen (como
<a href="model_doc/bart">Bart</a> o <a href="model_doc/t5">T5</a>).</p>
<h3 id="sharded-ddp">Sharded DDP</h3>
<p>Otro nombre para el concepto fundamental de <a href="#zero-redundancy-optimizer-zero">ZeRO</a> utilizado por varias otras implementaciones de ZeRO.</p>
<h3 id="stride">stride</h3>
<p>En <a href="#convolution">convoluci√≥n</a> o <a href="#pooling">agrupaci√≥n</a>, el paso (stride) se refiere a la distancia que recorre el n√∫cleo sobre una matriz. Un paso de 1 significa que el n√∫cleo se mueve un p√≠xel a la vez, y un paso de 2 significa que el n√∫cleo se mueve dos p√≠xeles a la vez.</p>
<h3 id="supervised-learning">supervised learning</h3>
<p>Una forma de entrenamiento de modelos que utiliza directamente datos etiquetados para corregir y dirigir el rendimiento del modelo. Los datos se introducen en el modelo en entrenamiento, y sus predicciones se comparan con las etiquetas conocidas. El modelo actualiza sus pesos en funci√≥n de cu√°n incorrectas fueron sus predicciones, y el proceso se repite para optimizar el rendimiento del modelo.</p>
<h2 id="t">T</h2>
<h3 id="tensor-parallelism-tp">Tensor Parallelism (TP)</h3>
<p>T√©cnica de paralelismo para entrenamiento en m√∫ltiples GPU en la que cada tensor se divide en m√∫ltiples fragmentos, de modo que en lugar de tener todo el tensor en una sola GPU, cada fragmento del tensor reside en su GPU designada. Los fragmentos se procesan por separado y en paralelo en diferentes GPU y los resultados se sincronizan al final del paso de procesamiento.Esto es lo que a veces se llama paralelismo horizontal, ya que la divisi√≥n ocurre a nivel horizontal.
Obt√©n m√°s informaci√≥n sobre el Paralelismo de Tensores <a href="perf_train_gpu_many#tensor-parallelism">aqu√≠</a>.</p>
<h3 id="token">token</h3>
<p>Parte de una oraci√≥n, generalmente una palabra, pero tambi√©n puede ser una sub-palabra (las palabras no comunes a menudo se dividen en sub-palabras) o un s√≠mbolo de puntuaci√≥n.</p>
<h3 id="token-type-ids">token Type IDs</h3>
<p>Algunos modelos tienen como objetivo realizar clasificaci√≥n en pares de oraciones o responder preguntas.</p>
<p><Youtube id="0u3ioSwev3s"/></p>
<p>Estos requieren que dos secuencias diferentes se unan en una √∫nica entrada "input_ids", lo cual generalmente se realiza con
la ayuda de tokens especiales, como el token de clasificaci√≥n (<code>[CLS]</code>) y el token separador (<code>[SEP]</code>). Por ejemplo, el modelo BERT construye sus dos secuencias de entrada de la siguiente manera:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="c1"># [CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]</span>
</span></code></pre></div>
<p>Podemos utilizar nuestro tokenizador para generar autom√°ticamente una oraci√≥n de este tipo al pasar las dos secuencias a <code>tokenizer</code> como dos argumentos (y no como una lista, como antes) de la siguiente manera:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BertTokenizer</span>
</span><span id="__span-13-2"><a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>
</span><span id="__span-13-3"><a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google-bert/bert-base-cased&quot;</span><span class="p">)</span>
</span><span id="__span-13-4"><a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">sequence_a</span> <span class="o">=</span> <span class="s2">&quot;HuggingFace is based in NYC&quot;</span>
</span><span id="__span-13-5"><a id="__codelineno-13-5" name="__codelineno-13-5" href="#__codelineno-13-5"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">sequence_b</span> <span class="o">=</span> <span class="s2">&quot;Where is HuggingFace based?&quot;</span>
</span><span id="__span-13-6"><a id="__codelineno-13-6" name="__codelineno-13-6" href="#__codelineno-13-6"></a>
</span><span id="__span-13-7"><a id="__codelineno-13-7" name="__codelineno-13-7" href="#__codelineno-13-7"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">encoded_dict</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sequence_a</span><span class="p">,</span> <span class="n">sequence_b</span><span class="p">)</span>
</span><span id="__span-13-8"><a id="__codelineno-13-8" name="__codelineno-13-8" href="#__codelineno-13-8"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">decoded</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">encoded_dict</span><span class="p">[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">])</span>
</span></code></pre></div>
<p>Que devolver√°:</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="nb">print</span><span class="p">(</span><span class="n">decoded</span><span class="p">)</span>
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a><span class="p">[</span><span class="n">CLS</span><span class="p">]</span> <span class="n">HuggingFace</span> <span class="ow">is</span> <span class="n">based</span> <span class="ow">in</span> <span class="n">NYC</span> <span class="p">[</span><span class="n">SEP</span><span class="p">]</span> <span class="n">Where</span> <span class="ow">is</span> <span class="n">HuggingFace</span> <span class="n">based</span><span class="err">?</span> <span class="p">[</span><span class="n">SEP</span><span class="p">]</span>
</span></code></pre></div>
<p>Esto es suficiente para que algunos modelos comprendan d√≥nde termina una secuencia y comienza otra. Sin embargo, otros modelos, como BERT, tambi√©n utilizan identificadores de tipo de token (tambi√©n llamados identificadores de segmento). Se representan como una m√°scara binaria que identifica los dos tipos de secuencia en el modelo.</p>
<p>El tokenizador devuelve esta m√°scara como la entrada "token_type_ids":</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">encoded_dict</span><span class="p">[</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">]</span>
</span><span id="__span-15-2"><a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
</span></code></pre></div>
<p>La primera secuencia, el "contexto" utilizado para la pregunta, tiene todos sus tokens representados por un <code>0</code>, mientras que la segunda secuencia, correspondiente a la "pregunta", tiene todos sus tokens representados por un <code>1</code>.</p>
<p>Algunos modelos, como [<code>XLNetModel</code>], utilizan un token adicional representado por un <code>2</code>.</p>
<h3 id="transfer-learning">transfer learning</h3>
<p>Una t√©cnica que implica tomar un modelo pre-entrenado y adaptarlo a un conjunto de datos espec√≠fico para tu tarea. En lugar de entrenar un modelo desde cero, puedes aprovechar el conocimiento obtenido de un modelo existente como punto de partida. Esto acelera el proceso de aprendizaje y reduce la cantidad de datos de entrenamiento necesarios.</p>
<h3 id="transformer">transformer</h3>
<p>Arquitectura de modelo de aprendizaje profundo basada en auto-atenci√≥n (Self-attention).</p>
<h2 id="u">U</h2>
<h3 id="unsupervised-learning">unsupervised learning</h3>
<p>Una forma de entrenamiento de modelos en la que los datos proporcionados al modelo no est√°n etiquetados. Las t√©cnicas de aprendizaje no supervisado aprovechan la informaci√≥n estad√≠stica de la distribuci√≥n de datos para encontrar patrones √∫tiles para la tarea en cuesti√≥n.</p>
<h2 id="z">Z</h2>
<h3 id="zero-redundancy-optimizer-zero">Zero Redundancy Optimizer (ZeRO)</h3>
<p>T√©cnica de paralelismo que realiza la fragmentaci√≥n de los tensores de manera algo similar a <a href="#tensor-parallelism-tp">TensorParallel</a>, excepto que todo el tensor se reconstruye a tiempo para una computaci√≥n hacia adelante o hacia atr√°s, por lo tanto, el modelo no necesita ser modificado. Este m√©todo tambi√©n admite diversas t√©cnicas de descarga para compensar la memoria limitada de la GPU. Obt√©n m√°s informaci√≥n sobre ZeRO <a href="perf_train_gpu_many#zero-data-parallelism">aqu√≠</a>.</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.instant", "navigation.sections", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow", "content.tabs.link", "content.code.copy"], "search": "../../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>