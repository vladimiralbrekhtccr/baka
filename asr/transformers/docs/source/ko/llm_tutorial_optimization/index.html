
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>Llm tutorial optimization - Ohayou</title>
      
    
    
      <link rel="stylesheet" href="../../../../../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../../assets/extra.css">
    
    <script>__md_scope=new URL("../../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#optimizing-llms-for-speed-and-memory" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../../.." title="Ohayou" class="md-header__button md-logo" aria-label="Ohayou" data-md-component="logo">
      
  <img src="../../../../../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Ohayou
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Llm tutorial optimization
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../ohayou/" class="md-tabs__link">
        
  
  
    
  
  Ohayou

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../../vllm/open_ai_vllm_example_a_v_t/" class="md-tabs__link">
          
  
  
    
  
  vLLM

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../../llm/speculative_decoding/" class="md-tabs__link">
          
  
  
    
  
  LLM

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../../vlm/qwen3_vl_4B_object_detection/" class="md-tabs__link">
          
  
  
    
  
  VLM

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../md_format_helpers/" class="md-tabs__link">
        
  
  
    
  
  MD format helpers

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../docker/" class="md-tabs__link">
        
  
  
    
  
  Docker

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../linux/" class="md-tabs__link">
        
  
  
    
  
  Linux

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../moe/" class="md-tabs__link">
        
  
  
    
  
  Mixture of Experts

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../slurm/" class="md-tabs__link">
        
  
  
    
  
  Slurm

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../../japanese-phrases/" class="md-tabs__link">
          
  
  
    
  
  Japanese Phrases

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../hackathon/index.md" class="md-tabs__link">
        
  
  
    
  
  Hack

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../../.." title="Ohayou" class="md-nav__button md-logo" aria-label="Ohayou" data-md-component="logo">
      
  <img src="../../../../../../assets/logo.png" alt="logo">

    </a>
    Ohayou
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../ohayou/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Ohayou
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    vLLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            vLLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../vllm/open_ai_vllm_example_a_v_t/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Single Request
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../vllm/bash_vllm_serve/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Bash online serve
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../vllm/benchmarks/performance_eval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Benchmarks
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    LLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            LLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../llm/speculative_decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speculative Decoding
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    VLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            VLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../vlm/qwen3_vl_4B_object_detection/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Qwen3VL_adema_grounding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../vlm/qwen3_vla_4B_audio_training_aspandiyar/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Qwen3VLA_aspandiyar_thinking
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../md_format_helpers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MD format helpers
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Docker
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../linux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linux
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../moe/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mixture of Experts
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../slurm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Slurm
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../../japanese-phrases/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Japanese Phrases
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11" id="__nav_11_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            Japanese Phrases
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../../japanese-phrases/daily-life/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Daily Life
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11_2" id="__nav_11_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_11_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11_2">
            <span class="md-nav__icon md-icon"></span>
            Daily Life
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../japanese-phrases/daily-life/shopping/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shopping
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../../japanese-phrases/greetings/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Greetings
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11_3" id="__nav_11_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_11_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11_3">
            <span class="md-nav__icon md-icon"></span>
            Greetings
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../japanese-phrases/greetings/casual/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Casual
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../japanese-phrases/emotions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Emotions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../japanese-phrases/anime-manga/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Anime/Manga
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../hackathon/index.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hack
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#1-1-lower-precision" class="md-nav__link">
    <span class="md-ellipsis">
      1. 낮은 정밀도 [[1-lower-precision]]
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#2-2-flash-attention" class="md-nav__link">
    <span class="md-ellipsis">
      2. 플래시 어텐션 [[2-flash-attention]]
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#3-3-architectural-innovations" class="md-nav__link">
    <span class="md-ellipsis">
      3. 아키텍처 혁신 [[3-architectural-innovations]]
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3. 아키텍처 혁신 [[3-architectural-innovations]]">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#31-31-improving-positional-embeddings-of-llms" class="md-nav__link">
    <span class="md-ellipsis">
      3.1 대규모 언어 모델의 위치 임베딩 개선 [[31-improving-positional-embeddings-of-llms]]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#32-32-the-key-value-cache" class="md-nav__link">
    <span class="md-ellipsis">
      3.2 키-값 캐시 [[32-the-key-value-cache]]
    </span>
  </a>
  
    <nav class="md-nav" aria-label="3.2 키-값 캐시 [[32-the-key-value-cache]]">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#321-321-multi-round-conversation" class="md-nav__link">
    <span class="md-ellipsis">
      3.2.1 멀티 라운드 대화 [[321-multi-round-conversation]]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#322-mqa-322-multi-query-attention-mqa" class="md-nav__link">
    <span class="md-ellipsis">
      3.2.2 멀티 쿼리 어텐션 (MQA) [[322-multi-query-attention-mqa]]
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#323-gqa-323-grouped-query-attention-gqa" class="md-nav__link">
    <span class="md-ellipsis">
      3.2.3 그룹 쿼리 어텐션 (GQA) [[323-grouped-query-attention-gqa]]
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#conclusion" class="md-nav__link">
    <span class="md-ellipsis">
      결론 [[conclusion]]
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<!--Copyright 2023 The HuggingFace Team. All rights reserved.
Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.
⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.
-->
<h1 id="optimizing-llms-for-speed-and-memory">대규모 언어 모델의 속도 및 메모리 최적화 [[optimizing-llms-for-speed-and-memory]]</h1>
<p>[[open-in-colab]]</p>
<p>GPT3/4, <a href="https://huggingface.co/tiiuae/falcon-40b">Falcon</a>, <a href="https://huggingface.co/meta-llama/Llama-2-70b-hf">Llama</a>와 같은 대규모 언어 모델의 인간 중심 과제를 해결하는 능력이 빠르게 발전하고 있으며, 현대 지식 기반 산업에서 필수 도구로 자리잡고 있습니다. 그러나 이러한 모델을 실제 과제에 배포하는 것은 여전히 어려운 과제입니다.</p>
<ul>
<li>인간과 비슷한 텍스트 이해 및 생성 능력을 보이기 위해, 현재 대규모 언어 모델은 수십억 개의 매개변수로 구성되어야 합니다 (참조: <a href="https://huggingface.co/papers/2001.08361">Kaplan et al</a>, <a href="https://huggingface.co/papers/2206.07682">Wei et. al</a>). 이는 추론을 위한 메모리 요구를 크게 증가시킵니다.</li>
<li>많은 실제 과제에서 대규모 언어 모델은 방대한 맥락 정보를 제공받아야 합니다. 이는 모델이 추론 과정에서 매우 긴 입력 시퀀스를 처리할 수 있어야 한다는 것을 뜻합니다.  </li>
</ul>
<p>이러한 과제의 핵심은 대규모 언어 모델의 계산 및 메모리 활용 능력을 증대시키는 데 있습니다. 특히 방대한 입력 시퀀스를 처리할 때 이러한 능력이 중요합니다.</p>
<p>이 가이드에서는 효율적인 대규모 언어 모델 배포를 위한 효과적인 기법들을 살펴보겠습니다. </p>
<ol>
<li>
<p><strong>낮은 정밀도:</strong> 연구에 따르면, <a href="./main_classes/quantization">8비트와 4비트</a>와 같이 낮은 수치 정밀도로 작동하면 모델 성능의 큰 저하 없이 계산상의 이점을 얻을 수 있습니다.</p>
</li>
<li>
<p><strong>플래시 어텐션:</strong> 플래시 어텐션은 메모리 효율성을 높일 뿐만 아니라 최적화된 GPU 메모리 활용을 통해 효율성을 향상시키는 어텐션 알고리즘의 변형입니다.</p>
</li>
<li>
<p><strong>아키텍처 혁신:</strong> 추론 시 대규모 언어 모델은 주로 동일한 방식(긴 입력 맥락을 가진 자기회귀 텍스트 생성 방식)으로 배포되는데, 더 효율적인 추론을 가능하게 하는 특화된 모델 아키텍처가 제안되었습니다. 이러한 모델 아키텍처의 가장 중요한 발전으로는 <a href="https://huggingface.co/papers/2108.12409">Alibi</a>, <a href="https://huggingface.co/papers/2104.09864">Rotary embeddings</a>, <a href="https://huggingface.co/papers/1911.02150">Multi-Query Attention (MQA)</a>, <a href="https://huggingface.co/papers/2305.13245">Grouped-Query-Attention (GQA)</a>이 있습니다. </p>
</li>
</ol>
<p>이 가이드에서는 텐서의 관점에서 자기회귀 생성에 대한 분석을 제공합니다. 낮은 정밀도를 채택하는 것의 장단점을 논의하고, 최신 어텐션 알고리즘을 포괄적으로 탐구하며, 향상된 대규모 언어 모델 아키텍처에 대해 논합니다. 이 과정에서 각 기능의 개선 사항을 보여주는 실용적인 예제를 확인합니다.</p>
<h2 id="1-1-lower-precision">1. 낮은 정밀도 [[1-lower-precision]]</h2>
<p>대규모 언어 모델을 가중치 행렬과 벡터의 집합으로 보고, 텍스트 입력을 벡터의 시퀀스로 본다면, 대규모 언어 모델의 메모리 요구사항을 가장 잘 이해할 수 있습니다. 이어지는 내용에서 <em>가중치</em>는 모델의 모든 가중치 행렬과 벡터를 의미합니다.   </p>
<p>이 가이드를 작성하는 시점의 대규모 언어 모델은 최소 몇십억 개의 매개변수로 구성되어 있습니다. 각 매개변수는 <code>4.5689</code>와 같은 십진수로 이루어져 있으며, 보통 <a href="https://en.wikipedia.org/wiki/Single-precision_floating-point_format">float32</a>, <a href="https://en.wikipedia.org/wiki/Bfloat16_floating-point_format">bfloat16</a> 또는 <a href="https://en.wikipedia.org/wiki/Half-precision_floating-point_format">float16</a> 형식으로 저장됩니다. 이를 통해 대규모 언어 모델을 메모리에 로드하는 데 필요한 메모리의 요구사항을 쉽게 계산할 수 있습니다:</p>
<blockquote>
<p><em>X * 10억 개의 매개변수를 가진 모델의 가중치를 로드하려면 float32 정밀도에서 대략 4 * X GB의 VRAM이 필요합니다.</em></p>
</blockquote>
<p>요즘에는 모델이 float32 정밀도로 훈련되는 경우는 드물고, 일반적으로 bfloat16 정밀도나 가끔 float16 정밀도로 훈련됩니다. 따라서 경험적으로 알아낸 법칙은 다음과 같습니다:</p>
<blockquote>
<p><em>X * 10억 개의 매개변수를 가진 모델의 가중치를 로드하려면 bfloat16/float16 정밀도에서 대략 2 * X GB의 VRAM이 필요합니다.</em></p>
</blockquote>
<p>짧은 텍스트 입력(1024 토큰 미만)의 경우, 추론을 위한 메모리 요구 사항의 대부분은 가중치를 로드하는 데 필요한 메모리 요구 사항입니다. 따라서 지금은 추론을 위한 메모리 요구 사항이 모델의 가중치를 GPU VRAM에 로드하는 데 필요한 메모리 요구 사항과 같다고 가정합시다.</p>
<p>모델을 bfloat16으로 로드하는 데 대략 얼마나 많은 VRAM이 필요한지 몇 가지 예를 들어보겠습니다:</p>
<ul>
<li><strong>GPT3</strong>는 2 * 175 GB = <strong>350 GB</strong> VRAM이 필요합니다.</li>
<li><a href="https://huggingface.co/bigscience/bloom"><strong>Bloom</strong></a>은 2 * 176 GB = <strong>352 GB</strong> VRAM이 필요합니다.</li>
<li><a href="https://huggingface.co/meta-llama/Llama-2-70b-hf"><strong>Llama-2-70b</strong></a>는 2 * 70 GB = <strong>140 GB</strong> VRAM이 필요합니다.</li>
<li><a href="https://huggingface.co/tiiuae/falcon-40b"><strong>Falcon-40b</strong></a>는 2 * 40 GB = <strong>80 GB</strong> VRAM이 필요합니다.</li>
<li><a href="https://huggingface.co/mosaicml/mpt-30b"><strong>MPT-30b</strong></a>는 2 * 30 GB = <strong>60 GB</strong> VRAM이 필요합니다.</li>
<li><a href="https://huggingface.co/bigcode/starcoder"><strong>bigcode/starcoder</strong></a>는 2 * 15.5 GB = <strong>31 GB</strong> VRAM이 필요합니다.</li>
</ul>
<p>이 문서를 작성하는 시점에서, 현재 시장에서 가장 큰 GPU 칩은 80GB의 VRAM을 제공하는 A100과 H100입니다. 앞서 언급된 대부분의 모델들은 로드하기 위해서는 최소 80GB 이상의 용량을 필요로 하며, 따라서 <a href="https://huggingface.co/docs/transformers/perf_train_gpu_many#tensor-parallelism">텐서 병렬 처리</a> 및/또는 <a href="https://huggingface.co/docs/transformers/perf_train_gpu_many#naive-model-parallelism-vertical-and-pipeline-parallelism">파이프라인 병렬 처리</a>를 반드시 필요로 합니다.</p>
<p>🤗 Transformers는 텐서 병렬 처리를 바로 지원하지 않습니다. 이는 모델 아키텍처가 특정 방식으로 작성되어야 하기 때문입니다. 텐서 병렬 처리를 지원하는 방식으로 모델을 작성하는 데 관심이 있다면 <a href="https://github.com/huggingface/text-generation-inference/tree/main/server/text_generation_server/models/custom_modeling">the text-generation-inference library</a>를 참조해 보시기 바랍니다.</p>
<p>기본적인 파이프라인 병렬 처리는 바로 지원됩니다. 이를 위해 단순히 모델을 <code>device="auto"</code>로 로드하면 <a href="https://huggingface.co/docs/accelerate/v0.22.0/en/concept_guides/big_model_inference">여기</a>에 설명된 대로 사용 가능한 GPU에 모델의 서로 다른 레이어를 자동으로 배치합니다. 이것은 매우 효과적이긴 하지만 이러한 기본 파이프라인 병렬 처리는 GPU 유휴 문제를 해결하지 못한다는 점을 유의해야 합니다. 더 발전된 파이프라인 병렬 처리가 필요하며, 이에 대한 설명은 <a href="https://huggingface.co/docs/transformers/en/perf_train_gpu_many#naive-model-parallelism-vertical-and-pipeline-parallelism">여기</a>에서 확인할 수 있습니다.</p>
<p>80GB A100 GPU 8개를 가진 노드에 접근할 수 있다면, BLOOM을 다음과 같이 로드할 수 있습니다.</p>
<p><div class="language-bash highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>!pip<span class="w"> </span>install<span class="w"> </span>transformers<span class="w"> </span>accelerate<span class="w"> </span>bitsandbytes<span class="w"> </span>optimum
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloom&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></code></pre></div></p>
<p><code>device_map="auto"</code>를 사용하면 모든 사용 가능한 GPU에 어텐션 레이어가 고르게 분산됩니다.</p>
<p>이 가이드에서는 <a href="https://huggingface.co/bigcode/octocoder">bigcode/octocoder</a>를 사용할 것입니다. 이 모델은 단일 40GB A100 GPU 장치에서 실행할 수 있습니다. 앞으로 적용할 모든 메모리 및 속도 최적화는 모델 또는 텐서 병렬 처리를 필요로 하는 다른 모델에도 동일하게 적용될 수 있습니다.</p>
<p>모델이 bfloat16 정밀도로 로드되기 때문에, 위의 경험적으로 알아낸 법칙을 사용하면 <code>bigcode/octocoder</code>를 사용하여 추론을 실행하기 위한 메모리 요구 사항이 약 31GB VRAM일 것으로 예상됩니다. 한 번 시도해 보겠습니다.</p>
<p>먼저 모델과 토크나이저를 로드한 다음, 둘 다 Transformers의 <a href="https://huggingface.co/docs/transformers/main_classes/pipelines">파이프라인</a> 객체에 전달합니다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">pipeline</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigcode/octocoder&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-2-5"><a id="__codelineno-2-5" name="__codelineno-2-5" href="#__codelineno-2-5"></a><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigcode/octocoder&quot;</span><span class="p">)</span>
</span><span id="__span-2-6"><a id="__codelineno-2-6" name="__codelineno-2-6" href="#__codelineno-2-6"></a>
</span><span id="__span-2-7"><a id="__codelineno-2-7" name="__codelineno-2-7" href="#__codelineno-2-7"></a><span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="n">prompt</span> <span class="o">=</span> <span class="s2">&quot;Question: Please write a function in Python that transforms bytes to Giga bytes.</span><span class="se">\n\n</span><span class="s2">Answer:&quot;</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="n">result</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">60</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;generated_text&quot;</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">):]</span>
</span><span id="__span-3-4"><a id="__codelineno-3-4" name="__codelineno-3-4" href="#__codelineno-3-4"></a><span class="n">result</span>
</span></code></pre></div>
<p><strong>출력</strong>:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a>Here<span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>Python<span class="w"> </span><span class="k">function</span><span class="w"> </span>that<span class="w"> </span>transforms<span class="w"> </span>bytes<span class="w"> </span>to<span class="w"> </span>Giga<span class="w"> </span>bytes:<span class="se">\n\n</span><span class="sb">```</span>python<span class="se">\n</span>def<span class="w"> </span>bytes_to_giga_bytes<span class="o">(</span>bytes<span class="o">)</span>:<span class="se">\n</span><span class="w">    </span><span class="k">return</span><span class="w"> </span>bytes<span class="w"> </span>/<span class="w"> </span><span class="m">1024</span><span class="w"> </span>/<span class="w"> </span><span class="m">1024</span><span class="w"> </span>/<span class="w"> </span><span class="m">1024</span><span class="se">\n</span><span class="sb">```</span><span class="se">\n\n</span>This<span class="w"> </span><span class="k">function</span><span class="w"> </span>takes<span class="w"> </span>a<span class="w"> </span>single
</span></code></pre></div></p>
<p>좋습니다. 이제 결과를 직접 사용하여 바이트를 기가바이트로 변환할 수 있습니다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="k">def</span><span class="w"> </span><span class="nf">bytes_to_giga_bytes</span><span class="p">(</span><span class="nb">bytes</span><span class="p">):</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a>  <span class="k">return</span> <span class="nb">bytes</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span> <span class="o">/</span> <span class="mi">1024</span>
</span></code></pre></div>
<p><a href="https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html"><code>torch.cuda.max_memory_allocated</code></a>를 호출하여 최대 GPU 메모리 할당을 측정해 보겠습니다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="n">bytes_to_giga_bytes</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">())</span>
</span></code></pre></div>
<p><strong>출력</strong>:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="m">29</span>.0260648727417
</span></code></pre></div></p>
<p>대략적으로 계산한 결과와 거의 일치합니다! 바이트에서 킬로바이트로 변환할 때 1000이 아닌 1024로 곱해야 하므로 숫자가 정확하지 않은 것을 알 수 있습니다. 따라서 대략적으로 계산할 때 공식은 "최대 X GB"으로 이해할 수 있습니다. 만약 우리가 모델을 float32 정밀도로 실행하려고 했다면 더 큰 크기인 64GB의 VRAM이 필요했을 것입니다.</p>
<blockquote>
<p>거의 모든 모델이 요즘 bfloat16으로 학습되므로, <a href="https://discuss.pytorch.org/t/bfloat16-native-support/117155/5">GPU가 bfloat16을 지원</a>한다면 모델을 float32 정밀도로 실행할 이유가 없습니다. float32로 돌리는 모델은 학습할 때 사용했던 정밀도보다 더 나은 추론 결과를 제공하지 않습니다.</p>
</blockquote>
<p>모델 가중치가 어떤 정밀도 형식으로 Hub에 저장되어 있는지 확실하지 않은 경우, HuggingFace Hub에서 해당 체크포인트 config의 <code>"dtype"</code>을 확인하면 됩니다, <em>예</em>를 들어 <a href="https://huggingface.co/meta-llama/Llama-2-7b-hf/blob/6fdf2e60f86ff2481f2241aaee459f85b5b0bbb9/config.json#L21">여기</a>를 확인하세요. 모델을 <code>from_pretrained(..., dtype=...)</code>로 로드할 때는 config에 명시된 정밀도 유형과 동일한 정밀도로 설정하는 것이 권장됩니다. 단, 원래 유형이 float32인 경우 추론을 위해 <code>float16</code> 또는 <code>bfloat16</code>을 둘 다 사용할 수 있습니다.</p>
<p>이제 <code>flush(...)</code> 함수를 정의하여 모든 메모리를 해제하고, GPU 메모리의 최대 할당량을 정확하게 측정하도록 합시다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="k">del</span> <span class="n">pipe</span>
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="k">del</span> <span class="n">model</span>
</span><span id="__span-8-3"><a id="__codelineno-8-3" name="__codelineno-8-3" href="#__codelineno-8-3"></a>
</span><span id="__span-8-4"><a id="__codelineno-8-4" name="__codelineno-8-4" href="#__codelineno-8-4"></a><span class="kn">import</span><span class="w"> </span><span class="nn">gc</span>
</span><span id="__span-8-5"><a id="__codelineno-8-5" name="__codelineno-8-5" href="#__codelineno-8-5"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-8-6"><a id="__codelineno-8-6" name="__codelineno-8-6" href="#__codelineno-8-6"></a>
</span><span id="__span-8-7"><a id="__codelineno-8-7" name="__codelineno-8-7" href="#__codelineno-8-7"></a><span class="k">def</span><span class="w"> </span><span class="nf">flush</span><span class="p">():</span>
</span><span id="__span-8-8"><a id="__codelineno-8-8" name="__codelineno-8-8" href="#__codelineno-8-8"></a>  <span class="n">gc</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</span><span id="__span-8-9"><a id="__codelineno-8-9" name="__codelineno-8-9" href="#__codelineno-8-9"></a>  <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">empty_cache</span><span class="p">()</span>
</span><span id="__span-8-10"><a id="__codelineno-8-10" name="__codelineno-8-10" href="#__codelineno-8-10"></a>  <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">reset_peak_memory_stats</span><span class="p">()</span>
</span></code></pre></div>
<p>다음 실험을 위해 바로 호출해 봅시다.</p>
<p><div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="n">flush</span><span class="p">()</span>
</span></code></pre></div>
최근 버전의 accelerate 라이브러리에서는 <code>release_memory()</code>라는 유틸리티 메소드도 사용할 수 있습니다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">accelerate.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">release_memory</span>
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="c1"># ...</span>
</span><span id="__span-10-3"><a id="__codelineno-10-3" name="__codelineno-10-3" href="#__codelineno-10-3"></a>
</span><span id="__span-10-4"><a id="__codelineno-10-4" name="__codelineno-10-4" href="#__codelineno-10-4"></a><span class="n">release_memory</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</span></code></pre></div>
<p>만약 GPU에 32GB의 VRAM이 없다면 어떻게 될까요? 모델 가중치를 성능에 큰 손실 없이 8비트 또는 4비트로 양자화할 수 있다는 것이 밝혀졌습니다(참고: <a href="https://huggingface.co/papers/2208.07339">Dettmers et al.</a>). 최근의 <a href="https://huggingface.co/papers/2210.17323">GPTQ 논문</a> 에서는 모델을 3비트 또는 2비트로 양자화해도 성능 손실이 허용 가능한 수준임을 보여주었습니다🤯.</p>
<p>너무 자세한 내용은 다루지 않고 설명하자면, 양자화는 가중치의 정밀도를 줄이면서 모델의 추론 결과를 가능한 한 정확하게(즉, bfloat16과 최대한 가깝게) 유지하려고 합니다. 양자화는 특히 텍스트 생성에 잘 작동하는데, 이는 우리가 <em>가장 가능성 있는 다음 토큰 집합</em>을 선택하는 것에 초점을 두고 있기 때문이며, 다음 토큰의 <em>logit</em> 분포값을 정확하게 예측할 필요는 없기 때문입니다. 핵심은 다음 토큰 <em>logit</em> 분포가 대략적으로 동일하게 유지되어 <code>argmax</code> 또는 <code>topk</code> 연산이 동일한 결과를 제공하는 것입니다.</p>
<p>다양한 양자화 기법이 존재하지만, 자세히 다루지는 않을 것입니다. 일반적으로 모든 양자화 기법은 다음과 같이 작동합니다:</p>
<ul>
<li>
<ol>
<li>모든 가중치를 목표 정밀도로 양자화합니다.</li>
</ol>
</li>
<li>
<ol>
<li>양자화된 가중치를 로드하고, bfloat16 정밀도의 입력 벡터 시퀀스를 모델에 전달합니다.</li>
</ol>
</li>
<li>
<ol>
<li>가중치를 동적으로 bfloat16으로 반대로 양자화(dequantize)하여 입력 벡터와 함께 bfloat16 정밀도로 계산을 수행합니다.</li>
</ol>
</li>
</ul>
<p>간단히 말해서, <em>입력-가중치 행렬</em> 곱셈은, \( X \)가 <em>입력</em>, \( W \)가 가중치 행렬, \( Y \)가 출력인 경우 다음과 같습니다:</p>
<p>$$ Y = X * W $$</p>
<p>위 공식이 다음과 같이 변경됩니다</p>
<p>$$ Y = X * \text{dequantize}(W) $$</p>
<p>모든 행렬 곱셈에 대해 위와 같이 수행됩니다. 입력이 네트워크 그래프를 통과하면서 모든 가중치 행렬에 대해 역양자화(dequantization)와 재양자화(re-quantization)가 순차적으로 수행됩니다.</p>
<p>따라서, 양자화된 가중치를 사용할 때 추론 시간이 감소하지 <strong>않고</strong> 오히려 증가하는 경우가 많습니다. 이제 이론은 충분하니 실제로 시도해 봅시다! Transformers를 사용하여 가중치를 양자화하려면 <a href="https://github.com/TimDettmers/bitsandbytes"><code>bitsandbytes</code></a> 라이브러리가 설치되어 있는지 확인해야 합니다.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a>!pip<span class="w"> </span>install<span class="w"> </span>bitsandbytes
</span></code></pre></div>
<p>그런 다음 <code>from_pretrained</code>에 <code>load_in_8bit=True</code> 플래그를 추가하여 8비트 양자화로 모델을 로드할 수 있습니다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigcode/octocoder&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span></code></pre></div>
<p>이제 예제를 다시 실행하고 메모리 사용량을 측정해 봅시다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</span><span id="__span-13-2"><a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a>
</span><span id="__span-13-3"><a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a><span class="n">result</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">60</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;generated_text&quot;</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">):]</span>
</span><span id="__span-13-4"><a id="__codelineno-13-4" name="__codelineno-13-4" href="#__codelineno-13-4"></a><span class="n">result</span>
</span></code></pre></div>
<p><strong>출력</strong>:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a>Here<span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>Python<span class="w"> </span><span class="k">function</span><span class="w"> </span>that<span class="w"> </span>transforms<span class="w"> </span>bytes<span class="w"> </span>to<span class="w"> </span>Giga<span class="w"> </span>bytes:<span class="se">\n\n</span><span class="sb">```</span>python<span class="se">\n</span>def<span class="w"> </span>bytes_to_giga_bytes<span class="o">(</span>bytes<span class="o">)</span>:<span class="se">\n</span><span class="w">    </span><span class="k">return</span><span class="w"> </span>bytes<span class="w"> </span>/<span class="w"> </span><span class="m">1024</span><span class="w"> </span>/<span class="w"> </span><span class="m">1024</span><span class="w"> </span>/<span class="w"> </span><span class="m">1024</span><span class="se">\n</span><span class="sb">```</span><span class="se">\n\n</span>This<span class="w"> </span><span class="k">function</span><span class="w"> </span>takes<span class="w"> </span>a<span class="w"> </span>single
</span></code></pre></div></p>
<p>좋습니다. 정확도 손실 없이 이전과 동일한 결과를 얻고 있습니다! 이번에는 사용된 메모리 양을 확인해 봅시다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="n">bytes_to_giga_bytes</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">())</span>
</span></code></pre></div>
<p><strong>출력</strong>:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="m">15</span>.219234466552734
</span></code></pre></div></p>
<p>훨씬 적네요! 메모리 사용량이 15GB를 조금 넘는 수준으로 줄어들어 4090과 같은 소비자용 GPU에서도 이 모델을 실행할 수 있습니다. 메모리 효율성에서 매우 큰 향상을 보이고 있으며 모델 출력의 품질 저하도 거의 없습니다. 그러나 추론 중에 약간의 속도 저하가 발생한 것을 확인할 수 있습니다.</p>
<p>모델을 삭제하고 메모리를 다시 초기화합니다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="k">del</span> <span class="n">model</span>
</span><span id="__span-17-2"><a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a><span class="k">del</span> <span class="n">pipe</span>
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="n">flush</span><span class="p">()</span>
</span></code></pre></div>
<p>이제 4비트 양자화가 제공하는 최대 GPU 메모리 사용량을 확인해 봅시다. 4비트로 모델을 양자화하려면 이전과 동일한 API를 사용하되 이번에는 <code>load_in_8bit=True</code> 대신 <code>load_in_4bit=True</code>를 전달하면 됩니다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigcode/octocoder&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">pad_token_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</span><span id="__span-19-2"><a id="__codelineno-19-2" name="__codelineno-19-2" href="#__codelineno-19-2"></a>
</span><span id="__span-19-3"><a id="__codelineno-19-3" name="__codelineno-19-3" href="#__codelineno-19-3"></a><span class="n">pipe</span> <span class="o">=</span> <span class="n">pipeline</span><span class="p">(</span><span class="s2">&quot;text-generation&quot;</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</span><span id="__span-19-4"><a id="__codelineno-19-4" name="__codelineno-19-4" href="#__codelineno-19-4"></a>
</span><span id="__span-19-5"><a id="__codelineno-19-5" name="__codelineno-19-5" href="#__codelineno-19-5"></a><span class="n">result</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">60</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="s2">&quot;generated_text&quot;</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">):]</span>
</span><span id="__span-19-6"><a id="__codelineno-19-6" name="__codelineno-19-6" href="#__codelineno-19-6"></a><span class="n">result</span>
</span></code></pre></div>
<p><strong>출력</strong>:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-20-1"><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a>Here<span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>Python<span class="w"> </span><span class="k">function</span><span class="w"> </span>that<span class="w"> </span>transforms<span class="w"> </span>bytes<span class="w"> </span>to<span class="w"> </span>Giga<span class="w"> </span>bytes:<span class="se">\n\n</span><span class="sb">```</span><span class="se">\n</span>def<span class="w"> </span>bytes_to_gigabytes<span class="o">(</span>bytes<span class="o">)</span>:<span class="se">\n</span><span class="w">    </span><span class="k">return</span><span class="w"> </span>bytes<span class="w"> </span>/<span class="w"> </span><span class="m">1024</span><span class="w"> </span>/<span class="w"> </span><span class="m">1024</span><span class="w"> </span>/<span class="w"> </span><span class="m">1024</span><span class="se">\n</span><span class="sb">```</span><span class="se">\n\n</span>This<span class="w"> </span><span class="k">function</span><span class="w"> </span>takes<span class="w"> </span>a<span class="w"> </span>single<span class="w"> </span>argument
</span></code></pre></div></p>
<p>바로 전 코드 스니펫에서 <code>python</code>만 누락되고, 이 전과 거의 동일한 출력 텍스트를 보고 있습니다. 이제 얼마나 많은 메모리가 필요했는지 확인해 봅시다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-21-1"><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a><span class="n">bytes_to_giga_bytes</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">max_memory_allocated</span><span class="p">())</span>
</span></code></pre></div>
<p><strong>출력</strong>:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-22-1"><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a><span class="m">9</span>.543574333190918
</span></code></pre></div></p>
<p>9.5GB밖에 되지 않습니다! 150억 개 이상의 파라미터를 가진 모델인 것을 감안하면 매우 적은 양입니다.</p>
<p>여기서는 모델의 정확도 저하가 거의 없음을 확인할 수 있지만, 실제로는 4비트 양자화를 8비트 양자화나 <code>bfloat16</code>를 사용한 추론 결과와 비교하면 결과가 다를 수 있습니다. 사용자가 직접 시도해 보는 것이 좋겠습니다.</p>
<p>또한 4비트 양자화에 사용된 더 공격적인 양자화 방법으로 인해 추론 시 \( \text{quantize} \)와 \( \text{dequantize} \) 과정이 더 오래 걸리므로 여기서도 8비트 양자화와 비교하여 추론 속도가 약간 느려졌음을 유의하세요.</p>
<p><div class="language-python highlight"><pre><span></span><code><span id="__span-23-1"><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a><span class="k">del</span> <span class="n">model</span>
</span><span id="__span-23-2"><a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a><span class="k">del</span> <span class="n">pipe</span>
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-24-1"><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a><span class="n">flush</span><span class="p">()</span>
</span></code></pre></div></p>
<p>전체적으로 OctoCoder를 8비트 정밀도로 실행하면 필요한 GPU VRAM이 32GB에서 15GB로 줄어들었고, 4비트 정밀도로 모델을 실행하면 필요한 GPU VRAM이 9GB로 더 줄어드는 것을 확인했습니다.</p>
<p>4비트 양자화는 RTX3090, V100, T4와 같은 GPU에서 모델을 실행할 수 있게 해주며, 이는 대부분의 사람들이 접근할 수 있는 GPU입니다.</p>
<p>양자화에 대한 더 많은 정보를 확인하고 4비트보다 더 적은 GPU VRAM 메모리로 모델을 양자화하거나, 더 많은 양자화 관련 정보를 보려면 <a href="https://huggingface.co/docs/transformers/main/en/main_classes/quantization#autogptq-integration%60"><code>AutoGPTQ</code></a> 구현을 참조하는 것을 추천합니다.</p>
<blockquote>
<p>결론적으로, 모델 양자화는 향상된 메모리 효율성과 모델 정확성 간의 균형을 맞추는 것이며, 경우에 따라 추론 시간에도 영향을 미칠 수 있습니다.</p>
</blockquote>
<p>실제 사례에서 GPU 메모리가 충분하다면, 양자화를 고려할 필요가 없습니다. 그러나 많은 GPU는 양자화 없이 대규모 언어 모델을 실행할 수 없으며, 이 경우 4비트 및 8비트 양자화가 매우 유용한 도구입니다.</p>
<p>사용과 관련한 더 자세한 정보는 <a href="https://huggingface.co/docs/transformers/main_classes/quantization#general-usage">트랜스포머 양자화 문서</a>를 참고하는 것을 강력히 추천합니다. 다음으로, 더 나은 알고리즘과 개선된 모델 아키텍처를 사용하여 계산 및 메모리 효율성을 향상시키는 방법을 살펴보겠습니다.</p>
<h2 id="2-2-flash-attention">2. 플래시 어텐션 [[2-flash-attention]]</h2>
<p>오늘날의 최고 성능을 자랑하는 대규모 언어 모델은 대체로 피드포워드 레이어(feed-forward layer), 활성화 레이어(activation layer), 레이어 정규화 레이어(layer normalization layer), 그리고 가장 중요한 셀프 어텐션 레이어(self-attention layer)로 구성된 아키텍처를 공유하고 있습니다.</p>
<p>셀프 어텐션 레이어는 입력 토큰 간의 문맥적 관계를 이해할 수 있게 해 주기 때문에 대규모 언어 모델의 핵심 요소입니다.
하지만 셀프 어텐션 레이어의 최대 GPU 메모리 소비는 입력 토큰의 수(이하 \( N \)으로 표기)와 함께 계산 및 메모리 복잡성이 <em>2차적</em>으로 증가합니다. 입력 시퀀스가 짧은 경우(최대 1000개)에는 크게 눈에 띄지 않지만, 더 긴 입력 시퀀스(약 16000개)에서는 심각한 문제가 됩니다.</p>
<p>자세히 한 번 들여다 봅시다. 길이 \( N \)의 입력 \( \mathbf{X} \)에 대한 셀프 어텐션 레이어의 출력 \( \mathbf{O} \)을 계산하는 공식은 다음과 같습니다:</p>
<p>$$ \textbf{O} = \text{Attn}(\mathbf{X}) = \mathbf{V} \times \text{Softmax}(\mathbf{QK}^T) \text{ with } \mathbf{Q} = \mathbf{W}_q \mathbf{X}, \mathbf{V} = \mathbf{W}_v \mathbf{X}, \mathbf{K} = \mathbf{W}_k \mathbf{X} $$</p>
<p>\( \mathbf{X} = (\mathbf{x}1, ... \mathbf{x}{N}) \)는 어텐션 레이어의 입력 시퀀스입니다. 프로젝션 \( \mathbf{Q} \)와 \( \mathbf{K} \)는 각각 \( N \)개의 벡터로 구성되며, 그 결과 \( \mathbf{QK}^T \)의 크기는 \( N^2 \)가 됩니다.</p>
<p>대규모 언어 모델은 일반적으로 여러 개의 어텐션 헤드를 가지고 있어 여러 개의 셀프 어텐션 계산을 병렬로 수행합니다. 대규모 언어 모델이 40개의 어텐션 헤드를 가지고 bfloat16 정밀도로 실행된다고 가정하면, \( \mathbf{QK^T} \) 행렬을 저장하는 데 필요한 메모리를 \( 40 * 2 * N^2 \) 바이트로 계산할 수 있습니다. \( N=1000 \)일 때는 약 50MB의 VRAM만 필요하지만, \( N=16000 \)일 때는 19GB의 VRAM이 필요하며, \( N=100,000 \)일 때는 \( \mathbf{QK^T} \) 행렬을 저장하기 위해 거의 1TB의 VRAM이 필요합니다.</p>
<p>요약하자면, 기본 셀프 어텐션 알고리즘은 큰 입력 컨텍스트에 대해 매우 과도한 메모리 사용을 요구하게 됩니다.</p>
<p>대규모 언어 모델의 텍스트 이해 및 생성 능력이 개선되면서 점점 더 복잡한 작업에 사용되고 있습니다. 한때 몇 문장의 번역이나 요약을 처리하던 모델이 이제는 전체 페이지를 처리해야 하게 되면서 광범위한 입력 길이를 처리할 수 있는 능력이 요구되고 있습니다.</p>
<p>어떻게 하면 큰 입력 길이에 대한 과도한 메모리 요구를 없앨 수 있을까요? \( QK^T \) 행렬을 제거하는 새로운 셀프 어텐션 메커니즘을 계산하는 방법이 필요합니다. <a href="https://huggingface.co/papers/2205.14135">Tri Dao et al.</a>은 바로 이러한 새로운 알고리즘을 개발하였고, 그것이 <strong>플래시 어텐션(Flash Attention)</strong>입니다.</p>
<p>간단히 말해, 플래시 어텐션은 \(\mathbf{V} \times \text{Softmax}(\mathbf{QK}^T\)) 계산을 분할하는데, 여러 번의 소프트맥스 계산을 반복하면서 작은 청크 단위로 출력을 계산합니다:</p>
<p>$$ \textbf{O}<em ij="ij">i \leftarrow s^a</em>} * \textbf{O<em ij="ij">i + s^b</em>} * \mathbf{V<em i_j="i,j">{j} \times \text{Softmax}(\mathbf{QK}^T</em> $$}) \text{ for multiple } i, j \text{ iterations</p>
<p>여기서 \( s^a_{ij} \)와 \( s^b_{ij} \)는 각 \( i \)와 \( j \)에 대해 계산되는 소프트맥스 정규화 통계량입니다.</p>
<p>플래시 어텐션의 전체 알고리즘은 더 복잡하며, 본 가이드의 범위를 벗어나기 때문에 크게 단순화하였습니다. 여러분은 잘 작성된 <a href="https://huggingface.co/papers/2205.14135">Flash Attention paper</a> 논문을 참조하여 더 자세한 내용을 확인해 보시기 바랍니다.</p>
<p>주요 요점은 다음과 같습니다:</p>
<blockquote>
<p>소프트맥스 정규화 통계량과 몇 가지 스마트한 수학적 방법을 사용함으로써, 플래시 어텐션은 기본 셀프 어텐션 레이어와 <strong>숫자적으로 동일한</strong> 출력을 제공하고 메모리 비용은 \( N \)에 따라 선형적으로만 증가합니다.</p>
</blockquote>
<p>공식을 보면, 플래시 어텐션이 더 많은 계산을 필요로 하기 때문에 기본 셀프 어텐션 공식보다 훨씬 느릴 것이라고 생각할 수 있습니다. 실제로 플래시 어텐션은 소프트맥스 정규화 통계량을 지속적으로 다시 계산해야 하기 때문에 일반 어텐션보다 더 많은 FLOP이 필요합니다. (더 자세한 내용은 <a href="https://huggingface.co/papers/2205.14135">논문</a>을 참조하세요)</p>
<blockquote>
<p>그러나 플래시 어텐션은 기본 어텐션보다 추론 속도가 훨씬 빠릅니다. 이는 GPU의 느리고 고대역폭 메모리(VRAM)의 사용량을 크게 줄이고 대신 빠른 온칩 메모리(SRAM)에 집중할 수 있기 때문입니다.</p>
</blockquote>
<p>본질적으로, 플래시 어텐션의 모든 중간 단계의 쓰기 및 읽기 작업은 느린 VRAM 메모리에 접근하지 않고 빠른 <em>온칩</em> SRAM 메모리를 사용하여 출력 벡터 \( \mathbf{O} \)를 계산할 수 있도록 합니다.</p>
<p>현실적으로 플래시 어텐션이 사용 가능한 경우 이를 <strong>사용하지 않을</strong> 이유는 전혀 없습니다. 이 알고리즘은 수학적으로 동일한 출력을 제공하며, 더 빠르고 메모리 효율적입니다.</p>
<p>실제 예를 살펴보겠습니다.</p>
<h2 id="3-3-architectural-innovations">3. 아키텍처 혁신 [[3-architectural-innovations]]</h2>
<p>지금까지 우리는 계산 및 메모리 효율성을 개선하기 위해 다음을 살펴보았습니다:</p>
<ul>
<li>가중치를 낮은 정밀도 형식으로 변환</li>
<li>셀프 어텐션 알고리즘을 보다 더 메모리 및 계산 효율적인 버전으로 교체</li>
</ul>
<p>이제 긴 텍스트 입력이 필요한 작업에 가장 효과적이고 효율적인 대규모 언어 모델 아키텍처로 변경하는 방법을 살펴보겠습니다. 작업의 예시는 다음과 같습니다:
-   검색 증강 질의 응답
-   요약
-   채팅</p>
<p><em>채팅</em>을 위해서는 대규모 언어 모델이 긴 텍스트 입력을 처리하는 것뿐만 아니라 사용자와 어시스턴트 간의 대화도 효율적으로 처리할 수 있어야 합니다(예: ChatGPT).</p>
<p>한번 학습된 후에는 대규모 언어 모델의 기본 아키텍처를 변경하기 어렵기 때문에, 대규모 언어 모델의 작업에 대한 고려를 미리 하고 이에 따라 모델의 아키텍처를 최적화하는 것이 중요합니다. 긴 입력 시퀀스에 대해 메모리 또는 성능의 병목 현상을 빠르게 발생시키는 모델 아키텍처의 중요한 두 가지 구성 요소가 있습니다.</p>
<ul>
<li>위치 임베딩</li>
<li>키-값 캐시</li>
</ul>
<p>각 구성 요소를 더 자세히 살펴보겠습니다.</p>
<h3 id="31-31-improving-positional-embeddings-of-llms">3.1 대규모 언어 모델의 위치 임베딩 개선 [[31-improving-positional-embeddings-of-llms]]</h3>
<p>셀프 어텐션은 각 토큰을 서로의 토큰과 연관시킵니다.
예를 들어, 텍스트 입력 시퀀스 <em>"Hello", "I", "love", "you"</em>의 \( \text{Softmax}(\mathbf{QK}^T) \) 행렬은 다음과 같을 수 있습니다:</p>
<p><img alt="" src="/blog/assets/163_optimize_llm/self_attn_tokens.png" /></p>
<p>각 단어 토큰은 다른 모든 단어 토큰에 주의를 기울이는 확률 질량을 부여받아 모든 다른 단어 토큰과 관계를 맺게 됩니다. 예를 들어, 단어 <em>"love"</em>는 단어 <em>"Hello"</em>에 5%, <em>"I"</em>에 30%, 그리고 자신에게 65%의 주의를 기울입니다.</p>
<p>셀프 어텐션 기반 대규모 언어 모델이 위치 임베딩이 없는 경우 텍스트 입력의 위치를 이해하는 데 큰 어려움을 겪을 것입니다. 이는 \( \mathbf{QK}^T \)에 의해 계산된 확률 점수가 상대적 위치 거리에 상관없이 각 단어 토큰을 다른 모든 단어 토큰과 \( O(1) \) 계산으로 연관시키기 때문입니다. 따라서 위치 임베딩이 없는 대규모 언어 모델은 각 토큰이 다른 모든 토큰과 동일한 거리에 있는 것으로 나타나기 때문에, <em>"Hello I love you"</em>와 <em>"You love I hello"</em>를 구분하는 것이 매우 어렵습니다.</p>
<p>대규모 언어 모델이 문장의 순서를 이해하려면 추가적인 <em>단서</em>가 필요하며, 이는 일반적으로 <em>위치 인코딩</em> (또는 <em>위치 임베딩</em>이라고도 함)의 형태로 적용됩니다. 
위치 인코딩은 각 토큰의 위치를 숫자 표현으로 인코딩하여 대규모 언어 모델이 문장의 순서를 더 잘 이해할 수 있도록 도와줍니다.</p>
<p><a href="https://huggingface.co/papers/1706.03762"><em>Attention Is All You Need</em></a> 논문의 저자들은 사인 함수 기반의 위치 임베딩 \( \mathbf{P} = \mathbf{p}_1, \ldots, \mathbf{p}_N \)을 도입했습니다. 각 벡터 \( \mathbf{p}_i \)는 위치 \( i \)의 사인 함수로 계산됩니다. 위치 인코딩은 입력 시퀀스 벡터에 단순히 더해져 \( \mathbf{\hat{X}} = \mathbf{\hat{x}}_1, \ldots, \mathbf{\hat{x}}_N \) = \( \mathbf{x}_1 + \mathbf{p}_1, \ldots, \mathbf{x}_N + \mathbf{p}_N \) 모델이 문장 순서를 더 잘 학습할 수 있도록 합니다.</p>
<p>고정된 위치 임베딩 대신 <a href="https://huggingface.co/papers/1810.04805">Devlin et al.</a>과 같은 다른 연구자들은 학습된 위치 인코딩을 사용했습니다. 이 경우 위치 임베딩 \( \mathbf{P} \)은 학습 중에 사용됩니다.</p>
<p>사인 함수 및 학습된 위치 임베딩은 문장 순서를 대규모 언어 모델에 인코딩하는 주요 방법이었지만, 이러한 위치 인코딩과 관련된 몇 가지 문제가 발견되었습니다:</p>
<ol>
<li>사인 함수와 학습된 위치 임베딩은 모두 절대 위치 임베딩으로, 각 위치 ID \( 0, \ldots, N \)에 대해 고유한 임베딩을 인코딩합니다. <a href="https://huggingface.co/papers/2009.13658">Huang et al.</a> 및 <a href="https://huggingface.co/papers/2104.09864">Su et al.</a>의 연구에 따르면, 절대 위치 임베딩은 긴 텍스트 입력에 대해 대규모 언어 모델 성능이 저하됩니다. 긴 텍스트 입력의 경우, 모델이 절대 위치 대신 입력 토큰 간의 상대적 위치 거리를 학습하는 것이 유리합니다.</li>
<li>학습된 위치 임베딩을 사용할 때, 대규모 언어 모델은 고정된 입력 길이 \( N \)으로 학습되어야 하므로, 학습된 입력 길이보다 더 긴 입력 길이에 대해 추론하는 것이 어렵습니다.</li>
</ol>
<p>최근에는 위에서 언급한 문제를 해결할 수 있는 상대적 위치 임베딩이 더 인기를 끌고 있습니다. 특히 다음과 같은 방법들이 주목받고 있습니다:</p>
<ul>
<li><a href="https://huggingface.co/papers/2104.09864">Rotary Position Embedding (RoPE)</a></li>
<li><a href="https://huggingface.co/papers/2108.12409">ALiBi</a></li>
</ul>
<p><em>RoPE</em>와 <em>ALiBi</em>는 모두 셀프 어텐션 알고리즘 내에서 직접적으로 문장 순서를 모델에게 알려주는 것이 최선이라고 주장합니다. 이는 단어 토큰이 서로 관계를 맺는 곳이기 때문입니다. 구체적으로, 문장 순서를 \( \mathbf{QK}^T \) 계산을 수정하는 방식으로 알려주어야 한다는 것입니다. </p>
<p>너무 많은 세부 사항을 다루지 않고, <em>RoPE</em>는 위치 정보를 쿼리-키 쌍에 인코딩할 수 있다고 지적합니다. 예를 들어, 각 벡터 \( \mathbf{q}_i \)와 \( \mathbf{x}_j \)를 각각 \( \theta * i \)와 \( \theta * j \)의 각도로 회전시킴으로써 다음과 같이 표현할 수 있습니다:</p>
<p>$$ \mathbf{\hat{q}}<em -j="-j" _theta_="\theta," i="i">i^T \mathbf{\hat{x}}_j = \mathbf{{q}}_i^T \mathbf{R}</em>_j. $$} \mathbf{{x}</p>
<p>여기서 \( \mathbf{R}_{\theta, i - j} \)는 회전 행렬을 나타냅니다. \( \theta \)는 훈련 중에 <em>학습되지 않으며</em>, 대신 학습 중 최대 입력 시퀀스 길이에 따라 사전 정의된 값으로 설정됩니다.</p>
<blockquote>
<p>이렇게 함으로써 \( \mathbf{q}_i \)와 \( \mathbf{q}_j \) 간의 확률 점수는 \( i \ne j \)인 경우에만 영향을 받으며, 각 벡터의 특정 위치 \( i \)와 \( j \)와는 상관없이 오직 상대적 거리 \( i - j \)에만 의존하게 됩니다.</p>
</blockquote>
<p><em>RoPE</em>는 현재 여러 중요한 대규모 언어 모델이 사용되고 있습니다. 예를 들면:</p>
<ul>
<li><a href="https://huggingface.co/tiiuae/falcon-40b"><strong>Falcon</strong></a></li>
<li><a href="https://huggingface.co/papers/2302.13971"><strong>Llama</strong></a></li>
<li><a href="https://huggingface.co/papers/2204.02311"><strong>PaLM</strong></a></li>
</ul>
<p>대안으로, <em>ALiBi</em>는 훨씬 더 간단한 상대적 위치 인코딩 방식을 제안합니다. 입력 토큰 간의 상대적 거리를 음수인 정수로서 사전 정의된 값 <code>m</code>으로 스케일링하여 \( \mathbf{QK}^T \) 행렬의 각 쿼리-키 항목에 소프트맥스 계산 직전에 추가합니다.</p>
<p><img alt="" src="/blog/assets/163_optimize_llm/alibi.png" /></p>
<p><a href="https://huggingface.co/papers/2108.12409">ALiBi</a> 논문에서 보여주듯이, 이 간단한 상대적 위치 인코딩은 매우 긴 텍스트 입력 시퀀스에서도 모델이 높은 성능을 유지할 수 있게 합니다.</p>
<p><em>ALiBi</em>는 현재 여러 중요한 대규모 언어 모델 모델이 사용하고 있습니다. 예를 들면:</p>
<ul>
<li><a href="https://huggingface.co/mosaicml/mpt-30b"><strong>MPT</strong></a></li>
<li><a href="https://huggingface.co/bigscience/bloom"><strong>BLOOM</strong></a></li>
</ul>
<p><em>RoPE</em>와 <em>ALiBi</em> 위치 인코딩은 모두 학습 중에 보지 못한 입력 길이에 대해 확장할 수 있으며, <em>ALiBi</em>가 <em>RoPE</em>보다 더 잘 확장되는 것으로 나타났습니다. <em>ALiBi</em>의 경우, 하삼각 위치 행렬의 값을 입력 시퀀스 길이에 맞추어 증가시키기만 하면 됩니다. <em>RoPE</em>의 경우, 학습 중에 사용된 동일한 \( \theta \)를 유지하면 학습 중에 보지 못한 매우 긴 텍스트 입력을 전달할 때 성능이 저하됩니다(참고: <a href="https://huggingface.co/papers/2108.12409">Press et al.</a>). 그러나 커뮤니티는 \( \theta \)를 조정하는 몇 가지 효과적인 트릭을 찾아냈으며, 이를 통해 <em>RoPE</em> 위치 임베딩이 확장된 텍스트 입력 시퀀스에서도 잘 작동할 수 있게 되었습니다(참고: <a href="https://github.com/huggingface/transformers/pull/24653">here</a>).</p>
<blockquote>
<p>RoPE와 ALiBi는 모두 훈련 중에 <em>학습되지 않는</em> 상대적 위치 임베딩으로 다음과 같은 직관에 기반합니다:
 -   텍스트 입력에 대한 위치 단서는 셀프 어텐션 레이어의 \( QK^T \) 행렬에 직접 제공되어야 합니다.
 -   대규모 언어 모델은 일정한 <em>상대적</em> 거리 위치 인코딩을 서로 학습하도록 유도되어야 합니다.
 -   텍스트 입력 토큰 간의 거리가 멀어질수록, 그들의 쿼리-값 확률은 낮아져야 합니다. RoPE와 ALiBi는 서로 멀리 떨어진 토큰의 쿼리-키 확률을 낮춥니다. RoPE는 쿼리-키 벡터 간의 각도를 증가시켜 벡터 곱을 감소시키는 방식으로, ALiBi는 벡터 곱에 큰 음수를 추가하는 방식으로 이 작업을 수행합니다.</p>
</blockquote>
<p>결론적으로, 큰 텍스트 입력을 처리해야 하는 작업에 배포될 예정인  대규모 언어 모델은 RoPE와 ALiBi와 같은 상대적 위치 임베딩으로 훈련하는 것이 더 좋습니다. 또한 RoPE와 ALiBi를 사용하여 훈련된  대규모 언어 모델이 고정 길이 \( N_1 = 2048 \)에서만 훈련되었더라도 위치 임베딩을 외삽하여 \( N_1 \)보다 훨씬 큰 텍스트 입력 \( N_2 = 8192 &gt; N_1 \)로 실습에서 사용할 수 있음을 유의하세요.</p>
<h3 id="32-32-the-key-value-cache">3.2 키-값 캐시 [[32-the-key-value-cache]]</h3>
<p>대규모 언어 모델을 이용한 자기회귀 텍스트 생성은 입력 시퀀스를 반복적으로 넣고, 다음 토큰을 샘플링하며, 그 다음 토큰을 입력 시퀀스에 추가하고, 대규모 언어 모델이 생성을 완료했다는 토큰을 생성할 때까지 이를 계속 수행하는 방식으로 작동합니다.</p>
<p>자기회귀 생성이 어떻게 작동하는지에 대한 시각적 설명을 보려면 <a href="https://huggingface.co/docs/transformers/llm_tutorial#generate-text">Transformer's Generate Text Tutorial</a>을 참조하세요.</p>
<p>자기회귀 생성이 실제로 어떻게 작동하는지 보여주는 간단한 코드 스니펫을 실행해 보겠습니다. 여기서는 <code>torch.argmax</code>를 통해 가장 가능성이 높은 다음 토큰을 가져올 것입니다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-25-1"><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a><span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
</span><span id="__span-25-2"><a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a>
</span><span id="__span-25-3"><a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
</span><span id="__span-25-4"><a id="__codelineno-25-4" name="__codelineno-25-4" href="#__codelineno-25-4"></a>  <span class="n">next_logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">)[</span><span class="s2">&quot;logits&quot;</span><span class="p">][:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
</span><span id="__span-25-5"><a id="__codelineno-25-5" name="__codelineno-25-5" href="#__codelineno-25-5"></a>  <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">next_logits</span><span class="p">,</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-25-6"><a id="__codelineno-25-6" name="__codelineno-25-6" href="#__codelineno-25-6"></a>
</span><span id="__span-25-7"><a id="__codelineno-25-7" name="__codelineno-25-7" href="#__codelineno-25-7"></a>  <span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">next_token_id</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-25-8"><a id="__codelineno-25-8" name="__codelineno-25-8" href="#__codelineno-25-8"></a>  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;shape of input_ids&quot;</span><span class="p">,</span> <span class="n">input_ids</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="__span-25-9"><a id="__codelineno-25-9" name="__codelineno-25-9" href="#__codelineno-25-9"></a>
</span><span id="__span-25-10"><a id="__codelineno-25-10" name="__codelineno-25-10" href="#__codelineno-25-10"></a><span class="n">generated_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">5</span><span class="p">:])</span>
</span><span id="__span-25-11"><a id="__codelineno-25-11" name="__codelineno-25-11" href="#__codelineno-25-11"></a><span class="n">generated_text</span>
</span></code></pre></div>
<p><strong>출력</strong>:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-26-1"><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a>shape<span class="w"> </span>of<span class="w"> </span>input_ids<span class="w"> </span>torch.Size<span class="o">([</span><span class="m">1</span>,<span class="w"> </span><span class="m">21</span><span class="o">])</span>
</span><span id="__span-26-2"><a id="__codelineno-26-2" name="__codelineno-26-2" href="#__codelineno-26-2"></a>shape<span class="w"> </span>of<span class="w"> </span>input_ids<span class="w"> </span>torch.Size<span class="o">([</span><span class="m">1</span>,<span class="w"> </span><span class="m">22</span><span class="o">])</span>
</span><span id="__span-26-3"><a id="__codelineno-26-3" name="__codelineno-26-3" href="#__codelineno-26-3"></a>shape<span class="w"> </span>of<span class="w"> </span>input_ids<span class="w"> </span>torch.Size<span class="o">([</span><span class="m">1</span>,<span class="w"> </span><span class="m">23</span><span class="o">])</span>
</span><span id="__span-26-4"><a id="__codelineno-26-4" name="__codelineno-26-4" href="#__codelineno-26-4"></a>shape<span class="w"> </span>of<span class="w"> </span>input_ids<span class="w"> </span>torch.Size<span class="o">([</span><span class="m">1</span>,<span class="w"> </span><span class="m">24</span><span class="o">])</span>
</span><span id="__span-26-5"><a id="__codelineno-26-5" name="__codelineno-26-5" href="#__codelineno-26-5"></a>shape<span class="w"> </span>of<span class="w"> </span>input_ids<span class="w"> </span>torch.Size<span class="o">([</span><span class="m">1</span>,<span class="w"> </span><span class="m">25</span><span class="o">])</span>
</span><span id="__span-26-6"><a id="__codelineno-26-6" name="__codelineno-26-6" href="#__codelineno-26-6"></a><span class="o">[</span><span class="s1">&#39; Here is a Python function&#39;</span><span class="o">]</span>
</span></code></pre></div></p>
<p>보시다시피 샘플링된 토큰에 의해 텍스트 입력 토큰을 매번 증가시킵니다.</p>
<p>매우 예외적인 경우를 제외하고, 대규모 언어 모델은 <a href="https://huggingface.co/docs/transformers/tasks/language_modeling#causal-language-modeling">인과적인 언어 모델링 목표</a>를 사용하여 학습되므로 어텐션 점수의 상삼각 행렬을 마스킹합니다. 이것이 위의 두 다이어그램에서 어텐션 점수가 비어 있는 이유입니다 (즉, 0 확률을 가짐). 인과 언어 모델링에 대한 빠른 요약은 <a href="https://jalammar.github.io/illustrated-gpt2/#part-2-illustrated-self-attention"><em>Illustrated Self Attention 블로그</em></a>를 참조할 수 있습니다.</p>
<p>결과적으로, 토큰은 <em>절대</em> 이전 토큰에 의존하지 않습니다. 더 구체적으로는 \( \mathbf{q}_i \) 벡터가 \( j &gt; i \)인 경우 어떤 키, 값 벡터 \( \mathbf{k}_j, \mathbf{v}j \)와도 연관되지 않습니다. 대신 \( \mathbf{q}i \)는 이전의 키-값 벡터 \( \mathbf{k}{m &lt; i}, \mathbf{v}{m &lt; i} \text{ , for } m \in {0, \ldots i - 1} \)에만 주의를 기울입니다. 불필요한 계산을 줄이기 위해 각 층의 키-값 벡터를 모든 이전 시간 단계에 대해 캐시할 수 있습니다.</p>
<p>다음으로, 대규모 언어 모델이 각 포워드 패스마다 키-값 캐시를 검색하고 전달하여 이를 활용하도록 합니다. 
Transformers에서는 <code>forward</code> 호출에 <code>use_cache</code> 플래그를 전달하여 키-값 캐시를 검색한 다음 현재 토큰과 함께 전달할 수 있습니다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-27-1"><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a><span class="n">past_key_values</span> <span class="o">=</span> <span class="kc">None</span> <span class="c1"># past_key_values 는 키-값 캐시를 의미</span>
</span><span id="__span-27-2"><a id="__codelineno-27-2" name="__codelineno-27-2" href="#__codelineno-27-2"></a><span class="n">generated_tokens</span> <span class="o">=</span> <span class="p">[]</span>
</span><span id="__span-27-3"><a id="__codelineno-27-3" name="__codelineno-27-3" href="#__codelineno-27-3"></a><span class="n">next_token_id</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)[</span><span class="s2">&quot;input_ids&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span><span class="p">)</span>
</span><span id="__span-27-4"><a id="__codelineno-27-4" name="__codelineno-27-4" href="#__codelineno-27-4"></a>
</span><span id="__span-27-5"><a id="__codelineno-27-5" name="__codelineno-27-5" href="#__codelineno-27-5"></a><span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
</span><span id="__span-27-6"><a id="__codelineno-27-6" name="__codelineno-27-6" href="#__codelineno-27-6"></a>  <span class="n">next_logits</span><span class="p">,</span> <span class="n">past_key_values</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">next_token_id</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">=</span><span class="n">past_key_values</span><span class="p">,</span> <span class="n">use_cache</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">to_tuple</span><span class="p">()</span>
</span><span id="__span-27-7"><a id="__codelineno-27-7" name="__codelineno-27-7" href="#__codelineno-27-7"></a>  <span class="n">next_logits</span> <span class="o">=</span> <span class="n">next_logits</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">:]</span>
</span><span id="__span-27-8"><a id="__codelineno-27-8" name="__codelineno-27-8" href="#__codelineno-27-8"></a>  <span class="n">next_token_id</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">next_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</span><span id="__span-27-9"><a id="__codelineno-27-9" name="__codelineno-27-9" href="#__codelineno-27-9"></a>
</span><span id="__span-27-10"><a id="__codelineno-27-10" name="__codelineno-27-10" href="#__codelineno-27-10"></a>  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;shape of input_ids&quot;</span><span class="p">,</span> <span class="n">next_token_id</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</span><span id="__span-27-11"><a id="__codelineno-27-11" name="__codelineno-27-11" href="#__codelineno-27-11"></a>  <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;length of key-value cache&quot;</span><span class="p">,</span> <span class="n">past_key_values</span><span class="o">.</span><span class="n">get_seq_length</span><span class="p">())</span>  <span class="c1"># past_key_values 형태: [num_layers, 0 for k, 1 for v, batch_size, length, hidden_dim]</span>
</span><span id="__span-27-12"><a id="__codelineno-27-12" name="__codelineno-27-12" href="#__codelineno-27-12"></a>  <span class="n">generated_tokens</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">next_token_id</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
</span><span id="__span-27-13"><a id="__codelineno-27-13" name="__codelineno-27-13" href="#__codelineno-27-13"></a>
</span><span id="__span-27-14"><a id="__codelineno-27-14" name="__codelineno-27-14" href="#__codelineno-27-14"></a><span class="n">generated_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generated_tokens</span><span class="p">)</span>
</span><span id="__span-27-15"><a id="__codelineno-27-15" name="__codelineno-27-15" href="#__codelineno-27-15"></a><span class="n">generated_text</span>
</span></code></pre></div>
<p><strong>출력</strong>:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-28-1"><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a>shape<span class="w"> </span>of<span class="w"> </span>input_ids<span class="w"> </span>torch.Size<span class="o">([</span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span><span class="o">])</span>
</span><span id="__span-28-2"><a id="__codelineno-28-2" name="__codelineno-28-2" href="#__codelineno-28-2"></a>length<span class="w"> </span>of<span class="w"> </span>key-value<span class="w"> </span>cache<span class="w"> </span><span class="m">20</span>
</span><span id="__span-28-3"><a id="__codelineno-28-3" name="__codelineno-28-3" href="#__codelineno-28-3"></a>shape<span class="w"> </span>of<span class="w"> </span>input_ids<span class="w"> </span>torch.Size<span class="o">([</span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span><span class="o">])</span>
</span><span id="__span-28-4"><a id="__codelineno-28-4" name="__codelineno-28-4" href="#__codelineno-28-4"></a>length<span class="w"> </span>of<span class="w"> </span>key-value<span class="w"> </span>cache<span class="w"> </span><span class="m">21</span>
</span><span id="__span-28-5"><a id="__codelineno-28-5" name="__codelineno-28-5" href="#__codelineno-28-5"></a>shape<span class="w"> </span>of<span class="w"> </span>input_ids<span class="w"> </span>torch.Size<span class="o">([</span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span><span class="o">])</span>
</span><span id="__span-28-6"><a id="__codelineno-28-6" name="__codelineno-28-6" href="#__codelineno-28-6"></a>length<span class="w"> </span>of<span class="w"> </span>key-value<span class="w"> </span>cache<span class="w"> </span><span class="m">22</span>
</span><span id="__span-28-7"><a id="__codelineno-28-7" name="__codelineno-28-7" href="#__codelineno-28-7"></a>shape<span class="w"> </span>of<span class="w"> </span>input_ids<span class="w"> </span>torch.Size<span class="o">([</span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span><span class="o">])</span>
</span><span id="__span-28-8"><a id="__codelineno-28-8" name="__codelineno-28-8" href="#__codelineno-28-8"></a>length<span class="w"> </span>of<span class="w"> </span>key-value<span class="w"> </span>cache<span class="w"> </span><span class="m">23</span>
</span><span id="__span-28-9"><a id="__codelineno-28-9" name="__codelineno-28-9" href="#__codelineno-28-9"></a>shape<span class="w"> </span>of<span class="w"> </span>input_ids<span class="w"> </span>torch.Size<span class="o">([</span><span class="m">1</span>,<span class="w"> </span><span class="m">1</span><span class="o">])</span>
</span><span id="__span-28-10"><a id="__codelineno-28-10" name="__codelineno-28-10" href="#__codelineno-28-10"></a>length<span class="w"> </span>of<span class="w"> </span>key-value<span class="w"> </span>cache<span class="w"> </span><span class="m">24</span>
</span><span id="__span-28-11"><a id="__codelineno-28-11" name="__codelineno-28-11" href="#__codelineno-28-11"></a><span class="o">[</span><span class="s1">&#39; Here&#39;</span>,<span class="w"> </span><span class="s1">&#39; is&#39;</span>,<span class="w"> </span><span class="s1">&#39; a&#39;</span>,<span class="w"> </span><span class="s1">&#39; Python&#39;</span>,<span class="w"> </span><span class="s1">&#39; function&#39;</span><span class="o">]</span>
</span></code></pre></div></p>
<p>키-값 캐시를 사용할 때, 텍스트 입력 토큰의 길이는 <em>증가하지 않고</em> 단일 입력 벡터로 유지되는 것을 볼 수 있습니다. 반면에 키-값 캐시의 길이는 각 디코딩 단계마다 하나씩 증가합니다.</p>
<blockquote>
<p>키-값 캐시를 사용하면 \( \mathbf{QK}^T \)가 본질적으로 \( \mathbf{q}_c\mathbf{K}^T \)로 줄어드는데, 여기서 \( \mathbf{q}_c \)는 현재 전달된 입력 토큰의 쿼리 프로젝션으로, <em>항상</em> 단일 벡터입니다.</p>
</blockquote>
<p>키-값 캐시를 사용하는 것에는 두 가지 장점이 있습니다:
-   전체 \( \mathbf{QK}^T \) 행렬을 계산하는 것과 비교하여 계산 효율성이 크게 향상됩니다. 이는 추론 속도의 증가로 이어집니다.
-   생성된 토큰 수에 따라 필요한 최대 메모리가 이차적으로 증가하지 않고, 선형적으로만 증가합니다.</p>
<blockquote>
<p>더 긴 입력 시퀀스에 대해 동일한 결과와 큰 속도 향상을 가져오기 때문에 키-값 캐시를 <em>항상</em> 사용해야 합니다. Transformers는 텍스트 파이프라인이나 <a href="https://huggingface.co/docs/transformers/main_classes/text_generation"><code>generate</code> 메서드</a>를 사용할 때 기본적으로 키-값 캐시를 활성화합니다.</p>
</blockquote>
<p><Tip warning={true}></p>
<p>참고로, 키-값 캐시를 사용할 것을 권장하지만, 이를 사용할 때 LLM 출력이 약간 다를 수 있습니다. 이것은 행렬 곱셈 커널 자체의 특성 때문입니다 -- 더 자세한 내용은 <a href="https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535">여기</a>에서 읽어볼 수 있습니다.</p>
<p></Tip></p>
<h4 id="321-321-multi-round-conversation">3.2.1 멀티 라운드 대화 [[321-multi-round-conversation]]</h4>
<p>키-값 캐시는 여러 번의 자기회귀 디코딩이 필요한 채팅과 같은 애플리케이션에 특히 유용합니다. 예제를 살펴보겠습니다.</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-29-1"><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a>User:<span class="w"> </span>How<span class="w"> </span>many<span class="w"> </span>people<span class="w"> </span>live<span class="w"> </span><span class="k">in</span><span class="w"> </span>France?
</span><span id="__span-29-2"><a id="__codelineno-29-2" name="__codelineno-29-2" href="#__codelineno-29-2"></a>Assistant:<span class="w"> </span>Roughly<span class="w"> </span><span class="m">75</span><span class="w"> </span>million<span class="w"> </span>people<span class="w"> </span>live<span class="w"> </span><span class="k">in</span><span class="w"> </span>France
</span><span id="__span-29-3"><a id="__codelineno-29-3" name="__codelineno-29-3" href="#__codelineno-29-3"></a>User:<span class="w"> </span>And<span class="w"> </span>how<span class="w"> </span>many<span class="w"> </span>are<span class="w"> </span><span class="k">in</span><span class="w"> </span>Germany?
</span><span id="__span-29-4"><a id="__codelineno-29-4" name="__codelineno-29-4" href="#__codelineno-29-4"></a>Assistant:<span class="w"> </span>Germany<span class="w"> </span>has<span class="w"> </span>ca.<span class="w"> </span><span class="m">81</span><span class="w"> </span>million<span class="w"> </span>inhabitants
</span></code></pre></div>
<p>이 채팅에서 대규모 언어 모델은 두 번의 자기회귀 디코딩을 실행합니다:
  1. 첫 번째로, 키-값 캐시는 비어 있고 입력 프롬프트는 <code>"User: How many people live in France?"</code>입니다. 모델은 자기회귀적으로 <code>"Roughly 75 million people live in France"</code>라는 텍스트를 생성하며 디코딩 단계마다 키-값 캐시를 증가시킵니다.
  2. 두 번째로, 입력 프롬프트는 <code>"User: How many people live in France? \n Assistant: Roughly 75 million people live in France \n User: And how many in Germany?"</code>입니다. 캐시 덕분에 첫 번째 두 문장에 대한 모든 키-값 벡터는 이미 계산되어 있습니다. 따라서 입력 프롬프트는 <code>"User: And how many in Germany?"</code>로만 구성됩니다. 줄어든 입력 프롬프트를 처리하는 동안 계산된 키-값 벡터가 첫 번째 디코딩의 키-값 캐시에 연결됩니다. 두 번째 어시스턴트의 답변인 <code>"Germany has ca. 81 million inhabitants"</code>는 <code>"User: How many people live in France? \n Assistant: Roughly 75 million people live in France \n User: And how many are in Germany?"</code>의 인코딩된 키-값 벡터로 구성된 키-값 캐시를 사용하여 자기회귀적으로 생성됩니다.</p>
<p>여기서 두 가지를 주목해야 합니다:
  1. 대규모 언어 모델이 대화의 모든 이전 문맥을 이해할 수 있도록 모든 문맥을 유지하는 것이 채팅에 배포된 대규모 언어 모델에서는 매우 중요합니다. 예를 들어, 위의 예에서 대규모 언어 모델은 사용자가 <code>"And how many are in Germany"</code>라고 물을 때 인구를 언급하고 있음을 이해해야 합니다.
  2. 키-값 캐시는 채팅에서 매우 유용합니다. 이는 인코딩된 채팅 기록을 처음부터 다시 인코딩할 필요 없이 계속해서 확장할 수 있게 해주기 때문입니다(예: 인코더-디코더 아키텍처를 사용할 때와 같은 경우).</p>
<p><code>transformers</code>에서 <code>generate</code> 호출은 기본적으로 <code>use_cache=True</code>와 함께 <code>return_dict_in_generate=True</code>를 전달하면 <code>past_key_values</code>를 반환합니다. 이는 아직 <code>pipeline</code> 인터페이스를 통해서는 사용할 수 없습니다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-30-1"><a id="__codelineno-30-1" name="__codelineno-30-1" href="#__codelineno-30-1"></a><span class="c1"># 일반적인 생성</span>
</span><span id="__span-30-2"><a id="__codelineno-30-2" name="__codelineno-30-2" href="#__codelineno-30-2"></a><span class="n">prompt</span> <span class="o">=</span> <span class="n">system_prompt</span> <span class="o">+</span> <span class="s2">&quot;Question: Please write a function in Python that transforms bytes to Giga bytes.</span><span class="se">\n\n</span><span class="s2">Answer: Here&quot;</span>
</span><span id="__span-30-3"><a id="__codelineno-30-3" name="__codelineno-30-3" href="#__codelineno-30-3"></a><span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
</span><span id="__span-30-4"><a id="__codelineno-30-4" name="__codelineno-30-4" href="#__codelineno-30-4"></a><span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span> <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span><span id="__span-30-5"><a id="__codelineno-30-5" name="__codelineno-30-5" href="#__codelineno-30-5"></a><span class="n">decoded_output</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generation_output</span><span class="o">.</span><span class="n">sequences</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
</span><span id="__span-30-6"><a id="__codelineno-30-6" name="__codelineno-30-6" href="#__codelineno-30-6"></a>
</span><span id="__span-30-7"><a id="__codelineno-30-7" name="__codelineno-30-7" href="#__codelineno-30-7"></a><span class="c1"># 리턴된 `past_key_values`를 파이프라인화하여 다음 대화 라운드를 가속화</span>
</span><span id="__span-30-8"><a id="__codelineno-30-8" name="__codelineno-30-8" href="#__codelineno-30-8"></a><span class="n">prompt</span> <span class="o">=</span> <span class="n">decoded_output</span> <span class="o">+</span> <span class="s2">&quot;</span><span class="se">\n</span><span class="s2">Question: How can I modify the function above to return Mega bytes instead?</span><span class="se">\n\n</span><span class="s2">Answer: Here&quot;</span>
</span><span id="__span-30-9"><a id="__codelineno-30-9" name="__codelineno-30-9" href="#__codelineno-30-9"></a><span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">prompt</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
</span><span id="__span-30-10"><a id="__codelineno-30-10" name="__codelineno-30-10" href="#__codelineno-30-10"></a><span class="n">generation_output</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span>
</span><span id="__span-30-11"><a id="__codelineno-30-11" name="__codelineno-30-11" href="#__codelineno-30-11"></a>  <span class="o">**</span><span class="n">model_inputs</span><span class="p">,</span>
</span><span id="__span-30-12"><a id="__codelineno-30-12" name="__codelineno-30-12" href="#__codelineno-30-12"></a>  <span class="n">past_key_values</span><span class="o">=</span><span class="n">generation_output</span><span class="o">.</span><span class="n">past_key_values</span><span class="p">,</span>
</span><span id="__span-30-13"><a id="__codelineno-30-13" name="__codelineno-30-13" href="#__codelineno-30-13"></a>  <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">60</span><span class="p">,</span>
</span><span id="__span-30-14"><a id="__codelineno-30-14" name="__codelineno-30-14" href="#__codelineno-30-14"></a>  <span class="n">return_dict_in_generate</span><span class="o">=</span><span class="kc">True</span>
</span><span id="__span-30-15"><a id="__codelineno-30-15" name="__codelineno-30-15" href="#__codelineno-30-15"></a><span class="p">)</span>
</span><span id="__span-30-16"><a id="__codelineno-30-16" name="__codelineno-30-16" href="#__codelineno-30-16"></a><span class="n">tokenizer</span><span class="o">.</span><span class="n">batch_decode</span><span class="p">(</span><span class="n">generation_output</span><span class="o">.</span><span class="n">sequences</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="nb">len</span><span class="p">(</span><span class="n">prompt</span><span class="p">):]</span>
</span></code></pre></div>
<p><strong>출력</strong>:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-31-1"><a id="__codelineno-31-1" name="__codelineno-31-1" href="#__codelineno-31-1"></a><span class="w"> </span>is<span class="w"> </span>a<span class="w"> </span>modified<span class="w"> </span>version<span class="w"> </span>of<span class="w"> </span>the<span class="w"> </span><span class="k">function</span><span class="w"> </span>that<span class="w"> </span>returns<span class="w"> </span>Mega<span class="w"> </span>bytes<span class="w"> </span>instead.
</span><span id="__span-31-2"><a id="__codelineno-31-2" name="__codelineno-31-2" href="#__codelineno-31-2"></a>
</span><span id="__span-31-3"><a id="__codelineno-31-3" name="__codelineno-31-3" href="#__codelineno-31-3"></a>def<span class="w"> </span>bytes_to_megabytes<span class="o">(</span>bytes<span class="o">)</span>:
</span><span id="__span-31-4"><a id="__codelineno-31-4" name="__codelineno-31-4" href="#__codelineno-31-4"></a><span class="w">   </span><span class="k">return</span><span class="w"> </span>bytes<span class="w"> </span>/<span class="w"> </span><span class="m">1024</span><span class="w"> </span>/<span class="w"> </span><span class="m">1024</span>
</span><span id="__span-31-5"><a id="__codelineno-31-5" name="__codelineno-31-5" href="#__codelineno-31-5"></a>
</span><span id="__span-31-6"><a id="__codelineno-31-6" name="__codelineno-31-6" href="#__codelineno-31-6"></a>Answer:<span class="w"> </span>The<span class="w"> </span><span class="k">function</span><span class="w"> </span>takes<span class="w"> </span>a<span class="w"> </span>number<span class="w"> </span>of<span class="w"> </span>bytes<span class="w"> </span>as<span class="w"> </span>input<span class="w"> </span>and<span class="w"> </span>returns<span class="w"> </span>the<span class="w"> </span>number<span class="w"> </span>of
</span></code></pre></div></p>
<p>훌륭합니다. 어텐션 층의 동일한 키와 값을 다시 계산하는 데 추가 시간이 소요되지 않습니다! 그러나 한 가지 문제가 있습니다. \( \mathbf{QK}^T \) 행렬에 필요한 최대 메모리는 크게 줄어들지만, 긴 입력 시퀀스나 다회차 채팅의 경우 키-값 캐시를 메모리에 보관하는 것이 매우 메모리 집약적이 될 수 있습니다. 키-값 캐시는 모든 자기 어텐션 층과 모든 어텐션 헤드에 대해 이전 입력 벡터 \( \mathbf{x}_i \text{, for } i \in {1, \ldots, c - 1} \)의 키-값 벡터를 저장해야 한다는 점을 기억하세요.</p>
<p>이전에 사용한 대규모 언어 모델 <code>bigcode/octocoder</code>에 대해 키-값 캐시에 저장해야 하는 부동 소수점 값의 수를 계산해 봅시다.
부동 소수점 값의 수는 시퀀스 길이의 두 배의 어텐션 헤드 수, 어텐션 헤드 차원, 레이어 수를 곱한 값입니다.
가상의 입력 시퀀스 길이 16000에서 대규모 언어 모델에 대해 이를 계산하면 다음과 같습니다.</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-32-1"><a id="__codelineno-32-1" name="__codelineno-32-1" href="#__codelineno-32-1"></a><span class="n">config</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span>
</span><span id="__span-32-2"><a id="__codelineno-32-2" name="__codelineno-32-2" href="#__codelineno-32-2"></a><span class="mi">2</span> <span class="o">*</span> <span class="mi">16_000</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">n_layer</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">n_head</span> <span class="o">*</span> <span class="n">config</span><span class="o">.</span><span class="n">n_embd</span> <span class="o">//</span> <span class="n">config</span><span class="o">.</span><span class="n">n_head</span>
</span></code></pre></div>
<p><strong>출력</strong>:
<div class="language-bash highlight"><pre><span></span><code><span id="__span-33-1"><a id="__codelineno-33-1" name="__codelineno-33-1" href="#__codelineno-33-1"></a><span class="m">7864320000</span>
</span></code></pre></div></p>
<p>대략 80억 개의 부동 소수점 값입니다! <code>float16</code> 정밀도로 80억 개의 부동 소수점 값을 저장하는 데는 약 15GB의 RAM이 필요하며, 이는 모델 가중치 자체의 절반 정도입니다.
연구자들은 키-값 캐시를 저장하는 데 필요한 메모리 비용을 크게 줄일 수 있는 두 가지 방법을 제안했으며, 이는 다음 절에서 살펴보겠습니다.</p>
<h4 id="322-mqa-322-multi-query-attention-mqa">3.2.2 멀티 쿼리 어텐션 (MQA) [[322-multi-query-attention-mqa]]</h4>
<p><a href="https://huggingface.co/papers/1911.02150">멀티 쿼리 어텐션 (MQA)</a>은 Noam Shazeer의 <em>Fast Transformer Decoding: One Write-Head is All You Need</em> 논문에서 제안되었습니다. 제목에서 알 수 있듯이, Noam은 <code>n_head</code> 키-값 프로젝션 가중치 대신, 모든 어텐션 헤드에서 공유되는 단일 헤드-값 프로젝션 가중치를 사용할 수 있으며, 이를 통해 모델 성능이 크게 저하되지 않는다는 것을 발견했습니다.</p>
<blockquote>
<p>단일 헤드-값 프로젝션 가중치를 사용함으로써, 키-값 벡터 \( \mathbf{k}_i, \mathbf{v}_i \)는 모든 어텐션 헤드에서 동일해야 하며, 이는 캐시에 <code>n_head</code> 개 대신 하나의 키-값 프로젝션 쌍만 저장하면 된다는 것을 의미합니다.</p>
</blockquote>
<p>대부분의 대규모 언어 모델이 20에서 100 사이의 어텐션 헤드를 사용하기 때문에, MQA는 키-값 캐시의 메모리 소비를 크게 줄입니다. 이 노트북에서 사용된 대규모 언어 모델의 경우, 입력 시퀀스 길이 16000에서 필요한 메모리 소비를 15GB에서 400MB 미만으로 줄일 수 있습니다.</p>
<p>메모리 절감 외에도, MQA는 계산 효율성도 향상시킵니다. 다음과 같이 설명합니다.
자기회귀 디코딩에서는 큰 키-값 벡터를 다시 로드하고, 현재 키-값 벡터 쌍과 연결한 후 \( \mathbf{q}_c\mathbf{K}^T \) 계산에 매 단계마다 입력해야 합니다. 자기회귀 디코딩의 경우, 지속적인 재로드에 필요한 메모리 대역폭이 심각한 시간 병목 현상을 가져올 수 있습니다. 키-값 벡터의 크기를 줄이면 접근해야 하는 메모리 양이 줄어들어 메모리 대역폭 병목 현상이 감소합니다. 자세한 내용은 <a href="https://huggingface.co/papers/1911.02150">Noam의 논문</a>을 참조하세요.</p>
<p>여기서 이해해야 할 중요한 부분은 키-값 어텐션 헤드 수를 1로 줄이는 것이 키-값 캐시를 사용할 때만 의미가 있다는 것입니다. 키-값 캐시 없이 단일 포워드 패스에 대한 모델의 최대 메모리 소비는 변경되지 않으며, 각 어텐션 헤드는 여전히 고유한 쿼리 벡터를 가지므로 각 어텐션 헤드는 여전히 다른 \( \mathbf{QK}^T \) 행렬을 가집니다.</p>
<p>MQA는 커뮤니티에서 널리 채택되어 현재 가장 인기 있는 많은 대규모 언어 모델에서 사용되고 있습니다.</p>
<ul>
<li><a href="https://huggingface.co/tiiuae/falcon-40b"><strong>Falcon</strong></a></li>
<li><a href="https://huggingface.co/papers/2204.02311"><strong>PaLM</strong></a></li>
<li><a href="https://huggingface.co/mosaicml/mpt-30b"><strong>MPT</strong></a></li>
<li><a href="https://huggingface.co/bigscience/bloom"><strong>BLOOM</strong></a></li>
</ul>
<p>또한, 이 노트북에서 사용된 체크포인트 <code>bigcode/octocoder</code>는 MQA를 사용합니다.</p>
<h4 id="323-gqa-323-grouped-query-attention-gqa">3.2.3 그룹 쿼리 어텐션 (GQA) [[323-grouped-query-attention-gqa]]</h4>
<p><a href="https://huggingface.co/papers/2305.13245">그룹 쿼리 어텐션 (GQA)</a>은 Google의 Ainslie 등의 연구진들에 의해 제안되었습니다. 그들은 MQA를 사용하는 것이 종종 일반적인 멀티 키-값 헤드 프로젝션을 사용하는 것보다 품질 저하를 가져올 수 있다는 것을 발견했습니다. 이 논문은 쿼리 헤드 프로젝션 가중치의 수를 너무 극단적으로 줄이는 대신, 더 많은 모델 성능을 유지할 수 있다고 주장합니다. 단일 키-값 프로젝션 가중치 대신, <code>n &lt; n_head</code> 키-값 프로젝션 가중치를 사용해야 합니다. <code>n_head</code>보다 훨씬 작은 <code>n</code>값, 예를 들어 2, 4 또는 8을 선택하면, MQA의 거의 모든 메모리 및 속도 이점을 유지하면서 모델 용량을 덜 희생하고 따라서 성능 저하를 줄일 수 있습니다.</p>
<p>또한, GQA의 저자들은 기존 모델 체크포인트를 원래 사전 학습 계산의 5% 정도의 적은 양으로 GQA 아키텍처로 <em>업트레이닝</em>할 수 있음을 발견했습니다. 원래 사전 학습 계산의 5%가 여전히 엄청난 양일 수 있지만, GQA <em>업트레이닝</em>은 기존 체크포인트가 더 긴 입력 시퀀스에서도 유용하도록 합니다.</p>
<p>GQA는 최근에 제안되었기 때문에 이 노트북을 작성할 당시에는 채택이 덜 되었습니다.
GQA의 가장 주목할 만한 적용 사례는 <a href="https://huggingface.co/meta-llama/Llama-2-70b-hf">Llama-v2</a>입니다.</p>
<blockquote>
<p>결론적으로, 대규모 언어 모델이 자기회귀 디코딩으로 배포되면서 채팅과 같이 큰 입력 시퀀스를 가진 작업을 처리해야 하는 경우 GQA 또는 MQA를 사용하는 것이 강력히 권장됩니다.</p>
</blockquote>
<h2 id="conclusion">결론 [[conclusion]]</h2>
<p>연구 커뮤니티는 점점 더 큰 대규모 언어 모델의 추론 시간을 가속화하기 위한 새로운 기발한 방법들을 끊임없이 찾아내고 있습니다. 예를 들어, <a href="https://huggingface.co/papers/2211.17192">추측 디코딩</a>이라는 유망한 연구 방향이 있습니다. 여기서 "쉬운 토큰"은 더 작고 빠른 언어 모델에 의해 생성되고, "어려운 토큰"만 대규모 언어 모델 자체에 의해 생성됩니다. 자세한 내용은 이 노트북의 범위를 벗어나지만, <a href="https://huggingface.co/blog/assisted-generation">멋진 블로그 포스트</a>에서 읽어볼 수 있습니다.</p>
<p>GPT3/4, Llama-2-70b, Claude, PaLM과 같은 거대한 대규모 언어 모델이 <a href="https://huggingface.co/chat/">Hugging Face Chat</a> 또는 ChatGPT와 같은 채팅 인터페이스에서 빠르게 실행될 수 있는 이유는 위에서 언급한 정밀도, 알고리즘, 아키텍처의 개선 덕분입니다. 앞으로 GPU, TPU 등과 같은 가속기는 점점 더 빨라지고 더 많은 메모리를 사용할 것입니다. 따라서 가장 좋은 알고리즘과 아키텍처를 사용하여 최고의 효율을 얻는 것이 중요합니다 🤗</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.instant", "navigation.sections", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow", "content.tabs.link", "content.code.copy"], "search": "../../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>