
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      
      
      <link rel="icon" href="../../../../../../../assets/favicon.ico">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.22">
    
    
      
        <title>Quantization - Ohayou</title>
      
    
    
      <link rel="stylesheet" href="../../../../../../../assets/stylesheets/main.84d31ad4.min.css">
      
        
        <link rel="stylesheet" href="../../../../../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="../../../../../../../assets/extra.css">
    
    <script>__md_scope=new URL("../../../../../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#transformers" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../../../../../.." title="Ohayou" class="md-header__button md-logo" aria-label="Ohayou" data-md-component="logo">
      
  <img src="../../../../../../../assets/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Ohayou
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Quantization
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
          <a href="javascript:void(0)" class="md-search__icon md-icon" title="Share" aria-label="Share" data-clipboard data-clipboard-text="" data-md-component="search-share" tabindex="-1">
            
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"/></svg>
          </a>
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
        <div class="md-search__suggest" data-md-component="search-suggest"></div>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../../ohayou/" class="md-tabs__link">
        
  
  
    
  
  Ohayou

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../.." class="md-tabs__link">
        
  
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../../../vllm/open_ai_vllm_example_a_v_t/" class="md-tabs__link">
          
  
  
    
  
  vLLM

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../../../llm/speculative_decoding/" class="md-tabs__link">
          
  
  
    
  
  LLM

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../../../vlm/qwen3_vl_4B_object_detection/" class="md-tabs__link">
          
  
  
    
  
  VLM

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../../md_format_helpers/" class="md-tabs__link">
        
  
  
    
  
  MD format helpers

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../../docker/" class="md-tabs__link">
        
  
  
    
  
  Docker

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../../linux/" class="md-tabs__link">
        
  
  
    
  
  Linux

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../../moe/" class="md-tabs__link">
        
  
  
    
  
  Mixture of Experts

      </a>
    </li>
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../../slurm/" class="md-tabs__link">
        
  
  
    
  
  Slurm

      </a>
    </li>
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../../../../../japanese-phrases/" class="md-tabs__link">
          
  
  
    
  
  Japanese Phrases

        </a>
      </li>
    
  

      
        
  
  
  
  
    <li class="md-tabs__item">
      <a href="../../../../../../../hackathon/index.md" class="md-tabs__link">
        
  
  
    
  
  Hack

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../../../../../.." title="Ohayou" class="md-nav__button md-logo" aria-label="Ohayou" data-md-component="logo">
      
  <img src="../../../../../../../assets/logo.png" alt="logo">

    </a>
    Ohayou
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../ohayou/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Ohayou
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../.." class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" >
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    vLLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            vLLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../vllm/open_ai_vllm_example_a_v_t/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Single Request
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../vllm/bash_vllm_serve/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Bash online serve
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../vllm/benchmarks/performance_eval/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Benchmarks
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    LLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            LLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../llm/speculative_decoding/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Speculative Decoding
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  
  <span class="md-ellipsis">
    VLM
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            VLM
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../vlm/qwen3_vl_4B_object_detection/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Qwen3VL_adema_grounding
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../vlm/qwen3_vla_4B_audio_training_aspandiyar/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Qwen3VLA_aspandiyar_thinking
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../md_format_helpers/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MD format helpers
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../docker/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Docker
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../linux/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Linux
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../moe/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Mixture of Experts
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../slurm/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Slurm
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../../../japanese-phrases/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Japanese Phrases
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11" id="__nav_11_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_11_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11">
            <span class="md-nav__icon md-icon"></span>
            Japanese Phrases
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11_2" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../../../japanese-phrases/daily-life/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Daily Life
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11_2" id="__nav_11_2_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_11_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11_2">
            <span class="md-nav__icon md-icon"></span>
            Daily Life
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../japanese-phrases/daily-life/shopping/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Shopping
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_11_3" >
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../../../../../../japanese-phrases/greetings/" class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Greetings
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_11_3" id="__nav_11_3_label" tabindex="0">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_11_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_11_3">
            <span class="md-nav__icon md-icon"></span>
            Greetings
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../japanese-phrases/greetings/casual/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Casual
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../japanese-phrases/emotions/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Emotions
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../japanese-phrases/anime-manga/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Anime/Manga
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../../../../../../../hackathon/index.md" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Hack
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#awq" class="md-nav__link">
    <span class="md-ellipsis">
      AWQé›†æˆ
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AWQé›†æˆ">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      é‡åŒ–ä¸€ä¸ªæ¨¡å‹
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      åŠ è½½ä¸€ä¸ªé‡åŒ–çš„æ¨¡å‹
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      ç¤ºä¾‹ä½¿ç”¨
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ç¤ºä¾‹ä½¿ç”¨">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#awq-flash-attention" class="md-nav__link">
    <span class="md-ellipsis">
      ç»“åˆ AWQ å’Œ Flash Attention
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_4" class="md-nav__link">
    <span class="md-ellipsis">
      åŸºå‡†æµ‹è¯•
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#google-colab" class="md-nav__link">
    <span class="md-ellipsis">
      Google colab æ¼”ç¤º
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#awqconfig" class="md-nav__link">
    <span class="md-ellipsis">
      AwqConfig
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autogptq" class="md-nav__link">
    <span class="md-ellipsis">
      AutoGPTQ é›†æˆ
    </span>
  </a>
  
    <nav class="md-nav" aria-label="AutoGPTQ é›†æˆ">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_5" class="md-nav__link">
    <span class="md-ellipsis">
      è¦æ±‚
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_6" class="md-nav__link">
    <span class="md-ellipsis">
      åŠ è½½å’Œé‡åŒ–æ¨¡å‹
    </span>
  </a>
  
    <nav class="md-nav" aria-label="åŠ è½½å’Œé‡åŒ–æ¨¡å‹">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gptq" class="md-nav__link">
    <span class="md-ellipsis">
      GPTQ é…ç½®
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_7" class="md-nav__link">
    <span class="md-ellipsis">
      é‡åŒ–
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hub" class="md-nav__link">
    <span class="md-ellipsis">
      æ¨é€é‡åŒ–æ¨¡å‹åˆ° ğŸ¤— Hub
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hub_1" class="md-nav__link">
    <span class="md-ellipsis">
      ä» ğŸ¤— Hub åŠ è½½ä¸€ä¸ªé‡åŒ–æ¨¡å‹
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exllama" class="md-nav__link">
    <span class="md-ellipsis">
      Exllamaå†…æ ¸åŠ å¿«æ¨ç†é€Ÿåº¦
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Exllamaå†…æ ¸åŠ å¿«æ¨ç†é€Ÿåº¦">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_8" class="md-nav__link">
    <span class="md-ellipsis">
      å¾®è°ƒä¸€ä¸ªé‡åŒ–æ¨¡å‹
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_9" class="md-nav__link">
    <span class="md-ellipsis">
      ç¤ºä¾‹æ¼”ç¤º
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gptqconfig" class="md-nav__link">
    <span class="md-ellipsis">
      GPTQConfig
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bitsandbytes" class="md-nav__link">
    <span class="md-ellipsis">
      bitsandbytes é›†æˆ
    </span>
  </a>
  
    <nav class="md-nav" aria-label="bitsandbytes é›†æˆ">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_10" class="md-nav__link">
    <span class="md-ellipsis">
      é€šç”¨ç”¨æ³•
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#fp4" class="md-nav__link">
    <span class="md-ellipsis">
      FP4 é‡åŒ–
    </span>
  </a>
  
    <nav class="md-nav" aria-label="FP4 é‡åŒ–">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_11" class="md-nav__link">
    <span class="md-ellipsis">
      è¦æ±‚
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_12" class="md-nav__link">
    <span class="md-ellipsis">
      æç¤ºå’Œæœ€ä½³å®è·µ
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#4" class="md-nav__link">
    <span class="md-ellipsis">
      åŠ è½½ 4 ä½é‡åŒ–çš„å¤§æ¨¡å‹
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#8" class="md-nav__link">
    <span class="md-ellipsis">
      åŠ è½½ 8 ä½é‡åŒ–çš„å¤§æ¨¡å‹
    </span>
  </a>
  
    <nav class="md-nav" aria-label="åŠ è½½ 8 ä½é‡åŒ–çš„å¤§æ¨¡å‹">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_13" class="md-nav__link">
    <span class="md-ellipsis">
      é«˜çº§ç”¨ä¾‹
    </span>
  </a>
  
    <nav class="md-nav" aria-label="é«˜çº§ç”¨ä¾‹">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_14" class="md-nav__link">
    <span class="md-ellipsis">
      æ›´æ”¹è®¡ç®—æ•°æ®ç±»å‹
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nf4-4" class="md-nav__link">
    <span class="md-ellipsis">
      ä½¿ç”¨ NF4ï¼ˆæ™®é€šæµ®ç‚¹æ•° 4ï¼‰æ•°æ®ç±»å‹
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_15" class="md-nav__link">
    <span class="md-ellipsis">
      ä½¿ç”¨åµŒå¥—é‡åŒ–è¿›è¡Œæ›´é«˜æ•ˆçš„å†…å­˜æ¨ç†
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hub_2" class="md-nav__link">
    <span class="md-ellipsis">
      å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ°ğŸ¤— Hub
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hub_3" class="md-nav__link">
    <span class="md-ellipsis">
      ä»ğŸ¤— HubåŠ è½½é‡åŒ–æ¨¡å‹
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_16" class="md-nav__link">
    <span class="md-ellipsis">
      é«˜çº§ç”¨ä¾‹
    </span>
  </a>
  
    <nav class="md-nav" aria-label="é«˜çº§ç”¨ä¾‹">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-gpu" class="md-nav__link">
    <span class="md-ellipsis">
      åœ¨ cpu å’Œ gpu ä¹‹é—´å¸è½½
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#llm_int8_threshold" class="md-nav__link">
    <span class="md-ellipsis">
      ä½¿ç”¨llm_int8_threshold
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_17" class="md-nav__link">
    <span class="md-ellipsis">
      è·³è¿‡æŸäº›æ¨¡å—çš„è½¬æ¢
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#8_1" class="md-nav__link">
    <span class="md-ellipsis">
      å¾®è°ƒå·²åŠ è½½ä¸º8ä½ç²¾åº¦çš„æ¨¡å‹
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bitsandbytesconfig" class="md-nav__link">
    <span class="md-ellipsis">
      BitsAndBytesConfig
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#optimum" class="md-nav__link">
    <span class="md-ellipsis">
      ä½¿ç”¨ ğŸ¤— optimum è¿›è¡Œé‡åŒ–
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<!--Copyright 2023 The HuggingFace Team. All rights reserved.

Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except in compliance with
the License. You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on
an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the
specific language governing permissions and limitations under the License.

âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be
rendered properly in your Markdown viewer.

-->

<h1 id="transformers">é‡åŒ– ğŸ¤— Transformers æ¨¡å‹</h1>
<h2 id="awq">AWQé›†æˆ</h2>
<p>AWQæ–¹æ³•å·²ç»åœ¨<a href="https://huggingface.co/papers/2306.00978"><em>AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration</em>è®ºæ–‡</a>ä¸­å¼•å…¥ã€‚é€šè¿‡AWQï¼Œæ‚¨å¯ä»¥ä»¥4ä½ç²¾åº¦è¿è¡Œæ¨¡å‹ï¼ŒåŒæ—¶ä¿ç•™å…¶åŸå§‹æ€§èƒ½ï¼ˆå³æ²¡æœ‰æ€§èƒ½é™çº§ï¼‰ï¼Œå¹¶å…·æœ‰æ¯”ä¸‹é¢ä»‹ç»çš„å…¶ä»–é‡åŒ–æ–¹æ³•æ›´å‡ºè‰²çš„ååé‡ - è¾¾åˆ°ä¸çº¯<code>float16</code>æ¨ç†ç›¸ä¼¼çš„ååé‡ã€‚</p>
<p>æˆ‘ä»¬ç°åœ¨æ”¯æŒä½¿ç”¨ä»»ä½•AWQæ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œè¿™æ„å‘³ç€ä»»ä½•äººéƒ½å¯ä»¥åŠ è½½å’Œä½¿ç”¨åœ¨Hubä¸Šæ¨é€æˆ–æœ¬åœ°ä¿å­˜çš„AWQæƒé‡ã€‚è¯·æ³¨æ„ï¼Œä½¿ç”¨AWQéœ€è¦è®¿é—®NVIDIA GPUã€‚ç›®å‰ä¸æ”¯æŒCPUæ¨ç†ã€‚</p>
<h3 id="_1">é‡åŒ–ä¸€ä¸ªæ¨¡å‹</h3>
<p>æˆ‘ä»¬å»ºè®®ç”¨æˆ·æŸ¥çœ‹ç”Ÿæ€ç³»ç»Ÿä¸­ä¸åŒçš„ç°æœ‰å·¥å…·ï¼Œä»¥ä½¿ç”¨AWQç®—æ³•å¯¹å…¶æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œä¾‹å¦‚ï¼š</p>
<ul>
<li><a href="https://github.com/mit-han-lab/llm-awq"><code>llm-awq</code></a>ï¼Œæ¥è‡ªMIT Han Lab</li>
<li><a href="https://github.com/casper-hansen/AutoAWQ"><code>autoawq</code></a>ï¼Œæ¥è‡ª<a href="https://github.com/casper-hansen"><code>casper-hansen</code></a></li>
<li>Intel neural compressorï¼Œæ¥è‡ªIntel - é€šè¿‡<a href="https://huggingface.co/docs/optimum/main/en/intel/optimization_inc"><code>optimum-intel</code></a>ä½¿ç”¨</li>
</ul>
<p>ç”Ÿæ€ç³»ç»Ÿä¸­å¯èƒ½å­˜åœ¨è®¸å¤šå…¶ä»–å·¥å…·ï¼Œè¯·éšæ—¶æå‡ºPRå°†å®ƒä»¬æ·»åŠ åˆ°åˆ—è¡¨ä¸­ã€‚
ç›®å‰ä¸ğŸ¤— Transformersçš„é›†æˆä»…é€‚ç”¨äºä½¿ç”¨<code>autoawq</code>å’Œ<code>llm-awq</code>é‡åŒ–åçš„æ¨¡å‹ã€‚å¤§å¤šæ•°ä½¿ç”¨<code>auto-awq</code>é‡åŒ–çš„æ¨¡å‹å¯ä»¥åœ¨ğŸ¤— Hubçš„<a href="https://huggingface.co/TheBloke"><code>TheBloke</code></a>å‘½åç©ºé—´ä¸‹æ‰¾åˆ°ï¼Œè¦ä½¿ç”¨<code>llm-awq</code>å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œè¯·å‚é˜…<a href="https://github.com/mit-han-lab/llm-awq/"><code>llm-awq</code></a>çš„ç¤ºä¾‹æ–‡ä»¶å¤¹ä¸­çš„<a href="https://github.com/mit-han-lab/llm-awq/blob/main/examples/convert_to_hf.py"><code>convert_to_hf.py</code></a>è„šæœ¬ã€‚</p>
<h3 id="_2">åŠ è½½ä¸€ä¸ªé‡åŒ–çš„æ¨¡å‹</h3>
<p>æ‚¨å¯ä»¥ä½¿ç”¨<code>from_pretrained</code>æ–¹æ³•ä»HubåŠ è½½ä¸€ä¸ªé‡åŒ–æ¨¡å‹ã€‚é€šè¿‡æ£€æŸ¥æ¨¡å‹é…ç½®æ–‡ä»¶ï¼ˆ<code>configuration.json</code>ï¼‰ä¸­æ˜¯å¦å­˜åœ¨<code>quantization_config</code>å±æ€§ï¼Œæ¥è¿›è¡Œç¡®è®¤æ¨é€çš„æƒé‡æ˜¯é‡åŒ–çš„ã€‚æ‚¨å¯ä»¥é€šè¿‡æ£€æŸ¥å­—æ®µ<code>quantization_config.quant_method</code>æ¥ç¡®è®¤æ¨¡å‹æ˜¯å¦ä»¥AWQæ ¼å¼è¿›è¡Œé‡åŒ–ï¼Œè¯¥å­—æ®µåº”è¯¥è®¾ç½®ä¸º<code>"awq"</code>ã€‚è¯·æ³¨æ„ï¼Œä¸ºäº†æ€§èƒ½åŸå› ï¼Œé»˜è®¤æƒ…å†µä¸‹åŠ è½½æ¨¡å‹å°†è®¾ç½®å…¶ä»–æƒé‡ä¸º<code>float16</code>ã€‚å¦‚æœæ‚¨æƒ³æ›´æ”¹è¿™ç§è®¾ç½®ï¼Œå¯ä»¥é€šè¿‡å°†<code>dtype</code>å‚æ•°è®¾ç½®ä¸º<code>torch.float32</code>æˆ–<code>torch.bfloat16</code>ã€‚åœ¨ä¸‹é¢çš„éƒ¨åˆ†ä¸­ï¼Œæ‚¨å¯ä»¥æ‰¾åˆ°ä¸€äº›ç¤ºä¾‹ç‰‡æ®µå’Œnotebookã€‚</p>
<h2 id="_3">ç¤ºä¾‹ä½¿ç”¨</h2>
<p>é¦–å…ˆï¼Œæ‚¨éœ€è¦å®‰è£…<a href="https://github.com/casper-hansen/AutoAWQ"><code>autoawq</code></a>åº“</p>
<div class="language-bash highlight"><pre><span></span><code><span id="__span-0-1"><a id="__codelineno-0-1" name="__codelineno-0-1" href="#__codelineno-0-1"></a>pip<span class="w"> </span>install<span class="w"> </span>autoawq
</span></code></pre></div>
<div class="language-python highlight"><pre><span></span><code><span id="__span-1-1"><a id="__codelineno-1-1" name="__codelineno-1-1" href="#__codelineno-1-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span><span id="__span-1-2"><a id="__codelineno-1-2" name="__codelineno-1-2" href="#__codelineno-1-2"></a>
</span><span id="__span-1-3"><a id="__codelineno-1-3" name="__codelineno-1-3" href="#__codelineno-1-3"></a><span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;TheBloke/zephyr-7B-alpha-AWQ&quot;</span>
</span><span id="__span-1-4"><a id="__codelineno-1-4" name="__codelineno-1-4" href="#__codelineno-1-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>å¦‚æœæ‚¨é¦–å…ˆå°†æ¨¡å‹åŠ è½½åˆ°CPUä¸Šï¼Œè¯·ç¡®ä¿åœ¨ä½¿ç”¨ä¹‹å‰å°†å…¶ç§»åŠ¨åˆ°GPUè®¾å¤‡ä¸Šã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-2-1"><a id="__codelineno-2-1" name="__codelineno-2-1" href="#__codelineno-2-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span><span id="__span-2-2"><a id="__codelineno-2-2" name="__codelineno-2-2" href="#__codelineno-2-2"></a>
</span><span id="__span-2-3"><a id="__codelineno-2-3" name="__codelineno-2-3" href="#__codelineno-2-3"></a><span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;TheBloke/zephyr-7B-alpha-AWQ&quot;</span>
</span><span id="__span-2-4"><a id="__codelineno-2-4" name="__codelineno-2-4" href="#__codelineno-2-4"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="awq-flash-attention">ç»“åˆ AWQ å’Œ Flash Attention</h3>
<p>æ‚¨å¯ä»¥å°†AWQé‡åŒ–ä¸Flash Attentionç»“åˆèµ·æ¥ï¼Œå¾—åˆ°ä¸€ä¸ªæ—¢è¢«é‡åŒ–åˆæ›´å¿«é€Ÿçš„æ¨¡å‹ã€‚åªéœ€ä½¿ç”¨<code>from_pretrained</code>åŠ è½½æ¨¡å‹ï¼Œå¹¶ä¼ é€’<code>attn_implementation="flash_attention_2"</code>å‚æ•°ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-3-1"><a id="__codelineno-3-1" name="__codelineno-3-1" href="#__codelineno-3-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span><span id="__span-3-2"><a id="__codelineno-3-2" name="__codelineno-3-2" href="#__codelineno-3-2"></a>
</span><span id="__span-3-3"><a id="__codelineno-3-3" name="__codelineno-3-3" href="#__codelineno-3-3"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;TheBloke/zephyr-7B-alpha-AWQ&quot;</span><span class="p">,</span> <span class="n">attn_implementation</span><span class="o">=</span><span class="s2">&quot;flash_attention_2&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;cuda:0&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="_4">åŸºå‡†æµ‹è¯•</h3>
<p>æˆ‘ä»¬ä½¿ç”¨<a href="https://github.com/huggingface/optimum-benchmark"><code>optimum-benchmark</code></a>åº“è¿›è¡Œäº†ä¸€äº›é€Ÿåº¦ã€ååé‡å’Œå»¶è¿ŸåŸºå‡†æµ‹è¯•ã€‚</p>
<p>è¯·æ³¨æ„ï¼Œåœ¨ç¼–å†™æœ¬æ–‡æ¡£éƒ¨åˆ†æ—¶ï¼Œå¯ç”¨çš„é‡åŒ–æ–¹æ³•åŒ…æ‹¬ï¼š<code>awq</code>ã€<code>gptq</code>å’Œ<code>bitsandbytes</code>ã€‚</p>
<p>åŸºå‡†æµ‹è¯•åœ¨ä¸€å°NVIDIA-A100å®ä¾‹ä¸Šè¿è¡Œï¼Œä½¿ç”¨<a href="https://huggingface.co/TheBloke/Mistral-7B-v0.1-AWQ"><code>TheBloke/Mistral-7B-v0.1-AWQ</code></a>ä½œä¸ºAWQæ¨¡å‹ï¼Œ<a href="https://huggingface.co/TheBloke/Mistral-7B-v0.1-GPTQ"><code>TheBloke/Mistral-7B-v0.1-GPTQ</code></a>ä½œä¸ºGPTQæ¨¡å‹ã€‚æˆ‘ä»¬è¿˜å°†å…¶ä¸<code>bitsandbytes</code>é‡åŒ–æ¨¡å‹å’Œ<code>float16</code>æ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ç»“æœç¤ºä¾‹ï¼š</p>
<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/forward_memory_plot.png">
</div>

<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/generate_memory_plot.png">
</div>

<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/generate_throughput_plot.png">
</div>

<div style="text-align: center">
<img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/forward_latency_plot.png">
</div>

<p>ä½ å¯ä»¥åœ¨<a href="https://github.com/huggingface/optimum-benchmark/tree/main/examples/running-mistrals">æ­¤é“¾æ¥</a>ä¸­æ‰¾åˆ°å®Œæ•´çš„ç»“æœä»¥åŠåŒ…ç‰ˆæœ¬ã€‚</p>
<p>ä»ç»“æœæ¥çœ‹ï¼ŒAWQé‡åŒ–æ–¹æ³•æ˜¯æ¨ç†ã€æ–‡æœ¬ç”Ÿæˆä¸­æœ€å¿«çš„é‡åŒ–æ–¹æ³•ï¼Œå¹¶ä¸”åœ¨æ–‡æœ¬ç”Ÿæˆçš„å³°å€¼å†…å­˜æ–¹é¢å±äºæœ€ä½ã€‚ç„¶è€Œï¼Œå¯¹äºæ¯æ‰¹æ•°æ®ï¼ŒAWQä¼¼ä¹æœ‰æœ€å¤§çš„å‰å‘å»¶è¿Ÿã€‚</p>
<h3 id="google-colab">Google colab æ¼”ç¤º</h3>
<p>æŸ¥çœ‹å¦‚ä½•åœ¨<a href="https://colab.research.google.com/drive/1HzZH89yAXJaZgwJDhQj9LqSBux932BvY">Google Colabæ¼”ç¤º</a>ä¸­ä½¿ç”¨æ­¤é›†æˆï¼</p>
<h3 id="awqconfig">AwqConfig</h3>
<p>[[autodoc]] AwqConfig</p>
<h2 id="autogptq"><code>AutoGPTQ</code> é›†æˆ</h2>
<p>ğŸ¤— Transformerså·²ç»æ•´åˆäº†<code>optimum</code> APIï¼Œç”¨äºå¯¹è¯­è¨€æ¨¡å‹æ‰§è¡ŒGPTQé‡åŒ–ã€‚æ‚¨å¯ä»¥ä»¥8ã€4ã€3ç”šè‡³2ä½åŠ è½½å’Œé‡åŒ–æ‚¨çš„æ¨¡å‹ï¼Œè€Œæ€§èƒ½æ— æ˜æ˜¾ä¸‹é™ï¼Œå¹¶ä¸”æ¨ç†é€Ÿåº¦æ›´å¿«ï¼è¿™å—åˆ°å¤§å¤šæ•°GPUç¡¬ä»¶çš„æ”¯æŒã€‚</p>
<p>è¦äº†è§£æ›´å¤šå…³äºé‡åŒ–æ¨¡å‹çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ï¼š
- <a href="https://huggingface.co/papers/2210.17323">GPTQ</a>è®ºæ–‡
- <code>optimum</code>å…³äºGPTQé‡åŒ–çš„<a href="https://huggingface.co/docs/optimum/llm_quantization/usage_guides/quantization">æŒ‡å—</a>
- ç”¨ä½œåç«¯çš„<a href="https://github.com/PanQiWei/AutoGPTQ"><code>AutoGPTQ</code></a>åº“</p>
<h3 id="_5">è¦æ±‚</h3>
<p>ä¸ºäº†è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œæ‚¨éœ€è¦å®‰è£…ï¼š</p>
<ul>
<li>
<p>å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ <code>AutoGPTQ</code> åº“
<code>pip install auto-gptq</code></p>
</li>
<li>
<p>ä»æºä»£ç å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„<code>optimum</code>
<code>pip install git+https://github.com/huggingface/optimum.git</code></p>
</li>
<li>
<p>ä»æºä»£ç å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„<code>transformers</code>
<code>pip install git+https://github.com/huggingface/transformers.git</code></p>
</li>
<li>
<p>å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„<code>accelerate</code>åº“ï¼š 
<code>pip install --upgrade accelerate</code></p>
</li>
</ul>
<p>è¯·æ³¨æ„ï¼Œç›®å‰GPTQé›†æˆä»…æ”¯æŒæ–‡æœ¬æ¨¡å‹ï¼Œå¯¹äºè§†è§‰ã€è¯­éŸ³æˆ–å¤šæ¨¡æ€æ¨¡å‹å¯èƒ½ä¼šé‡åˆ°é¢„æœŸä»¥å¤–ç»“æœã€‚</p>
<h3 id="_6">åŠ è½½å’Œé‡åŒ–æ¨¡å‹</h3>
<p>GPTQæ˜¯ä¸€ç§åœ¨ä½¿ç”¨é‡åŒ–æ¨¡å‹ä¹‹å‰éœ€è¦è¿›è¡Œæƒé‡æ ¡å‡†çš„é‡åŒ–æ–¹æ³•ã€‚å¦‚æœæ‚¨æƒ³ä»å¤´å¼€å§‹å¯¹transformersæ¨¡å‹è¿›è¡Œé‡åŒ–ï¼Œç”Ÿæˆé‡åŒ–æ¨¡å‹å¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´ï¼ˆåœ¨Google Colabä¸Šå¯¹<code>facebook/opt-350m</code>æ¨¡å‹é‡åŒ–çº¦ä¸º5åˆ†é’Ÿï¼‰ã€‚</p>
<p>å› æ­¤ï¼Œæœ‰ä¸¤ç§ä¸åŒçš„æƒ…å†µä¸‹æ‚¨å¯èƒ½æƒ³ä½¿ç”¨GPTQé‡åŒ–æ¨¡å‹ã€‚ç¬¬ä¸€ç§æƒ…å†µæ˜¯åŠ è½½å·²ç»ç”±å…¶ä»–ç”¨æˆ·åœ¨Hubä¸Šé‡åŒ–çš„æ¨¡å‹ï¼Œç¬¬äºŒç§æƒ…å†µæ˜¯ä»å¤´å¼€å§‹å¯¹æ‚¨çš„æ¨¡å‹è¿›è¡Œé‡åŒ–å¹¶ä¿å­˜æˆ–æ¨é€åˆ°Hubï¼Œä»¥ä¾¿å…¶ä»–ç”¨æˆ·ä¹Ÿå¯ä»¥ä½¿ç”¨å®ƒã€‚</p>
<h4 id="gptq">GPTQ é…ç½®</h4>
<p>ä¸ºäº†åŠ è½½å’Œé‡åŒ–ä¸€ä¸ªæ¨¡å‹ï¼Œæ‚¨éœ€è¦åˆ›å»ºä¸€ä¸ª[<code>GPTQConfig</code>]ã€‚æ‚¨éœ€è¦ä¼ é€’<code>bits</code>çš„æ•°é‡ï¼Œä¸€ä¸ªç”¨äºæ ¡å‡†é‡åŒ–çš„<code>dataset</code>ï¼Œä»¥åŠæ¨¡å‹çš„<code>tokenizer</code>ä»¥å‡†å¤‡æ•°æ®é›†ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-4-1"><a id="__codelineno-4-1" name="__codelineno-4-1" href="#__codelineno-4-1"></a><span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;facebook/opt-125m&quot;</span>
</span><span id="__span-4-2"><a id="__codelineno-4-2" name="__codelineno-4-2" href="#__codelineno-4-2"></a><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
</span><span id="__span-4-3"><a id="__codelineno-4-3" name="__codelineno-4-3" href="#__codelineno-4-3"></a><span class="n">gptq_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="s2">&quot;c4&quot;</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</span></code></pre></div>
<p>è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥å°†è‡ªå·±çš„æ•°æ®é›†ä»¥å­—ç¬¦ä¸²åˆ—è¡¨å½¢å¼ä¼ é€’åˆ°æ¨¡å‹ã€‚ç„¶è€Œï¼Œå¼ºçƒˆå»ºè®®æ‚¨ä½¿ç”¨GPTQè®ºæ–‡ä¸­æä¾›çš„æ•°æ®é›†ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-5-1"><a id="__codelineno-5-1" name="__codelineno-5-1" href="#__codelineno-5-1"></a><span class="n">dataset</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;auto-gptq is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.&quot;</span><span class="p">]</span>
</span><span id="__span-5-2"><a id="__codelineno-5-2" name="__codelineno-5-2" href="#__codelineno-5-2"></a><span class="n">quantization</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>
</span></code></pre></div>
<h4 id="_7">é‡åŒ–</h4>
<p>æ‚¨å¯ä»¥é€šè¿‡ä½¿ç”¨<code>from_pretrained</code>å¹¶è®¾ç½®<code>quantization_config</code>æ¥å¯¹æ¨¡å‹è¿›è¡Œé‡åŒ–ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-6-1"><a id="__codelineno-6-1" name="__codelineno-6-1" href="#__codelineno-6-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
</span><span id="__span-6-2"><a id="__codelineno-6-2" name="__codelineno-6-2" href="#__codelineno-6-2"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">gptq_config</span><span class="p">)</span>
</span></code></pre></div>
<p>è¯·æ³¨æ„ï¼Œæ‚¨éœ€è¦ä¸€ä¸ªGPUæ¥é‡åŒ–æ¨¡å‹ã€‚æˆ‘ä»¬å°†æ¨¡å‹æ”¾åœ¨cpuä¸­ï¼Œå¹¶å°†æ¨¡å—æ¥å›ç§»åŠ¨åˆ°gpuä¸­ï¼Œä»¥ä¾¿å¯¹å…¶è¿›è¡Œé‡åŒ–ã€‚</p>
<p>å¦‚æœæ‚¨æƒ³åœ¨ä½¿ç”¨ CPU å¸è½½çš„åŒæ—¶æœ€å¤§åŒ– GPU ä½¿ç”¨ç‡ï¼Œæ‚¨å¯ä»¥è®¾ç½® <code>device_map = "auto"</code>ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-7-1"><a id="__codelineno-7-1" name="__codelineno-7-1" href="#__codelineno-7-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
</span><span id="__span-7-2"><a id="__codelineno-7-2" name="__codelineno-7-2" href="#__codelineno-7-2"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">gptq_config</span><span class="p">)</span>
</span></code></pre></div>
<p>è¯·æ³¨æ„ï¼Œä¸æ”¯æŒç£ç›˜å¸è½½ã€‚æ­¤å¤–ï¼Œå¦‚æœç”±äºæ•°æ®é›†è€Œå†…å­˜ä¸è¶³ï¼Œæ‚¨å¯èƒ½éœ€è¦åœ¨<code>from_pretrained</code>ä¸­è®¾ç½®<code>max_memory</code>ã€‚æŸ¥çœ‹è¿™ä¸ª<a href="https://huggingface.co/docs/accelerate/usage_guides/big_modeling#designing-a-device-map">æŒ‡å—</a>ä»¥äº†è§£æœ‰å…³<code>device_map</code>å’Œ<code>max_memory</code>çš„æ›´å¤šä¿¡æ¯ã€‚</p>
<p><Tip warning={true}>
ç›®å‰ï¼ŒGPTQé‡åŒ–ä»…é€‚ç”¨äºæ–‡æœ¬æ¨¡å‹ã€‚æ­¤å¤–ï¼Œé‡åŒ–è¿‡ç¨‹å¯èƒ½ä¼šèŠ±è´¹å¾ˆå¤šæ—¶é—´ï¼Œå…·ä½“å–å†³äºç¡¬ä»¶æ€§èƒ½ï¼ˆ175Bæ¨¡å‹åœ¨NVIDIA A100ä¸Šéœ€è¦4å°æ—¶ï¼‰ã€‚è¯·åœ¨Hubä¸Šæ£€æŸ¥æ˜¯å¦æœ‰æ¨¡å‹çš„GPTQé‡åŒ–ç‰ˆæœ¬ã€‚å¦‚æœæ²¡æœ‰ï¼Œæ‚¨å¯ä»¥åœ¨GitHubä¸Šæäº¤éœ€æ±‚ã€‚ 
</Tip></p>
<h3 id="hub">æ¨é€é‡åŒ–æ¨¡å‹åˆ° ğŸ¤— Hub</h3>
<p>æ‚¨å¯ä»¥ä½¿ç”¨<code>push_to_hub</code>å°†é‡åŒ–æ¨¡å‹åƒä»»ä½•æ¨¡å‹ä¸€æ ·æ¨é€åˆ°Hubã€‚é‡åŒ–é…ç½®å°†ä¸æ¨¡å‹ä¸€èµ·ä¿å­˜å’Œæ¨é€ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-8-1"><a id="__codelineno-8-1" name="__codelineno-8-1" href="#__codelineno-8-1"></a><span class="n">quantized_model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;opt-125m-gptq&quot;</span><span class="p">)</span>
</span><span id="__span-8-2"><a id="__codelineno-8-2" name="__codelineno-8-2" href="#__codelineno-8-2"></a><span class="n">tokenizer</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;opt-125m-gptq&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>å¦‚æœæ‚¨æƒ³åœ¨æœ¬åœ°è®¡ç®—æœºä¸Šä¿å­˜é‡åŒ–æ¨¡å‹ï¼Œæ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨<code>save_pretrained</code>æ¥å®Œæˆï¼š</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-9-1"><a id="__codelineno-9-1" name="__codelineno-9-1" href="#__codelineno-9-1"></a><span class="n">quantized_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;opt-125m-gptq&quot;</span><span class="p">)</span>
</span><span id="__span-9-2"><a id="__codelineno-9-2" name="__codelineno-9-2" href="#__codelineno-9-2"></a><span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;opt-125m-gptq&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>è¯·æ³¨æ„ï¼Œå¦‚æœæ‚¨é‡åŒ–æ¨¡å‹æ—¶æƒ³ä½¿ç”¨<code>device_map</code>ï¼Œè¯·ç¡®ä¿åœ¨ä¿å­˜ä¹‹å‰å°†æ•´ä¸ªæ¨¡å‹ç§»åŠ¨åˆ°æ‚¨çš„GPUæˆ–CPUä¹‹ä¸€ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-10-1"><a id="__codelineno-10-1" name="__codelineno-10-1" href="#__codelineno-10-1"></a><span class="n">quantized_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
</span><span id="__span-10-2"><a id="__codelineno-10-2" name="__codelineno-10-2" href="#__codelineno-10-2"></a><span class="n">quantized_model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s2">&quot;opt-125m-gptq&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="hub_1">ä» ğŸ¤— Hub åŠ è½½ä¸€ä¸ªé‡åŒ–æ¨¡å‹</h3>
<p>æ‚¨å¯ä»¥ä½¿ç”¨<code>from_pretrained</code>ä»HubåŠ è½½é‡åŒ–æ¨¡å‹ã€‚
è¯·ç¡®ä¿æ¨é€æƒé‡æ˜¯é‡åŒ–çš„ï¼Œæ£€æŸ¥æ¨¡å‹é…ç½®å¯¹è±¡ä¸­æ˜¯å¦å­˜åœ¨<code>quantization_config</code>å±æ€§ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-11-1"><a id="__codelineno-11-1" name="__codelineno-11-1" href="#__codelineno-11-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
</span><span id="__span-11-2"><a id="__codelineno-11-2" name="__codelineno-11-2" href="#__codelineno-11-2"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{your_username}</span><span class="s2">/opt-125m-gptq&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>å¦‚æœæ‚¨æƒ³æ›´å¿«åœ°åŠ è½½æ¨¡å‹ï¼Œå¹¶ä¸”ä¸éœ€è¦åˆ†é…æ¯”å®é™…éœ€è¦å†…å­˜æ›´å¤šçš„å†…å­˜ï¼Œé‡åŒ–æ¨¡å‹ä¹Ÿä½¿ç”¨<code>device_map</code>å‚æ•°ã€‚ç¡®ä¿æ‚¨å·²å®‰è£…<code>accelerate</code>åº“ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-12-1"><a id="__codelineno-12-1" name="__codelineno-12-1" href="#__codelineno-12-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span>
</span><span id="__span-12-2"><a id="__codelineno-12-2" name="__codelineno-12-2" href="#__codelineno-12-2"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{your_username}</span><span class="s2">/opt-125m-gptq&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="exllama">Exllamaå†…æ ¸åŠ å¿«æ¨ç†é€Ÿåº¦</h3>
<p>ä¿ç•™æ ¼å¼ï¼šå¯¹äº 4 ä½æ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ exllama å†…æ ¸æ¥æé«˜æ¨ç†é€Ÿåº¦ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒå¤„äºå¯ç”¨çŠ¶æ€ã€‚æ‚¨å¯ä»¥é€šè¿‡åœ¨ [<code>GPTQConfig</code>] ä¸­ä¼ é€’ <code>use_exllama</code> æ¥æ›´æ”¹æ­¤é…ç½®ã€‚è¿™å°†è¦†ç›–å­˜å‚¨åœ¨é…ç½®ä¸­çš„é‡åŒ–é…ç½®ã€‚è¯·æ³¨æ„ï¼Œæ‚¨åªèƒ½è¦†ç›–ä¸å†…æ ¸ç›¸å…³çš„å±æ€§ã€‚æ­¤å¤–ï¼Œå¦‚æœæ‚¨æƒ³ä½¿ç”¨ exllama å†…æ ¸ï¼Œæ•´ä¸ªæ¨¡å‹éœ€è¦å…¨éƒ¨éƒ¨ç½²åœ¨ gpus ä¸Šã€‚æ­¤å¤–ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ ç‰ˆæœ¬ &gt; 0.4.2 çš„ Auto-GPTQ å¹¶ä¼ é€’ <code>device_map</code> = "cpu" æ¥æ‰§è¡Œ CPU æ¨ç†ã€‚å¯¹äº CPU æ¨ç†ï¼Œæ‚¨å¿…é¡»åœ¨ <code>GPTQConfig</code> ä¸­ä¼ é€’ <code>use_exllama = False</code>ã€‚</p>
<div class="language-py highlight"><pre><span></span><code><span id="__span-13-1"><a id="__codelineno-13-1" name="__codelineno-13-1" href="#__codelineno-13-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-13-2"><a id="__codelineno-13-2" name="__codelineno-13-2" href="#__codelineno-13-2"></a><span class="n">gptq_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
</span><span id="__span-13-3"><a id="__codelineno-13-3" name="__codelineno-13-3" href="#__codelineno-13-3"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{your_username}</span><span class="s2">/opt-125m-gptq&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">gptq_config</span><span class="p">)</span>
</span></code></pre></div>
<p>éšç€ exllamav2 å†…æ ¸çš„å‘å¸ƒï¼Œä¸ exllama å†…æ ¸ç›¸æ¯”ï¼Œæ‚¨å¯ä»¥è·å¾—æ›´å¿«çš„æ¨ç†é€Ÿåº¦ã€‚æ‚¨åªéœ€åœ¨ [<code>GPTQConfig</code>] ä¸­ä¼ é€’ <code>exllama_config={"version": 2}</code>ï¼š</p>
<div class="language-py highlight"><pre><span></span><code><span id="__span-14-1"><a id="__codelineno-14-1" name="__codelineno-14-1" href="#__codelineno-14-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-14-2"><a id="__codelineno-14-2" name="__codelineno-14-2" href="#__codelineno-14-2"></a><span class="n">gptq_config</span> <span class="o">=</span> <span class="n">GPTQConfig</span><span class="p">(</span><span class="n">bits</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">exllama_config</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;version&quot;</span><span class="p">:</span><span class="mi">2</span><span class="p">})</span>
</span><span id="__span-14-3"><a id="__codelineno-14-3" name="__codelineno-14-3" href="#__codelineno-14-3"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{your_username}</span><span class="s2">/opt-125m-gptq&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span> <span class="o">=</span> <span class="n">gptq_config</span><span class="p">)</span>
</span></code></pre></div>
<p>è¯·æ³¨æ„ï¼Œç›®å‰ä»…æ”¯æŒ 4 ä½æ¨¡å‹ã€‚æ­¤å¤–ï¼Œå¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨ peft å¯¹é‡åŒ–æ¨¡å‹è¿›è¡Œå¾®è°ƒï¼Œå»ºè®®ç¦ç”¨ exllama å†…æ ¸ã€‚ </p>
<p>æ‚¨å¯ä»¥åœ¨æ­¤æ‰¾åˆ°è¿™äº›å†…æ ¸çš„åŸºå‡†æµ‹è¯• <a href="https://github.com/huggingface/optimum/tree/main/tests/benchmark#gptq-benchmark">è¿™é‡Œ</a></p>
<h4 id="_8">å¾®è°ƒä¸€ä¸ªé‡åŒ–æ¨¡å‹</h4>
<p>åœ¨Hugging Faceç”Ÿæ€ç³»ç»Ÿçš„å®˜æ–¹æ”¯æŒä¸‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨GPTQè¿›è¡Œé‡åŒ–åçš„æ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚ 
è¯·æŸ¥çœ‹<code>peft</code>åº“äº†è§£æ›´å¤šè¯¦æƒ…ã€‚</p>
<h3 id="_9">ç¤ºä¾‹æ¼”ç¤º</h3>
<p>è¯·æŸ¥çœ‹ Google Colab <a href="https://colab.research.google.com/drive/1_TIrmuKOFhuRRiTWN94ilkUFu6ZX4ceb?usp=sharing">notebook</a>ï¼Œäº†è§£å¦‚ä½•ä½¿ç”¨GPTQé‡åŒ–æ‚¨çš„æ¨¡å‹ä»¥åŠå¦‚ä½•ä½¿ç”¨peftå¾®è°ƒé‡åŒ–æ¨¡å‹ã€‚</p>
<h3 id="gptqconfig">GPTQConfig</h3>
<p>[[autodoc]] GPTQConfig</p>
<h2 id="bitsandbytes"><code>bitsandbytes</code> é›†æˆ</h2>
<p>ğŸ¤— Transformers ä¸ <code>bitsandbytes</code> ä¸Šæœ€å¸¸ç”¨çš„æ¨¡å—ç´§å¯†é›†æˆã€‚æ‚¨å¯ä»¥ä½¿ç”¨å‡ è¡Œä»£ç ä»¥ 8 ä½ç²¾åº¦åŠ è½½æ‚¨çš„æ¨¡å‹ã€‚
è‡ªbitsandbytesçš„0.37.0ç‰ˆæœ¬å‘å¸ƒä»¥æ¥ï¼Œå¤§å¤šæ•°GPUç¡¬ä»¶éƒ½æ”¯æŒè¿™ä¸€ç‚¹ã€‚</p>
<p>åœ¨<a href="https://huggingface.co/papers/2208.07339">LLM.int8()</a>è®ºæ–‡ä¸­äº†è§£æ›´å¤šå…³äºé‡åŒ–æ–¹æ³•çš„ä¿¡æ¯ï¼Œæˆ–è€…åœ¨<a href="https://huggingface.co/blog/hf-bitsandbytes-integration">åšå®¢æ–‡ç« </a>ä¸­äº†è§£å…³äºåˆä½œçš„æ›´å¤šä¿¡æ¯ã€‚</p>
<p>è‡ªå…¶â€œ0.39.0â€ç‰ˆæœ¬å‘å¸ƒä»¥æ¥ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨FP4æ•°æ®ç±»å‹ï¼Œé€šè¿‡4ä½é‡åŒ–åŠ è½½ä»»ä½•æ”¯æŒâ€œdevice_mapâ€çš„æ¨¡å‹ã€‚</p>
<p>å¦‚æœæ‚¨æƒ³é‡åŒ–è‡ªå·±çš„ pytorch æ¨¡å‹ï¼Œè¯·æŸ¥çœ‹ ğŸ¤— Accelerate çš„<a href="https://huggingface.co/docs/accelerate/main/en/usage_guides/quantization">æ–‡æ¡£</a>ã€‚</p>
<p>ä»¥ä¸‹æ˜¯æ‚¨å¯ä»¥ä½¿ç”¨â€œbitsandbytesâ€é›†æˆå®Œæˆçš„äº‹æƒ…</p>
<h3 id="_10">é€šç”¨ç”¨æ³•</h3>
<p>åªè¦æ‚¨çš„æ¨¡å‹æ”¯æŒä½¿ç”¨ ğŸ¤— Accelerate è¿›è¡ŒåŠ è½½å¹¶åŒ…å« <code>torch.nn.Linear</code> å±‚ï¼Œæ‚¨å¯ä»¥åœ¨è°ƒç”¨ [<code>~PreTrainedModel.from_pretrained</code>] æ–¹æ³•æ—¶ä½¿ç”¨ <code>load_in_8bit</code> æˆ– <code>load_in_4bit</code> å‚æ•°æ¥é‡åŒ–æ¨¡å‹ã€‚è¿™ä¹Ÿåº”è¯¥é€‚ç”¨äºä»»ä½•æ¨¡æ€ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-15-1"><a id="__codelineno-15-1" name="__codelineno-15-1" href="#__codelineno-15-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
</span><span id="__span-15-2"><a id="__codelineno-15-2" name="__codelineno-15-2" href="#__codelineno-15-2"></a>
</span><span id="__span-15-3"><a id="__codelineno-15-3" name="__codelineno-15-3" href="#__codelineno-15-3"></a><span class="n">model_8bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/opt-350m&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span><span id="__span-15-4"><a id="__codelineno-15-4" name="__codelineno-15-4" href="#__codelineno-15-4"></a><span class="n">model_4bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/opt-350m&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></code></pre></div>
<p>é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰å…¶ä»–æ¨¡å—ï¼ˆä¾‹å¦‚ <code>torch.nn.LayerNorm</code>ï¼‰å°†è¢«è½¬æ¢ä¸º <code>torch.float16</code> ç±»å‹ã€‚ä½†å¦‚æœæ‚¨æƒ³æ›´æ”¹å®ƒä»¬çš„ <code>dtype</code>ï¼Œå¯ä»¥é‡è½½ <code>dtype</code> å‚æ•°ï¼š</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-16-1"><a id="__codelineno-16-1" name="__codelineno-16-1" href="#__codelineno-16-1"></a><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-16-2"><a id="__codelineno-16-2" name="__codelineno-16-2" href="#__codelineno-16-2"></a><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
</span><span id="__span-16-3"><a id="__codelineno-16-3" name="__codelineno-16-3" href="#__codelineno-16-3"></a>
</span><span id="__span-16-4"><a id="__codelineno-16-4" name="__codelineno-16-4" href="#__codelineno-16-4"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">model_8bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;facebook/opt-350m&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
</span><span id="__span-16-5"><a id="__codelineno-16-5" name="__codelineno-16-5" href="#__codelineno-16-5"></a><span class="o">&gt;&gt;&gt;</span> <span class="n">model_8bit</span><span class="o">.</span><span class="n">model</span><span class="o">.</span><span class="n">decoder</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">final_layer_norm</span><span class="o">.</span><span class="n">weight</span><span class="o">.</span><span class="n">dtype</span>
</span><span id="__span-16-6"><a id="__codelineno-16-6" name="__codelineno-16-6" href="#__codelineno-16-6"></a><span class="n">torch</span><span class="o">.</span><span class="n">float32</span>
</span></code></pre></div>
<h3 id="fp4">FP4 é‡åŒ–</h3>
<h4 id="_11">è¦æ±‚</h4>
<p>ç¡®ä¿åœ¨è¿è¡Œä»¥ä¸‹ä»£ç æ®µä¹‹å‰å·²å®Œæˆä»¥ä¸‹è¦æ±‚ï¼š</p>
<ul>
<li>
<p>æœ€æ–°ç‰ˆæœ¬ <code>bitsandbytes</code> åº“
<code>pip install bitsandbytes&gt;=0.39.0</code></p>
</li>
<li>
<p>å®‰è£…æœ€æ–°ç‰ˆæœ¬ <code>accelerate</code>
<code>pip install --upgrade accelerate</code></p>
</li>
<li>
<p>å®‰è£…æœ€æ–°ç‰ˆæœ¬ <code>transformers</code>
<code>pip install --upgrade transformers</code></p>
</li>
</ul>
<h4 id="_12">æç¤ºå’Œæœ€ä½³å®è·µ</h4>
<ul>
<li>
<p><strong>é«˜çº§ç”¨æ³•ï¼š</strong> è¯·å‚è€ƒ <a href="https://colab.research.google.com/drive/1ge2F1QSK8Q7h0hn3YKuBCOAS0bK8E0wf">æ­¤ Google Colab notebook</a> ä»¥è·å– 4 ä½é‡åŒ–é«˜çº§ç”¨æ³•å’Œæ‰€æœ‰å¯é€‰é€‰é¡¹ã€‚</p>
</li>
<li>
<p><strong>ä½¿ç”¨ <code>batch_size=1</code> å®ç°æ›´å¿«çš„æ¨ç†ï¼š</strong> è‡ª <code>bitsandbytes</code> çš„ <code>0.40.0</code> ç‰ˆæœ¬ä»¥æ¥ï¼Œè®¾ç½® <code>batch_size=1</code>ï¼Œæ‚¨å¯ä»¥ä»å¿«é€Ÿæ¨ç†ä¸­å—ç›Šã€‚è¯·æŸ¥çœ‹ <a href="https://github.com/TimDettmers/bitsandbytes/releases/tag/0.40.0">è¿™äº›å‘å¸ƒè¯´æ˜</a> ï¼Œå¹¶ç¡®ä¿ä½¿ç”¨å¤§äº <code>0.40.0</code> çš„ç‰ˆæœ¬ä»¥ç›´æ¥åˆ©ç”¨æ­¤åŠŸèƒ½ã€‚</p>
</li>
<li>
<p><strong>è®­ç»ƒï¼š</strong> æ ¹æ® <a href="https://huggingface.co/papers/2305.14314">QLoRA è®ºæ–‡</a>ï¼Œå¯¹äº4ä½åŸºæ¨¡å‹è®­ç»ƒï¼ˆä½¿ç”¨ LoRA é€‚é…å™¨ï¼‰ï¼Œåº”ä½¿ç”¨ <code>bnb_4bit_quant_type='nf4'</code>ã€‚</p>
</li>
<li>
<p><strong>æ¨ç†ï¼š</strong> å¯¹äºæ¨ç†ï¼Œ<code>bnb_4bit_quant_type</code> å¯¹æ€§èƒ½å½±å“ä¸å¤§ã€‚ä½†æ˜¯ä¸ºäº†ä¸æ¨¡å‹çš„æƒé‡ä¿æŒä¸€è‡´ï¼Œè¯·ç¡®ä¿ä½¿ç”¨ç›¸åŒçš„ <code>bnb_4bit_compute_dtype</code> å’Œ <code>dtype</code> å‚æ•°ã€‚</p>
</li>
</ul>
<h4 id="4">åŠ è½½ 4 ä½é‡åŒ–çš„å¤§æ¨¡å‹</h4>
<p>åœ¨è°ƒç”¨ <code>.from_pretrained</code> æ–¹æ³•æ—¶ä½¿ç”¨ <code>load_in_4bit=True</code>ï¼Œå¯ä»¥å°†æ‚¨çš„å†…å­˜ä½¿ç”¨é‡å‡å°‘åˆ°å¤§çº¦åŸæ¥çš„ 1/4ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-17-1"><a id="__codelineno-17-1" name="__codelineno-17-1" href="#__codelineno-17-1"></a><span class="c1"># pip install transformers accelerate bitsandbytes</span>
</span><span id="__span-17-2"><a id="__codelineno-17-2" name="__codelineno-17-2" href="#__codelineno-17-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span><span id="__span-17-3"><a id="__codelineno-17-3" name="__codelineno-17-3" href="#__codelineno-17-3"></a>
</span><span id="__span-17-4"><a id="__codelineno-17-4" name="__codelineno-17-4" href="#__codelineno-17-4"></a><span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;bigscience/bloom-1b7&quot;</span>
</span><span id="__span-17-5"><a id="__codelineno-17-5" name="__codelineno-17-5" href="#__codelineno-17-5"></a>
</span><span id="__span-17-6"><a id="__codelineno-17-6" name="__codelineno-17-6" href="#__codelineno-17-6"></a><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
</span><span id="__span-17-7"><a id="__codelineno-17-7" name="__codelineno-17-7" href="#__codelineno-17-7"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></code></pre></div>
<p><Tip warning={true}></p>
<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸€æ—¦æ¨¡å‹ä»¥ 4 ä½é‡åŒ–æ–¹å¼åŠ è½½ï¼Œå°±æ— æ³•å°†é‡åŒ–åçš„æƒé‡æ¨é€åˆ° Hub ä¸Šã€‚æ­¤å¤–ï¼Œæ‚¨ä¸èƒ½è®­ç»ƒ 4 ä½é‡åŒ–æƒé‡ï¼Œå› ä¸ºç›®å‰å°šä¸æ”¯æŒæ­¤åŠŸèƒ½ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ 4 ä½é‡åŒ–æ¨¡å‹æ¥è®­ç»ƒé¢å¤–å‚æ•°ï¼Œè¿™å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ä»‹ç»ã€‚</p>
<p></Tip></p>
<h3 id="8">åŠ è½½ 8 ä½é‡åŒ–çš„å¤§æ¨¡å‹</h3>
<p>æ‚¨å¯ä»¥é€šè¿‡åœ¨è°ƒç”¨ <code>.from_pretrained</code> æ–¹æ³•æ—¶ä½¿ç”¨ <code>load_in_8bit=True</code> å‚æ•°ï¼Œå°†å†…å­˜éœ€æ±‚å¤§è‡´å‡åŠæ¥åŠ è½½æ¨¡å‹</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-18-1"><a id="__codelineno-18-1" name="__codelineno-18-1" href="#__codelineno-18-1"></a><span class="c1"># pip install transformers accelerate bitsandbytes</span>
</span><span id="__span-18-2"><a id="__codelineno-18-2" name="__codelineno-18-2" href="#__codelineno-18-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
</span><span id="__span-18-3"><a id="__codelineno-18-3" name="__codelineno-18-3" href="#__codelineno-18-3"></a>
</span><span id="__span-18-4"><a id="__codelineno-18-4" name="__codelineno-18-4" href="#__codelineno-18-4"></a><span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;bigscience/bloom-1b7&quot;</span>
</span><span id="__span-18-5"><a id="__codelineno-18-5" name="__codelineno-18-5" href="#__codelineno-18-5"></a>
</span><span id="__span-18-6"><a id="__codelineno-18-6" name="__codelineno-18-6" href="#__codelineno-18-6"></a><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
</span><span id="__span-18-7"><a id="__codelineno-18-7" name="__codelineno-18-7" href="#__codelineno-18-7"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span></code></pre></div>
<p>ç„¶åï¼Œåƒé€šå¸¸ä½¿ç”¨ <code>PreTrainedModel</code> ä¸€æ ·ä½¿ç”¨æ‚¨çš„æ¨¡å‹ã€‚</p>
<p>æ‚¨å¯ä»¥ä½¿ç”¨ <code>get_memory_footprint</code> æ–¹æ³•æ£€æŸ¥æ¨¡å‹çš„å†…å­˜å ç”¨ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-19-1"><a id="__codelineno-19-1" name="__codelineno-19-1" href="#__codelineno-19-1"></a><span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">get_memory_footprint</span><span class="p">())</span>
</span></code></pre></div>
<p>é€šè¿‡è¿™ç§é›†æˆï¼Œæˆ‘ä»¬èƒ½å¤Ÿåœ¨è¾ƒå°çš„è®¾å¤‡ä¸ŠåŠ è½½å¤§æ¨¡å‹å¹¶è¿è¡Œå®ƒä»¬è€Œæ²¡æœ‰ä»»ä½•é—®é¢˜ã€‚</p>
<p><Tip warning={true}></p>
<p>éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œä¸€æ—¦æ¨¡å‹ä»¥ 8 ä½é‡åŒ–æ–¹å¼åŠ è½½ï¼Œé™¤äº†ä½¿ç”¨æœ€æ–°çš„ <code>transformers</code> å’Œ <code>bitsandbytes</code> ä¹‹å¤–ï¼Œç›®å‰å°šæ— æ³•å°†é‡åŒ–åçš„æƒé‡æ¨é€åˆ° Hub ä¸Šã€‚æ­¤å¤–ï¼Œæ‚¨ä¸èƒ½è®­ç»ƒ 8 ä½é‡åŒ–æƒé‡ï¼Œå› ä¸ºç›®å‰å°šä¸æ”¯æŒæ­¤åŠŸèƒ½ã€‚ä½†æ˜¯ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨ 8 ä½é‡åŒ–æ¨¡å‹æ¥è®­ç»ƒé¢å¤–å‚æ•°ï¼Œè¿™å°†åœ¨ä¸‹ä¸€éƒ¨åˆ†ä¸­ä»‹ç»ã€‚</p>
<p>æ³¨æ„ï¼Œ<code>device_map</code> æ˜¯å¯é€‰çš„ï¼Œä½†è®¾ç½® <code>device_map = 'auto'</code> æ›´é€‚åˆç”¨äºæ¨ç†ï¼Œå› ä¸ºå®ƒå°†æ›´æœ‰æ•ˆåœ°è°ƒåº¦å¯ç”¨èµ„æºä¸Šçš„æ¨¡å‹ã€‚</p>
<p></Tip></p>
<h4 id="_13">é«˜çº§ç”¨ä¾‹</h4>
<p>åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†ä»‹ç»ä½¿ç”¨ FP4 é‡åŒ–çš„ä¸€äº›é«˜çº§ç”¨ä¾‹ã€‚</p>
<h5 id="_14">æ›´æ”¹è®¡ç®—æ•°æ®ç±»å‹</h5>
<p>è®¡ç®—æ•°æ®ç±»å‹ç”¨äºæ”¹å˜åœ¨è¿›è¡Œè®¡ç®—æ—¶ä½¿ç”¨çš„æ•°æ®ç±»å‹ã€‚ä¾‹å¦‚ï¼Œhidden stateså¯ä»¥æ˜¯ <code>float32</code>ï¼Œä½†ä¸ºäº†åŠ é€Ÿï¼Œè®¡ç®—æ—¶å¯ä»¥è¢«è®¾ç½®ä¸º <code>bf16</code>ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œè®¡ç®—æ•°æ®ç±»å‹è¢«è®¾ç½®ä¸º <code>float32</code>ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-20-1"><a id="__codelineno-20-1" name="__codelineno-20-1" href="#__codelineno-20-1"></a><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
</span><span id="__span-20-2"><a id="__codelineno-20-2" name="__codelineno-20-2" href="#__codelineno-20-2"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BitsAndBytesConfig</span>
</span><span id="__span-20-3"><a id="__codelineno-20-3" name="__codelineno-20-3" href="#__codelineno-20-3"></a>
</span><span id="__span-20-4"><a id="__codelineno-20-4" name="__codelineno-20-4" href="#__codelineno-20-4"></a><span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bnb_4bit_compute_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
</span></code></pre></div>
<h4 id="nf4-4">ä½¿ç”¨ NF4ï¼ˆæ™®é€šæµ®ç‚¹æ•° 4ï¼‰æ•°æ®ç±»å‹</h4>
<p>æ‚¨è¿˜å¯ä»¥ä½¿ç”¨ NF4 æ•°æ®ç±»å‹ï¼Œè¿™æ˜¯ä¸€ç§é’ˆå¯¹ä½¿ç”¨æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–çš„æƒé‡è€Œé€‚åº”çš„æ–°å‹ 4 ä½æ•°æ®ç±»å‹ã€‚è¦è¿è¡Œï¼š</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-21-1"><a id="__codelineno-21-1" name="__codelineno-21-1" href="#__codelineno-21-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BitsAndBytesConfig</span>
</span><span id="__span-21-2"><a id="__codelineno-21-2" name="__codelineno-21-2" href="#__codelineno-21-2"></a>
</span><span id="__span-21-3"><a id="__codelineno-21-3" name="__codelineno-21-3" href="#__codelineno-21-3"></a><span class="n">nf4_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
</span><span id="__span-21-4"><a id="__codelineno-21-4" name="__codelineno-21-4" href="#__codelineno-21-4"></a>    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="__span-21-5"><a id="__codelineno-21-5" name="__codelineno-21-5" href="#__codelineno-21-5"></a>    <span class="n">bnb_4bit_quant_type</span><span class="o">=</span><span class="s2">&quot;nf4&quot;</span><span class="p">,</span>
</span><span id="__span-21-6"><a id="__codelineno-21-6" name="__codelineno-21-6" href="#__codelineno-21-6"></a><span class="p">)</span>
</span><span id="__span-21-7"><a id="__codelineno-21-7" name="__codelineno-21-7" href="#__codelineno-21-7"></a>
</span><span id="__span-21-8"><a id="__codelineno-21-8" name="__codelineno-21-8" href="#__codelineno-21-8"></a><span class="n">model_nf4</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">nf4_config</span><span class="p">)</span>
</span></code></pre></div>
<h4 id="_15">ä½¿ç”¨åµŒå¥—é‡åŒ–è¿›è¡Œæ›´é«˜æ•ˆçš„å†…å­˜æ¨ç†</h4>
<p>æˆ‘ä»¬è¿˜å»ºè®®ç”¨æˆ·ä½¿ç”¨åµŒå¥—é‡åŒ–æŠ€æœ¯ã€‚ä»æˆ‘ä»¬çš„ç»éªŒè§‚å¯Ÿæ¥çœ‹ï¼Œè¿™ç§æ–¹æ³•åœ¨ä¸å¢åŠ é¢å¤–æ€§èƒ½çš„æƒ…å†µä¸‹èŠ‚çœæ›´å¤šå†…å­˜ã€‚è¿™ä½¿å¾— llama-13b æ¨¡å‹èƒ½å¤Ÿåœ¨å…·æœ‰ 1024 ä¸ªåºåˆ—é•¿åº¦ã€1 ä¸ªæ‰¹æ¬¡å¤§å°å’Œ 4 ä¸ªæ¢¯åº¦ç´¯ç§¯æ­¥éª¤çš„ NVIDIA-T4 16GB ä¸Šè¿›è¡Œ fine-tuningã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-22-1"><a id="__codelineno-22-1" name="__codelineno-22-1" href="#__codelineno-22-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BitsAndBytesConfig</span>
</span><span id="__span-22-2"><a id="__codelineno-22-2" name="__codelineno-22-2" href="#__codelineno-22-2"></a>
</span><span id="__span-22-3"><a id="__codelineno-22-3" name="__codelineno-22-3" href="#__codelineno-22-3"></a><span class="n">double_quant_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
</span><span id="__span-22-4"><a id="__codelineno-22-4" name="__codelineno-22-4" href="#__codelineno-22-4"></a>    <span class="n">load_in_4bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="__span-22-5"><a id="__codelineno-22-5" name="__codelineno-22-5" href="#__codelineno-22-5"></a>    <span class="n">bnb_4bit_use_double_quant</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
</span><span id="__span-22-6"><a id="__codelineno-22-6" name="__codelineno-22-6" href="#__codelineno-22-6"></a><span class="p">)</span>
</span><span id="__span-22-7"><a id="__codelineno-22-7" name="__codelineno-22-7" href="#__codelineno-22-7"></a>
</span><span id="__span-22-8"><a id="__codelineno-22-8" name="__codelineno-22-8" href="#__codelineno-22-8"></a><span class="n">model_double_quant</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">double_quant_config</span><span class="p">)</span>
</span></code></pre></div>
<h3 id="hub_2">å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ°ğŸ¤— Hub</h3>
<p>æ‚¨å¯ä»¥ä½¿ç”¨ <code>push_to_hub</code> æ–¹æ³•å°†é‡åŒ–æ¨¡å‹æ¨é€åˆ° Hub ä¸Šã€‚è¿™å°†é¦–å…ˆæ¨é€é‡åŒ–é…ç½®æ–‡ä»¶ï¼Œç„¶åæ¨é€é‡åŒ–æ¨¡å‹æƒé‡ã€‚
è¯·ç¡®ä¿ä½¿ç”¨ <code>bitsandbytes&gt;0.37.2</code>ï¼ˆåœ¨æ’°å†™æœ¬æ–‡æ—¶ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ <code>bitsandbytes==0.38.0.post1</code>ï¼‰æ‰èƒ½ä½¿ç”¨æ­¤åŠŸèƒ½ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-23-1"><a id="__codelineno-23-1" name="__codelineno-23-1" href="#__codelineno-23-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
</span><span id="__span-23-2"><a id="__codelineno-23-2" name="__codelineno-23-2" href="#__codelineno-23-2"></a>
</span><span id="__span-23-3"><a id="__codelineno-23-3" name="__codelineno-23-3" href="#__codelineno-23-3"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloom-560m&quot;</span><span class="p">,</span> <span class="n">quantization_config</span><span class="o">=</span><span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
</span><span id="__span-23-4"><a id="__codelineno-23-4" name="__codelineno-23-4" href="#__codelineno-23-4"></a><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;bigscience/bloom-560m&quot;</span><span class="p">)</span>
</span><span id="__span-23-5"><a id="__codelineno-23-5" name="__codelineno-23-5" href="#__codelineno-23-5"></a>
</span><span id="__span-23-6"><a id="__codelineno-23-6" name="__codelineno-23-6" href="#__codelineno-23-6"></a><span class="n">model</span><span class="o">.</span><span class="n">push_to_hub</span><span class="p">(</span><span class="s2">&quot;bloom-560m-8bit&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p><Tip warning={true}></p>
<p>å¯¹å¤§æ¨¡å‹ï¼Œå¼ºçƒˆé¼“åŠ±å°† 8 ä½é‡åŒ–æ¨¡å‹æ¨é€åˆ° Hub ä¸Šï¼Œä»¥ä¾¿è®©ç¤¾åŒºèƒ½å¤Ÿä»å†…å­˜å ç”¨å‡å°‘å’ŒåŠ è½½ä¸­å—ç›Šï¼Œä¾‹å¦‚åœ¨ Google Colab ä¸ŠåŠ è½½å¤§æ¨¡å‹ã€‚</p>
<p></Tip></p>
<h3 id="hub_3">ä»ğŸ¤— HubåŠ è½½é‡åŒ–æ¨¡å‹</h3>
<p>æ‚¨å¯ä»¥ä½¿ç”¨ <code>from_pretrained</code> æ–¹æ³•ä» Hub åŠ è½½é‡åŒ–æ¨¡å‹ã€‚è¯·ç¡®ä¿æ¨é€çš„æƒé‡æ˜¯é‡åŒ–çš„ï¼Œæ£€æŸ¥æ¨¡å‹é…ç½®å¯¹è±¡ä¸­æ˜¯å¦å­˜åœ¨ <code>quantization_config</code> å±æ€§ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-24-1"><a id="__codelineno-24-1" name="__codelineno-24-1" href="#__codelineno-24-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
</span><span id="__span-24-2"><a id="__codelineno-24-2" name="__codelineno-24-2" href="#__codelineno-24-2"></a>
</span><span id="__span-24-3"><a id="__codelineno-24-3" name="__codelineno-24-3" href="#__codelineno-24-3"></a><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{your_username}</span><span class="s2">/bloom-560m-8bit&quot;</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
</span></code></pre></div>
<p>è¯·æ³¨æ„ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨ä¸éœ€è¦æŒ‡å®š <code>load_in_8bit=True</code> å‚æ•°ï¼Œä½†éœ€è¦ç¡®ä¿ <code>bitsandbytes</code> å’Œ <code>accelerate</code> å·²å®‰è£…ã€‚
æƒ…æ³¨æ„ï¼Œ<code>device_map</code> æ˜¯å¯é€‰çš„ï¼Œä½†è®¾ç½® <code>device_map = 'auto'</code> æ›´é€‚åˆç”¨äºæ¨ç†ï¼Œå› ä¸ºå®ƒå°†æ›´æœ‰æ•ˆåœ°è°ƒåº¦å¯ç”¨èµ„æºä¸Šçš„æ¨¡å‹ã€‚</p>
<h3 id="_16">é«˜çº§ç”¨ä¾‹</h3>
<p>æœ¬èŠ‚é¢å‘å¸Œæœ›æ¢ç´¢é™¤äº†åŠ è½½å’Œè¿è¡Œ 8 ä½æ¨¡å‹ä¹‹å¤–è¿˜èƒ½åšä»€ä¹ˆçš„è¿›é˜¶ç”¨æˆ·ã€‚</p>
<h4 id="cpu-gpu">åœ¨ <code>cpu</code> å’Œ <code>gpu</code> ä¹‹é—´å¸è½½</h4>
<p>æ­¤é«˜çº§ç”¨ä¾‹ä¹‹ä¸€æ˜¯èƒ½å¤ŸåŠ è½½æ¨¡å‹å¹¶å°†æƒé‡åˆ†æ´¾åˆ° <code>CPU</code> å’Œ <code>GPU</code> ä¹‹é—´ã€‚è¯·æ³¨æ„ï¼Œå°†åœ¨ CPU ä¸Šåˆ†æ´¾çš„æƒé‡ <strong>ä¸ä¼š</strong> è½¬æ¢ä¸º 8 ä½ï¼Œå› æ­¤ä¼šä¿ç•™ä¸º <code>float32</code>ã€‚æ­¤åŠŸèƒ½é€‚ç”¨äºæƒ³è¦é€‚åº”éå¸¸å¤§çš„æ¨¡å‹å¹¶å°†æ¨¡å‹åˆ†æ´¾åˆ° GPU å’Œ CPU ä¹‹é—´çš„ç”¨æˆ·ã€‚</p>
<p>é¦–å…ˆï¼Œä» <code>transformers</code> ä¸­åŠ è½½ä¸€ä¸ª [<code>BitsAndBytesConfig</code>]ï¼Œå¹¶å°†å±æ€§ <code>llm_int8_enable_fp32_cpu_offload</code> è®¾ç½®ä¸º <code>True</code>ï¼š</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-25-1"><a id="__codelineno-25-1" name="__codelineno-25-1" href="#__codelineno-25-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
</span><span id="__span-25-2"><a id="__codelineno-25-2" name="__codelineno-25-2" href="#__codelineno-25-2"></a>
</span><span id="__span-25-3"><a id="__codelineno-25-3" name="__codelineno-25-3" href="#__codelineno-25-3"></a><span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span><span class="n">llm_int8_enable_fp32_cpu_offload</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</span></code></pre></div>
<p>å‡è®¾æ‚¨æƒ³åŠ è½½ <code>bigscience/bloom-1b7</code> æ¨¡å‹ï¼Œæ‚¨çš„ GPUæ˜¾å­˜ä»…è¶³å¤Ÿå®¹çº³é™¤äº†<code>lm_head</code>å¤–çš„æ•´ä¸ªæ¨¡å‹ã€‚å› æ­¤ï¼Œæ‚¨å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼ç¼–å†™è‡ªå®šä¹‰çš„ device_mapï¼š</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-26-1"><a id="__codelineno-26-1" name="__codelineno-26-1" href="#__codelineno-26-1"></a><span class="n">device_map</span> <span class="o">=</span> <span class="p">{</span>
</span><span id="__span-26-2"><a id="__codelineno-26-2" name="__codelineno-26-2" href="#__codelineno-26-2"></a>    <span class="s2">&quot;transformer.word_embeddings&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="__span-26-3"><a id="__codelineno-26-3" name="__codelineno-26-3" href="#__codelineno-26-3"></a>    <span class="s2">&quot;transformer.word_embeddings_layernorm&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="__span-26-4"><a id="__codelineno-26-4" name="__codelineno-26-4" href="#__codelineno-26-4"></a>    <span class="s2">&quot;lm_head&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
</span><span id="__span-26-5"><a id="__codelineno-26-5" name="__codelineno-26-5" href="#__codelineno-26-5"></a>    <span class="s2">&quot;transformer.h&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="__span-26-6"><a id="__codelineno-26-6" name="__codelineno-26-6" href="#__codelineno-26-6"></a>    <span class="s2">&quot;transformer.ln_f&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
</span><span id="__span-26-7"><a id="__codelineno-26-7" name="__codelineno-26-7" href="#__codelineno-26-7"></a><span class="p">}</span>
</span></code></pre></div>
<p>ç„¶åå¦‚ä¸‹åŠ è½½æ¨¡å‹ï¼š</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-27-1"><a id="__codelineno-27-1" name="__codelineno-27-1" href="#__codelineno-27-1"></a><span class="n">model_8bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span><span id="__span-27-2"><a id="__codelineno-27-2" name="__codelineno-27-2" href="#__codelineno-27-2"></a>    <span class="s2">&quot;bigscience/bloom-1b7&quot;</span><span class="p">,</span>
</span><span id="__span-27-3"><a id="__codelineno-27-3" name="__codelineno-27-3" href="#__codelineno-27-3"></a>    <span class="n">device_map</span><span class="o">=</span><span class="n">device_map</span><span class="p">,</span>
</span><span id="__span-27-4"><a id="__codelineno-27-4" name="__codelineno-27-4" href="#__codelineno-27-4"></a>    <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">,</span>
</span><span id="__span-27-5"><a id="__codelineno-27-5" name="__codelineno-27-5" href="#__codelineno-27-5"></a><span class="p">)</span>
</span></code></pre></div>
<p>è¿™å°±æ˜¯å…¨éƒ¨å†…å®¹ï¼äº«å—æ‚¨çš„æ¨¡å‹å§ï¼</p>
<h4 id="llm_int8_threshold">ä½¿ç”¨<code>llm_int8_threshold</code></h4>
<p>æ‚¨å¯ä»¥ä½¿ç”¨ <code>llm_int8_threshold</code> å‚æ•°æ¥æ›´æ”¹å¼‚å¸¸å€¼çš„é˜ˆå€¼ã€‚â€œå¼‚å¸¸å€¼â€æ˜¯ä¸€ä¸ªå¤§äºç‰¹å®šé˜ˆå€¼çš„<code>hidden state</code>å€¼ã€‚
è¿™å¯¹åº”äº<code>LLM.int8()</code>è®ºæ–‡ä¸­æè¿°çš„å¼‚å¸¸æ£€æµ‹çš„å¼‚å¸¸é˜ˆå€¼ã€‚ä»»ä½•é«˜äºæ­¤é˜ˆå€¼çš„<code>hidden state</code>å€¼éƒ½å°†è¢«è§†ä¸ºå¼‚å¸¸å€¼ï¼Œå¯¹è¿™äº›å€¼çš„æ“ä½œå°†åœ¨ fp16 ä¸­å®Œæˆã€‚å€¼é€šå¸¸æ˜¯æ­£æ€åˆ†å¸ƒçš„ï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œå¤§å¤šæ•°å€¼åœ¨ [-3.5, 3.5] èŒƒå›´å†…ï¼Œä½†æœ‰ä¸€äº›é¢å¤–çš„ç³»ç»Ÿå¼‚å¸¸å€¼ï¼Œå¯¹äºå¤§æ¨¡å‹æ¥è¯´ï¼Œå®ƒä»¬çš„åˆ†å¸ƒéå¸¸ä¸åŒã€‚è¿™äº›å¼‚å¸¸å€¼é€šå¸¸åœ¨åŒºé—´ [-60, -6] æˆ– [6, 60] å†…ã€‚Int8 é‡åŒ–å¯¹äºå¹…åº¦ä¸º ~5 çš„å€¼æ•ˆæœå¾ˆå¥½ï¼Œä½†è¶…å‡ºè¿™ä¸ªèŒƒå›´ï¼Œæ€§èƒ½å°±ä¼šæ˜æ˜¾ä¸‹é™ã€‚ä¸€ä¸ªå¥½çš„é»˜è®¤é˜ˆå€¼æ˜¯ 6ï¼Œä½†å¯¹äºæ›´ä¸ç¨³å®šçš„æ¨¡å‹ï¼ˆå°æ¨¡å‹ã€å¾®è°ƒï¼‰å¯èƒ½éœ€è¦æ›´ä½çš„é˜ˆå€¼ã€‚
è¿™ä¸ªå‚æ•°ä¼šå½±å“æ¨¡å‹çš„æ¨ç†é€Ÿåº¦ã€‚æˆ‘ä»¬å»ºè®®å°è¯•è¿™ä¸ªå‚æ•°ï¼Œä»¥æ‰¾åˆ°æœ€é€‚åˆæ‚¨çš„ç”¨ä¾‹çš„å‚æ•°ã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-28-1"><a id="__codelineno-28-1" name="__codelineno-28-1" href="#__codelineno-28-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
</span><span id="__span-28-2"><a id="__codelineno-28-2" name="__codelineno-28-2" href="#__codelineno-28-2"></a>
</span><span id="__span-28-3"><a id="__codelineno-28-3" name="__codelineno-28-3" href="#__codelineno-28-3"></a><span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;bigscience/bloom-1b7&quot;</span>
</span><span id="__span-28-4"><a id="__codelineno-28-4" name="__codelineno-28-4" href="#__codelineno-28-4"></a>
</span><span id="__span-28-5"><a id="__codelineno-28-5" name="__codelineno-28-5" href="#__codelineno-28-5"></a><span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
</span><span id="__span-28-6"><a id="__codelineno-28-6" name="__codelineno-28-6" href="#__codelineno-28-6"></a>    <span class="n">llm_int8_threshold</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
</span><span id="__span-28-7"><a id="__codelineno-28-7" name="__codelineno-28-7" href="#__codelineno-28-7"></a><span class="p">)</span>
</span><span id="__span-28-8"><a id="__codelineno-28-8" name="__codelineno-28-8" href="#__codelineno-28-8"></a>
</span><span id="__span-28-9"><a id="__codelineno-28-9" name="__codelineno-28-9" href="#__codelineno-28-9"></a><span class="n">model_8bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span><span id="__span-28-10"><a id="__codelineno-28-10" name="__codelineno-28-10" href="#__codelineno-28-10"></a>    <span class="n">model_id</span><span class="p">,</span>
</span><span id="__span-28-11"><a id="__codelineno-28-11" name="__codelineno-28-11" href="#__codelineno-28-11"></a>    <span class="n">device_map</span><span class="o">=</span><span class="n">device_map</span><span class="p">,</span>
</span><span id="__span-28-12"><a id="__codelineno-28-12" name="__codelineno-28-12" href="#__codelineno-28-12"></a>    <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">,</span>
</span><span id="__span-28-13"><a id="__codelineno-28-13" name="__codelineno-28-13" href="#__codelineno-28-13"></a><span class="p">)</span>
</span><span id="__span-28-14"><a id="__codelineno-28-14" name="__codelineno-28-14" href="#__codelineno-28-14"></a><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
</span></code></pre></div>
<h4 id="_17">è·³è¿‡æŸäº›æ¨¡å—çš„è½¬æ¢</h4>
<p>ä¸€äº›æ¨¡å‹æœ‰å‡ ä¸ªéœ€è¦ä¿æŒæœªè½¬æ¢çŠ¶æ€ä»¥ç¡®ä¿ç¨³å®šæ€§çš„æ¨¡å—ã€‚ä¾‹å¦‚ï¼ŒJukebox æ¨¡å‹æœ‰å‡ ä¸ª <code>lm_head</code> æ¨¡å—éœ€è¦è·³è¿‡ã€‚ä½¿ç”¨ <code>llm_int8_skip_modules</code> å‚æ•°è¿›è¡Œç›¸åº”æ“ä½œã€‚</p>
<div class="language-python highlight"><pre><span></span><code><span id="__span-29-1"><a id="__codelineno-29-1" name="__codelineno-29-1" href="#__codelineno-29-1"></a><span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">AutoModelForCausalLM</span><span class="p">,</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">BitsAndBytesConfig</span>
</span><span id="__span-29-2"><a id="__codelineno-29-2" name="__codelineno-29-2" href="#__codelineno-29-2"></a>
</span><span id="__span-29-3"><a id="__codelineno-29-3" name="__codelineno-29-3" href="#__codelineno-29-3"></a><span class="n">model_id</span> <span class="o">=</span> <span class="s2">&quot;bigscience/bloom-1b7&quot;</span>
</span><span id="__span-29-4"><a id="__codelineno-29-4" name="__codelineno-29-4" href="#__codelineno-29-4"></a>
</span><span id="__span-29-5"><a id="__codelineno-29-5" name="__codelineno-29-5" href="#__codelineno-29-5"></a><span class="n">quantization_config</span> <span class="o">=</span> <span class="n">BitsAndBytesConfig</span><span class="p">(</span>
</span><span id="__span-29-6"><a id="__codelineno-29-6" name="__codelineno-29-6" href="#__codelineno-29-6"></a>    <span class="n">llm_int8_skip_modules</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;lm_head&quot;</span><span class="p">],</span>
</span><span id="__span-29-7"><a id="__codelineno-29-7" name="__codelineno-29-7" href="#__codelineno-29-7"></a><span class="p">)</span>
</span><span id="__span-29-8"><a id="__codelineno-29-8" name="__codelineno-29-8" href="#__codelineno-29-8"></a>
</span><span id="__span-29-9"><a id="__codelineno-29-9" name="__codelineno-29-9" href="#__codelineno-29-9"></a><span class="n">model_8bit</span> <span class="o">=</span> <span class="n">AutoModelForCausalLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
</span><span id="__span-29-10"><a id="__codelineno-29-10" name="__codelineno-29-10" href="#__codelineno-29-10"></a>    <span class="n">model_id</span><span class="p">,</span>
</span><span id="__span-29-11"><a id="__codelineno-29-11" name="__codelineno-29-11" href="#__codelineno-29-11"></a>    <span class="n">device_map</span><span class="o">=</span><span class="n">device_map</span><span class="p">,</span>
</span><span id="__span-29-12"><a id="__codelineno-29-12" name="__codelineno-29-12" href="#__codelineno-29-12"></a>    <span class="n">quantization_config</span><span class="o">=</span><span class="n">quantization_config</span><span class="p">,</span>
</span><span id="__span-29-13"><a id="__codelineno-29-13" name="__codelineno-29-13" href="#__codelineno-29-13"></a><span class="p">)</span>
</span><span id="__span-29-14"><a id="__codelineno-29-14" name="__codelineno-29-14" href="#__codelineno-29-14"></a><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_id</span><span class="p">)</span>
</span></code></pre></div>
<h4 id="8_1">å¾®è°ƒå·²åŠ è½½ä¸º8ä½ç²¾åº¦çš„æ¨¡å‹</h4>
<p>å€ŸåŠ©Hugging Faceç”Ÿæ€ç³»ç»Ÿä¸­é€‚é…å™¨ï¼ˆadaptersï¼‰çš„å®˜æ–¹æ”¯æŒï¼Œæ‚¨å¯ä»¥åœ¨8ä½ç²¾åº¦ä¸‹å¾®è°ƒæ¨¡å‹ã€‚è¿™ä½¿å¾—å¯ä»¥åœ¨å•ä¸ªGoogle Colabä¸­å¾®è°ƒå¤§æ¨¡å‹ï¼Œä¾‹å¦‚<code>flan-t5-large</code>æˆ–<code>facebook/opt-6.7b</code>ã€‚è¯·æŸ¥çœ‹<a href="https://github.com/huggingface/peft"><code>peft</code></a>åº“äº†è§£æ›´å¤šè¯¦æƒ…ã€‚</p>
<p>æ³¨æ„ï¼ŒåŠ è½½æ¨¡å‹è¿›è¡Œè®­ç»ƒæ—¶æ— éœ€ä¼ é€’<code>device_map</code>ã€‚å®ƒå°†è‡ªåŠ¨å°†æ‚¨çš„æ¨¡å‹åŠ è½½åˆ°GPUä¸Šã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨å¯ä»¥å°†è®¾å¤‡æ˜ å°„ä¸ºç‰¹å®šè®¾å¤‡ï¼ˆä¾‹å¦‚<code>cuda:0</code>ã€<code>0</code>ã€<code>torch.device('cuda:0')</code>ï¼‰ã€‚è¯·æ³¨æ„ï¼Œ<code>device_map=auto</code>ä»…åº”ç”¨äºæ¨ç†ã€‚</p>
<h3 id="bitsandbytesconfig">BitsAndBytesConfig</h3>
<p>[[autodoc]] BitsAndBytesConfig</p>
<h2 id="optimum">ä½¿ç”¨ ğŸ¤— <code>optimum</code> è¿›è¡Œé‡åŒ–</h2>
<p>è¯·æŸ¥çœ‹<a href="https://huggingface.co/docs/optimum/index">Optimum æ–‡æ¡£</a>ä»¥äº†è§£æ›´å¤šå…³äº<code>optimum</code>æ”¯æŒçš„é‡åŒ–æ–¹æ³•ï¼Œå¹¶æŸ¥çœ‹è¿™äº›æ–¹æ³•æ˜¯å¦é€‚ç”¨äºæ‚¨çš„ç”¨ä¾‹ã€‚</p>












                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../../../../../..", "features": ["navigation.tabs", "navigation.indexes", "navigation.instant", "navigation.sections", "navigation.top", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow", "content.tabs.link", "content.code.copy"], "search": "../../../../../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../../../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>