{"config":{"lang":["en","ru","ja"],"separator":"[\\s\\-,:!=\\[\\]()\"/']+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":""},{"location":"#how-to-cook","title":"how to cook","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"#_1","title":"Home","text":""},{"location":"docker/","title":"Docker","text":"<ol> <li>How to build docker for vLLM</li> </ol> <pre><code>sudo DOCKER_BUILDKIT=1 docker build -f docker/Dockerfile . \\\n  --target vllm-openai \\\n  --tag vllm-oylan-2_5:0.9.2-cu122 \\\n  --build-arg CUDA_VERSION=12.2.0 \\\n  --build-arg BUILD_BASE_IMAGE=nvidia/cuda:12.2.0-devel-ubuntu22.04 \\\n  --build-arg FINAL_BASE_IMAGE=nvidia/cuda:12.2.0-devel-ubuntu22.04 \\\n  --build-arg max_jobs=33 \\\n  --build-arg nvcc_threads=2 \\\n  --build-arg RUN_WHEEL_CHECK=false\n</code></pre> <ol> <li>Tar it</li> </ol> <pre><code>docker save -o vllm-oylan-2_5_0.9.2-cu124.tar vllm-oylan-2_5:0.9.2-cu124\n</code></pre> <ol> <li>Upload</li> </ol> <pre><code>huggingface-cli upload issai/docker_images_avlm_vLLM-int_ver1 vllm-oylan-2_5_0.9.2-cu124.tar\n</code></pre> <ol> <li>Download</li> </ol> <pre><code>wget --header=\"Authorization: Bearer &lt;YOUR_HF_TOKEN&gt;\" \\\n  \"https://huggingface.co/issai/docker_images_avlm_vLLM-int_ver1/resolve/main/vllm-oylan-2_5_0.9.2-cu124.tar\" \\\n  -O vllm-oylan-2_5_0.9.2-cu124.tar\n</code></pre> <ol> <li>Load it </li> </ol> <pre><code>docker load -i vllm-oylan-2_5_0.9.2-cu124.tar\n</code></pre> <ol> <li>Use it</li> </ol> <pre><code>bash docker_info/run_vllm_server_docker.sh\n</code></pre> <ol> <li>ybit docker step by step:</li> </ol> <pre><code>docker images\ndocker stop ki_oylan_a_v_t_2_5\ndocker rm ki_oylan_a_v_t_2_5\n</code></pre>"},{"location":"japanese-phrases/","title":"Japanese Phrases Collection \ud83c\uddef\ud83c\uddf5","text":""},{"location":"japanese-phrases/#daily-life","title":"\ud83d\udde3\ufe0f Daily Life","text":"\u8ab0\u304c\u30d0\u30ab\u3088\uff1f\u5b9f\u9a13\u53f0\u306b\u306a\u3063\u3066\u304b\u3089\u8a00\u3044\u306a\u3055\u3044\u3002\u6b21\u306f\u3084\u3055\u3057\u304f\u306d\u3002 <p>Romaji: Dare ga baka yo? Jikken-dai ni natte kara iinasai. Tsugi wa yasashiku ne.</p> <p>Translation: Who's the idiot? Say that after you become a test subject. Be gentle next time.</p> <p>Context: Playful/teasing, anime-style banter</p> <p>Grammar / Vocab: - \u3060\u308c \u2014 who - \u3070\u304b \u2014 idiot/fool - \u5b9f\u9a13\u53f0 \u2014 test subject - \u301c\u306a\u3063\u3066\u304b\u3089 \u2014 after becoming ~ - \u301c\u306a\u3055\u3044 \u2014 soft imperative 'do ~' - \u3084\u3055\u3057\u304f \u2014 gently/kindly</p>"},{"location":"japanese-phrases/#greetings","title":"\ud83d\udc4b Greetings","text":"\u304a\u306f\u3088\u3046\u3054\u3056\u3044\u307e\u3059 <p>Romaji: Ohayou gozaimasu</p> <p>Translation: Good morning (polite)</p> <p>Context: Standard morning greeting</p> <p>Grammar / Vocab: - \u304a\u306f\u3088\u3046 \u2014 good morning (casual) - \u3054\u3056\u3044\u307e\u3059 \u2014 polite form ending</p> \u304a\u75b2\u308c\u69d8\u3067\u3057\u305f <p>Romaji: Otsukaresama deshita</p> <p>Translation: Thank you for your hard work / Good job</p> <p>Context: Used when leaving work or after completing tasks</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/fatigue (polite) - \u69d8 \u2014 honorific suffix - \u3067\u3057\u305f \u2014 past tense polite form</p>"},{"location":"japanese-phrases/#emotions","title":"\ud83d\ude0a Emotions","text":"\u5b09\u3057\u3044\u3067\u3059 <p>Romaji: Ureshii desu</p> <p>Translation: I'm happy</p> <p>Context: Expressing happiness or joy</p> <p>Grammar / Vocab: - \u5b09\u3057\u3044 \u2014 happy/glad - \u3067\u3059 \u2014 polite form ending</p> \u60b2\u3057\u3044\u3067\u3059 <p>Romaji: Kanashii desu</p> <p>Translation: I'm sad</p> <p>Context: Expressing sadness</p> <p>Grammar / Vocab: - \u60b2\u3057\u3044 \u2014 sad - \u3067\u3059 \u2014 polite form ending</p>"},{"location":"japanese-phrases/#animemanga","title":"\ud83c\udf8c Anime/Manga","text":"\u3084\u3063\u3066\u3084\u308b\u305c\uff01 <p>Romaji: Yatte yaru ze!</p> <p>Translation: I'll do it! / I'll show them!</p> <p>Context: Determined, masculine speech from anime/manga</p> <p>Grammar / Vocab: - \u3084\u3063\u3066 \u2014 doing (te-form) - \u3084\u308b \u2014 to do (casual/rough) - \u305c \u2014 masculine sentence ending particle</p> \u4fe1\u3058\u3089\u308c\u306a\u3044 <p>Romaji: Shinjirarenai</p> <p>Translation: I can't believe it / Unbelievable</p> <p>Context: Common expression of disbelief</p> <p>Grammar / Vocab: - \u4fe1\u3058\u308b \u2014 to believe - \u3089\u308c\u306a\u3044 \u2014 potential negative form</p>"},{"location":"japanese-phrases/#how-to-add-new-phrases","title":"\u2795 How to Add New Phrases","text":"<p>Use this simple markdown structure:</p> <pre><code>??? info \"Japanese phrase\"\n\n    **Romaji:** Romanized text\n\n    **Translation:** English translation\n\n    **Context:** When/where it's used\n\n    **Grammar / Vocab:**\n    - **word** \u2014 definition\n    - **word** \u2014 definition\n</code></pre>"},{"location":"linux/","title":"Linux","text":"<p>I hate linux? nope, you are baka! </p> <p></p>"},{"location":"md_format_helpers/","title":"MD format helpers \ud83d\udd4a\ufe0f","text":""},{"location":"md_format_helpers/#show-video-show-image-show-audio","title":"Show video, Show image, Show audio","text":""},{"location":"md_format_helpers/#video","title":"Video","text":""},{"location":"md_format_helpers/#from-huggignface","title":"From Huggignface","text":"Your browser does not support the video tag."},{"location":"md_format_helpers/#from-youtube","title":"From YouTube","text":""},{"location":"md_format_helpers/#audio","title":"Audio","text":""},{"location":"md_format_helpers/#from-huggignface_1","title":"From Huggignface","text":""},{"location":"md_format_helpers/#image","title":"Image","text":""},{"location":"md_format_helpers/#from-huggignface_2","title":"From Huggignface","text":""},{"location":"ohayou/","title":"Ohayou","text":""},{"location":"japanese-phrases/","title":"Japanese Phrases Collection \ud83c\uddef\ud83c\uddf5","text":"<p>Welcome to the Japanese phrases collection! Click on any section in the sidebar to explore specific categories.</p>"},{"location":"japanese-phrases/#available-sections","title":"\ud83d\udcda Available Sections","text":"<ul> <li>\ud83d\udde3\ufe0f Daily Life - Common everyday phrases</li> <li>\ud83d\udc4b Greetings - Morning, evening, and social greetings  </li> <li>\ud83d\ude0a Emotions - Expressing feelings and emotions</li> <li>\ud83c\udf8c Anime/Manga - Popular phrases from Japanese media</li> </ul>"},{"location":"japanese-phrases/#how-to-add-new-phrases","title":"\u2795 How to Add New Phrases","text":"<p>Use this simple markdown structure:</p> <pre><code>??? info \"Japanese phrase\"\n\n    **Romaji:** Romanized text\n\n    **Translation:** English translation\n\n    **Context:** When/where it's used\n\n    **Grammar / Vocab:**\n    - **word** \u2014 definition\n    - **word** \u2014 definition\n</code></pre>"},{"location":"japanese-phrases/anime-manga/","title":"\ud83c\udf8c Anime/Manga","text":"<p>Popular phrases from Japanese anime and manga.</p>"},{"location":"japanese-phrases/anime-manga/#determined-expressions","title":"Determined Expressions","text":"\u3084\u3063\u3066\u3084\u308b\u305c\uff01 <p>Romaji: Yatte yaru ze!</p> <p>Translation: I'll do it! / I'll show them!</p> <p>Context: Determined, masculine speech from anime/manga</p> <p>Grammar / Vocab: - \u3084\u3063\u3066 \u2014 doing (te-form) - \u3084\u308b \u2014 to do (casual/rough) - \u305c \u2014 masculine sentence ending particle</p>"},{"location":"japanese-phrases/anime-manga/#reactions","title":"Reactions","text":"\u4fe1\u3058\u3089\u308c\u306a\u3044 <p>Romaji: Shinjirarenai</p> <p>Translation: I can't believe it / Unbelievable</p> <p>Context: Common expression of disbelief</p> <p>Grammar / Vocab: - \u4fe1\u3058\u308b \u2014 to believe - \u3089\u308c\u306a\u3044 \u2014 potential negative form</p>"},{"location":"japanese-phrases/daily-life/","title":"\ud83d\udde3\ufe0f Daily Life","text":"<p>Common phrases used in everyday Japanese conversation.</p>"},{"location":"japanese-phrases/daily-life/#basic-phrases","title":"Basic Phrases","text":"\u8ab0\u304c\u30d0\u30ab\u3088\uff1f\u5b9f\u9a13\u53f0\u306b\u306a\u3063\u3066\u304b\u3089\u8a00\u3044\u306a\u3055\u3044\u3002\u6b21\u306f\u3084\u3055\u3057\u304f\u306d\u3002 <p>Romaji: Dare ga baka yo? Jikken-dai ni natte kara iinasai. Tsugi wa yasashiku ne.</p> <p>Translation: Who's the idiot? Say that after you become a test subject. Be gentle next time.</p> <p>Context: Playful/teasing, anime-style banter</p> <p>Grammar / Vocab: - \u3060\u308c \u2014 who - \u3070\u304b \u2014 idiot/fool - \u5b9f\u9a13\u53f0 \u2014 test subject - \u301c\u306a\u3063\u3066\u304b\u3089 \u2014 after becoming ~ - \u301c\u306a\u3055\u3044 \u2014 soft imperative 'do ~' - \u3084\u3055\u3057\u304f \u2014 gently/kindly</p>"},{"location":"japanese-phrases/daily-life/shopping/","title":"Shopping Phrases","text":"<p>Essential phrases for shopping in Japan.</p> \u3053\u308c\u306f\u3044\u304f\u3089\u3067\u3059\u304b\uff1f <p>Romaji: Kore wa ikura desu ka?</p> <p>Translation: How much is this?</p> <p>Context: Asking for price when shopping</p> <p>Grammar / Vocab: - \u3053\u308c \u2014 this - \u3044\u304f\u3089 \u2014 how much - \u3067\u3059\u304b \u2014 polite question form</p> \u5b89\u304f\u3057\u3066\u304f\u3060\u3055\u3044 <p>Romaji: Yasuku shite kudasai</p> <p>Translation: Please make it cheaper</p> <p>Context: Negotiating price (mainly at markets)</p> <p>Grammar / Vocab: - \u5b89\u304f \u2014 cheaply - \u3057\u3066\u304f\u3060\u3055\u3044 \u2014 please do</p>"},{"location":"japanese-phrases/emotions/","title":"\ud83d\ude0a Emotions","text":"<p>Expressing feelings and emotions in Japanese.</p>"},{"location":"japanese-phrases/emotions/#happy-emotions","title":"Happy Emotions","text":"\u5b09\u3057\u3044\u3067\u3059 <p>Romaji: Ureshii desu</p> <p>Translation: I'm happy</p> <p>Context: Expressing happiness or joy</p> <p>Grammar / Vocab: - \u5b09\u3057\u3044 \u2014 happy/glad - \u3067\u3059 \u2014 polite form ending</p>"},{"location":"japanese-phrases/emotions/#sad-emotions","title":"Sad Emotions","text":"\u60b2\u3057\u3044\u3067\u3059 <p>Romaji: Kanashii desu</p> <p>Translation: I'm sad</p> <p>Context: Expressing sadness</p> <p>Grammar / Vocab: - \u60b2\u3057\u3044 \u2014 sad - \u3067\u3059 \u2014 polite form ending</p>"},{"location":"japanese-phrases/greetings/","title":"\ud83d\udc4b Greetings","text":"<p>Standard Japanese greetings for different situations.</p>"},{"location":"japanese-phrases/greetings/#morning-greetings","title":"Morning Greetings","text":"\u304a\u306f\u3088\u3046\u3054\u3056\u3044\u307e\u3059 <p>Romaji: Ohayou gozaimasu</p> <p>Translation: Good morning (polite)</p> <p>Context: Standard morning greeting</p> <p>Grammar / Vocab: - \u304a\u306f\u3088\u3046 \u2014 good morning (casual) - \u3054\u3056\u3044\u307e\u3059 \u2014 polite form ending</p>"},{"location":"japanese-phrases/greetings/#worksocial-greetings","title":"Work/Social Greetings","text":"\u304a\u75b2\u308c\u69d8\u3067\u3057\u305f <p>Romaji: Otsukaresama deshita</p> <p>Translation: Thank you for your hard work / Good job</p> <p>Context: Used when leaving work or after completing tasks</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/fatigue (polite) - \u69d8 \u2014 honorific suffix - \u3067\u3057\u305f \u2014 past tense polite form</p>"},{"location":"japanese-phrases/greetings/casual/","title":"Casual Greetings","text":"<p>Informal greetings used with friends and family.</p> \u304a\u3063\u3059\uff01 <p>Romaji: Ossu!</p> <p>Translation: Hey! / What's up!</p> <p>Context: Very casual greeting, mainly used by males</p> <p>Grammar / Vocab: - \u304a\u3063\u3059 \u2014 casual \"hey\" (masculine)</p> \u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p>"},{"location":"japanese-phrases/greetings/casual/#c2","title":"C2","text":""},{"location":"japanese-phrases/greetings/casual/#sad","title":"sad","text":"\u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p> \u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p> \u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p>"},{"location":"japanese-phrases/greetings/casual/#sad_1","title":"sad","text":"<p>Informal greetings used with friends and family.</p> \u304a\u3063\u3059\uff01 <p>Romaji: Ossu!</p> <p>Translation: Hey! / What's up!</p> <p>Context: Very casual greeting, mainly used by males</p> <p>Grammar / Vocab: - \u304a\u3063\u3059 \u2014 casual \"hey\" (masculine)</p> \u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p>"},{"location":"japanese-phrases/greetings/casual/#c2_1","title":"C2","text":""},{"location":"japanese-phrases/greetings/casual/#sad_2","title":"sad","text":"\u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p> \u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p> \u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p>"},{"location":"japanese-phrases/greetings/casual/#sad_3","title":"sad","text":""},{"location":"vllm/open_ai_vllm_example_a_v_t/","title":"OpenAI example inference for vLLM Online serve","text":""},{"location":"vllm/open_ai_vllm_example_a_v_t/#text-inference","title":"Text inference","text":"<pre><code>import time\nimport openai\n\nclient = openai.Client(\n    base_url=\"http://localhost:6655/v1\", api_key=\"EMPTY\"\n)\nMODEL = \"ki_oylan_a_v_t_2_5\"\n\n\nstart_time = time.perf_counter()\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"\u0422\u044b \u043b\u044e\u0431\u0438\u0448\u044c \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u0430\u043d\u0438\u043c\u0435. /no_think\"},\n        {\"role\": \"user\", \"content\": \"\u041a\u0430\u043a\u043e\u0435 \u0443 \u0442\u0435\u0431\u044f \u043b\u044e\u0431\u0438\u043c\u043e\u0435 \u0430\u043d\u0438\u043c\u0435?\"},\n    ],\n    temperature=0,\n    max_tokens=256,\n    stream=True\n)\n\nfirst_token_time = None\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        if first_token_time is None:  # first token arrived\n            first_token_time = time.perf_counter()\n            ttft = first_token_time - start_time\n            print(f\"\\n\\nTTFT: {ttft:.3f} seconds\\n\")\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"},{"location":"vllm/open_ai_vllm_example_a_v_t/#image-inference","title":"Image inference","text":"<pre><code># Image inference\nimport time\nfrom pathlib import Path\nimport openai\nimport base64\nclient = openai.Client(\n    base_url=\"http://localhost:6655/v1\", api_key=\"EMPTY\"\n)\nMODEL = \"ki_l\"\n\n# ########################################################################\n# ################ if you want to use local file #########################\n# ########################################################################\n# IMAGE_PATH = Path(\"/home/vladimir_albrekht/projects/digital_bridge/vllm/1_vladimir_utils/utils/benchs_perf/assets/cute_girl.jpg\")\n\n\n# def guess_mime(path: Path) -&gt; str:\n#     \"\"\"Guess MIME type from file extension\"\"\"\n#     ext = path.suffix.lower()\n#     if ext in [\".jpg\", \".jpeg\"]:\n#         return \"image/jpeg\"\n#     elif ext == \".png\":\n#         return \"image/png\"\n#     elif ext == \".webp\":\n#         return \"image/webp\"\n#     elif ext == \".gif\":\n#         return \"image/gif\"\n#     else:\n#         return \"image/jpeg\"  # Fallback\n\n# def encode_image(image_path: Path) -&gt; str:\n#     \"\"\"Encode image file to base64 string\"\"\"\n#     return base64.b64encode(image_path.read_bytes()).decode(\"utf-8\")\n\n# base64_image = encode_image(IMAGE_PATH)\n# mime_type = guess_mime(IMAGE_PATH)\n# # data:{mime_type};base64,{base64_image}\n# ########################################################################\n\n\n\nstart_time = time.perf_counter()\nresponse = client.chat.completions.create(\n  model=MODEL,\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Describe this image in detail\", # make sure to inclue &lt;image&gt; otherwise it will crush.\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\":  f\"https://huggingface.co/datasets/CCRss/kv_brain/resolve/main/Xnip2025-08-24_15-02-37.jpg\" # for local path `data:{mime_type};base64,{base64_image}`\n          },\n        },\n      ],\n    }\n  ],\n  max_tokens=256,\n  stream=True,\n  temperature=0.1\n)\n\n\nfirst_token_time = None\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        if first_token_time is None:  # first token arrived\n            first_token_time = time.perf_counter()\n            ttft = first_token_time - start_time\n            print(f\"\\n\\nTTFT: {ttft:.3f} seconds\\n\")\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"},{"location":"vllm/open_ai_vllm_example_a_v_t/#audio-inference","title":"Audio inference","text":"<pre><code># Audio request\nimport base64\nimport io\nfrom openai import OpenAI\nimport torch\nimport requests\nimport soundfile as sf\nimport numpy as np\n\n\nclient = OpenAI(base_url=\"http://localhost:6655/v1\", api_key=\"EMPTY\")\nMODEL = \"ki_oylan_a_v_t_2_5\"\n\n\ndef encode_audio_from_url(url: str) -&gt; str:\n    resp = requests.get(url)\n    resp.raise_for_status()\n    data, sr = sf.read(io.BytesIO(resp.content))  # reads MP3/WAV/etc.\n    if data.ndim &gt; 1:\n        data = np.mean(data, axis=1, keepdims=True)  # mono\n    if sr != 16000:\n        import torchaudio\n        tensor = torchaudio.functional.resample(torch.from_numpy(data.T), sr, 16000)\n        sr = 16000\n        data = tensor.numpy().T\n    with io.BytesIO() as buffer:\n        sf.write(buffer, data, sr, format=\"WAV\")\n        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n\n########################################################################\n################ if you want to use local file #########################\n########################################################################\n# def convert_to_wav_bytes(path: Path) -&gt; bytes:\n#     ext = path.suffix.lower()\n#     if ext == \".wav\":\n#         return path.read_bytes()  # Already in WAV format\n\n#     elif ext == \".mp3\":\n#         # Load MP3, convert to WAV format (16kHz mono float32)\n#         waveform, sr = torchaudio.load(path)\n#         if waveform.shape[0] &gt; 1:\n#             waveform = waveform.mean(dim=0, keepdim=True)  # Make mono\n#         if sr != 16000:\n#             resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n#             waveform = resampler(waveform)\n\n#         # Save to in-memory WAV\n#         with io.BytesIO() as buffer:\n#             torchaudio.save(buffer, waveform, 16000, format=\"wav\")\n#             return buffer.getvalue()\n\n#     else:\n#         raise ValueError(f\"Unsupported audio format: {ext}\")\n\n# def encode_audio(audio_path: Path) -&gt; str:\n#     \"\"\"Encode audio file to base64 string\"\"\"\n#     wav_bytes = convert_to_wav_bytes(audio_path)\n#     return base64.b64encode(wav_bytes).decode(\"utf-8\")\n\n# base64_audio = encode_audio(AUDIO_PATH)\n########################################################################\n\nstart_time = time.perf_counter()\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Your name is Oylan, you are a useful multi-modal large language model developed by ISSAI, Kazakhstan. /no_think\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Who is talking in this audio?\"\n                },\n                {\n                    \"type\": \"audio_url\",\n                    \"audio_url\": {\n                        \"url\": f\"data:audio/wav;base64,{encode_audio_from_url(\"https://huggingface.co/datasets/CCRss/kv_brain/resolve/main/yes_my_lord.mp3\")}\"\n                    }\n                }\n            ]\n        }\n    ],\n    max_tokens=512,\n    temperature=0.1,\n    stream=True\n)\n\n# if stream=false\n# print(\"\u2705 Response:\", response.choices[0].message.content)\nfirst_token_time = None\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        if first_token_time is None:  # first token arrived\n            first_token_time = time.perf_counter()\n            ttft = first_token_time - start_time\n            print(f\"\\n\\nTTFT: {ttft:.3f} seconds\\n\")\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"}]}