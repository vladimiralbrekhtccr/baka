{"config":{"lang":["en","ru","ja"],"separator":"[\\s\\-,:!=\\[\\]()\"/']+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":""},{"location":"#how-to-cook","title":"how to cook","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"#_1","title":"Home","text":""},{"location":"docker/","title":"Docker","text":"<ol> <li>How to build docker for vLLM</li> </ol> <pre><code>sudo DOCKER_BUILDKIT=1 docker build -f docker/Dockerfile . \\\n  --target vllm-openai \\\n  --tag vllm-oylan-2_5:0.9.2-cu122 \\\n  --build-arg CUDA_VERSION=12.2.0 \\\n  --build-arg BUILD_BASE_IMAGE=nvidia/cuda:12.2.0-devel-ubuntu22.04 \\\n  --build-arg FINAL_BASE_IMAGE=nvidia/cuda:12.2.0-devel-ubuntu22.04 \\\n  --build-arg max_jobs=33 \\\n  --build-arg nvcc_threads=2 \\\n  --build-arg RUN_WHEEL_CHECK=false\n</code></pre> <ol> <li>Tar it</li> </ol> <pre><code>docker save -o vllm-oylan-2_5_0.9.2-cu124.tar vllm-oylan-2_5:0.9.2-cu124\n</code></pre> <ol> <li>Upload</li> </ol> <pre><code>huggingface-cli upload issai/docker_images_avlm_vLLM-int_ver1 vllm-oylan-2_5_0.9.2-cu124.tar\n</code></pre> <ol> <li>Download</li> </ol> <pre><code>wget --header=\"Authorization: Bearer &lt;YOUR_HF_TOKEN&gt;\" \\\n  \"https://huggingface.co/issai/docker_images_avlm_vLLM-int_ver1/resolve/main/vllm-oylan-2_5_0.9.2-cu124.tar\" \\\n  -O vllm-oylan-2_5_0.9.2-cu124.tar\n</code></pre> <ol> <li>Load it </li> </ol> <pre><code>docker load -i vllm-oylan-2_5_0.9.2-cu124.tar\n</code></pre> <ol> <li>Use it</li> </ol> <pre><code>bash docker_info/run_vllm_server_docker.sh\n</code></pre> <ol> <li>ybit docker step by step:</li> </ol> <pre><code>docker images\ndocker stop ki_oylan_a_v_t_2_5\ndocker rm ki_oylan_a_v_t_2_5\n</code></pre>"},{"location":"japanese-phrases/","title":"Japanese Phrases Collection \ud83c\uddef\ud83c\uddf5","text":"<p>https://chatgpt.com/c/68bc3198-c084-8321-a95b-144cb57a1700</p>"},{"location":"japanese-phrases/#daily-life","title":"\ud83d\udde3\ufe0f Daily Life","text":"\u8ab0\u304c\u30d0\u30ab\u3088\uff1f\u5b9f\u9a13\u53f0\u306b\u306a\u3063\u3066\u304b\u3089\u8a00\u3044\u306a\u3055\u3044\u3002\u6b21\u306f\u3084\u3055\u3057\u304f\u306d\u3002 <p>Romaji: Dare ga baka yo? Jikken-dai ni natte kara iinasai. Tsugi wa yasashiku ne.</p> <p>Translation: Who's the idiot? Say that after you become a test subject. Be gentle next time.</p> <p>Context: Playful/teasing, anime-style banter</p> <p>Grammar / Vocab: - \u3060\u308c \u2014 who - \u3070\u304b \u2014 idiot/fool - \u5b9f\u9a13\u53f0 \u2014 test subject - \u301c\u306a\u3063\u3066\u304b\u3089 \u2014 after becoming ~ - \u301c\u306a\u3055\u3044 \u2014 soft imperative 'do ~' - \u3084\u3055\u3057\u304f \u2014 gently/kindly</p>"},{"location":"japanese-phrases/#greetings","title":"\ud83d\udc4b Greetings","text":"\u304a\u306f\u3088\u3046\u3054\u3056\u3044\u307e\u3059 <p>Romaji: Ohayou gozaimasu</p> <p>Translation: Good morning (polite)</p> <p>Context: Standard morning greeting</p> <p>Grammar / Vocab: - \u304a\u306f\u3088\u3046 \u2014 good morning (casual) - \u3054\u3056\u3044\u307e\u3059 \u2014 polite form ending</p> \u304a\u75b2\u308c\u69d8\u3067\u3057\u305f <p>Romaji: Otsukaresama deshita</p> <p>Translation: Thank you for your hard work / Good job</p> <p>Context: Used when leaving work or after completing tasks</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/fatigue (polite) - \u69d8 \u2014 honorific suffix - \u3067\u3057\u305f \u2014 past tense polite form</p>"},{"location":"japanese-phrases/#emotions","title":"\ud83d\ude0a Emotions","text":"\u5b09\u3057\u3044\u3067\u3059 <p>Romaji: Ureshii desu</p> <p>Translation: I'm happy</p> <p>Context: Expressing happiness or joy</p> <p>Grammar / Vocab: - \u5b09\u3057\u3044 \u2014 happy/glad - \u3067\u3059 \u2014 polite form ending</p> \u60b2\u3057\u3044\u3067\u3059 <p>Romaji: Kanashii desu</p> <p>Translation: I'm sad</p> <p>Context: Expressing sadness</p> <p>Grammar / Vocab: - \u60b2\u3057\u3044 \u2014 sad - \u3067\u3059 \u2014 polite form ending</p>"},{"location":"japanese-phrases/#animemanga","title":"\ud83c\udf8c Anime/Manga","text":"\u3084\u3063\u3066\u3084\u308b\u305c\uff01 <p>Romaji: Yatte yaru ze!</p> <p>Translation: I'll do it! / I'll show them!</p> <p>Context: Determined, masculine speech from anime/manga</p> <p>Grammar / Vocab: - \u3084\u3063\u3066 \u2014 doing (te-form) - \u3084\u308b \u2014 to do (casual/rough) - \u305c \u2014 masculine sentence ending particle</p> \u4fe1\u3058\u3089\u308c\u306a\u3044 <p>Romaji: Shinjirarenai</p> <p>Translation: I can't believe it / Unbelievable</p> <p>Context: Common expression of disbelief</p> <p>Grammar / Vocab: - \u4fe1\u3058\u308b \u2014 to believe - \u3089\u308c\u306a\u3044 \u2014 potential negative form</p>"},{"location":"japanese-phrases/#how-to-add-new-phrases","title":"\u2795 How to Add New Phrases","text":"<p>Use this simple markdown structure:</p> <pre><code>??? info \"Japanese phrase\"\n\n    **Romaji:** Romanized text\n\n    **Translation:** English translation\n\n    **Context:** When/where it's used\n\n    **Grammar / Vocab:**\n    - **word** \u2014 definition\n    - **word** \u2014 definition\n</code></pre>"},{"location":"linux/","title":"Linux","text":"<p>I hate linux? nope, you are baka! </p> <p></p>"},{"location":"linux/#give-permisison-to-writeread","title":"Give permisison to write/read","text":"<pre><code>chmod -R 770 /home/vladimir_albrekht\n</code></pre>"},{"location":"md_format_helpers/","title":"MD format helpers \ud83d\udd4a\ufe0f","text":""},{"location":"md_format_helpers/#show-video-show-image-show-audio","title":"Show video, Show image, Show audio","text":""},{"location":"md_format_helpers/#video","title":"Video","text":""},{"location":"md_format_helpers/#from-huggignface","title":"From Huggignface","text":"Your browser does not support the video tag."},{"location":"md_format_helpers/#from-youtube","title":"From YouTube","text":""},{"location":"md_format_helpers/#audio","title":"Audio","text":""},{"location":"md_format_helpers/#from-huggignface_1","title":"From Huggignface","text":""},{"location":"md_format_helpers/#image","title":"Image","text":""},{"location":"md_format_helpers/#from-huggignface_2","title":"From Huggignface","text":""},{"location":"md_format_helpers/#notes","title":"Notes","text":"<p>[!Note] Qwen3-Next-80B-A3B-Instruct supports only instruct (non-thinking) mode and does not generate <code>&lt;think&gt;&lt;/think&gt;</code> blocks in its output.</p>"},{"location":"moe/","title":"Mixture of experts","text":""},{"location":"moe/#components-of-moe","title":"Components of Moe","text":"<ol> <li> <p>Sparse MoE Layers instead of Feed-forward (FFN)</p> </li> <li> <p>Gate network or router - determines which tokens are sent to which expert.</p> </li> </ol> <p>Qwen3 pipeline closely mirrors DeepSeek R1 That means DeepSeek R1 paper is all you need</p> <p>Question: - In training what is verifiable rewards ?</p>"},{"location":"moe/#qwen3moe-next","title":"Qwen3Moe-Next","text":"<p>qwen blog</p> <p>[!Note] Switch to seperate think/no_think models why? It's some kind of new trend hmm.</p> <p>Qwen3-Next innovations: - hybrid attention mechanism - highly sparse (MoE) - training-stability-friendly optimizations - multi-token prediction mechanism</p> <p>Model architecture:</p> <pre><code>amount of layers = 48 [ 32 -delta net, \n                        12 std attention]\n\nQwen3NextForCausalLM(\n  (model): Qwen3NextModel(\n    (embed_tokens): Embedding(151936, 2048)\n    (layers): ModuleList(\n      (0-2): 3 x Qwen3NextDecoderLayer(\n        (linear_attn): Qwen3NextGatedDeltaNet(\n          (act): SiLU()\n          (conv1d): Conv1d(8192, 8192, kernel_size=(4,), stride=(1,), padding=(3,), groups=8192, bias=False)\n          (in_proj_qkvz): Linear(in_features=2048, out_features=12288, bias=False)\n          (in_proj_ba): Linear(in_features=2048, out_features=64, bias=False)\n          (norm): Qwen3NextRMSNormGated()\n          (out_proj): Linear(in_features=4096, out_features=2048, bias=False)\n        )\n        (mlp): Qwen3NextSparseMoeBlock(\n          (gate): Linear(in_features=2048, out_features=512, bias=False)\n          (experts): ModuleList(\n            (0-511): 512 x Qwen3NextMLP(\n              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n              (act_fn): SiLU()\n            )\n          )\n          (shared_expert): Qwen3NextMLP(\n            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n            (act_fn): SiLU()\n          )\n          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n        )\n        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n      )\n      (3): Qwen3NextDecoderLayer(\n        (self_attn): Qwen3NextAttention(\n          (q_proj): Linear(in_features=2048, out_features=8192, bias=False)\n          (k_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (v_proj): Linear(in_features=2048, out_features=512, bias=False)\n          (o_proj): Linear(in_features=4096, out_features=2048, bias=False)\n          (q_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n          (k_norm): Qwen3NextRMSNorm((256,), eps=1e-06)\n        )\n        (mlp): Qwen3NextSparseMoeBlock(\n          (gate): Linear(in_features=2048, out_features=512, bias=False)\n          (experts): ModuleList(\n            (0-511): 512 x Qwen3NextMLP(\n              (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n              (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n              (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n              (act_fn): SiLU()\n            )\n          )\n          (shared_expert): Qwen3NextMLP(\n            (gate_proj): Linear(in_features=2048, out_features=512, bias=False)\n            (up_proj): Linear(in_features=2048, out_features=512, bias=False)\n            (down_proj): Linear(in_features=512, out_features=2048, bias=False)\n            (act_fn): SiLU()\n          )\n          (shared_expert_gate): Linear(in_features=2048, out_features=1, bias=False)\n        )\n        (input_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n        (post_attention_layernorm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n      )\n    )\n    (norm): Qwen3NextRMSNorm((2048,), eps=1e-06)\n    (rotary_emb): Qwen3NextRotaryEmbedding()\n  )\n  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n)\n</code></pre>"},{"location":"moe/#-80b-a3b","title":"- 80B-A3B","text":"<p>Comparison:</p> <ul> <li>performnace of Qwen3-32B model, using less than 10% of its training cost (GPU hours)</li> <li>In inference, especially with context lengths over 32K tokens, it delivers more than 10x higher throughput \u2014 achieving extreme efficiency in both training and inference.</li> </ul>"},{"location":"ohayou/","title":"Ohayou","text":""},{"location":"uv/","title":"UV UwU","text":"<p>UwU \ud83e\udd17</p>"},{"location":"uv/#install-uv","title":"Install UV","text":""},{"location":"whisper_TST/","title":"Whisper Target Speaker Transcription","text":"<p>https://chatgpt.com/c/68c112be-9250-832c-bd09-a855eddba42f</p> <p>Whisper that can Transcribe only target speaker when there is noisy or other speakers on the background</p> <p>Input: Audio (with different speakers)</p> <p>Output:  Transcription with target speaker.</p> <p>Topics:</p> <ul> <li>Target Speech Extraction (TSE)</li> <li>Target-Speaker Voice Activity Detection (TS-VAD)</li> <li>Target-Speaker ASR (TS-ASR)</li> <li>Personalized ASR</li> </ul> <p>If before ASR:</p> <p>Target Speech Extraction (TSE) / VoiceFilter / SpeakerBeam: separate out one desired speaker given a short \u201cenrollment\u201d clip of them. Google\u2019s VoiceFilter-Lite is a classic on-device approach; NTT\u2019s SpeakerBeam is a strong research line. Use this before ASR.</p>"},{"location":"hackathon/","title":"Hack","text":""},{"location":"hackathon/#task-6-bi-gpt-natural-language-sql","title":"Task 6 \u0437\u0430\u0434\u0430\u0447\u0430: BI-GPT: \u0430\u0433\u0435\u043d\u0442 \u043f\u043e \u043a\u043e\u0440\u043f\u043e\u0440\u0430\u0442\u0438\u0432\u043d\u043e\u0439 \u0411\u0414 (Natural Language \u2192 SQL)","text":"<p>\u041a\u043e\u043d\u0442\u0435\u043a\u0441\u0442:</p> <p>\u0420\u0443\u043a\u043e\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u0438 \u0445\u043e\u0442\u044f\u0442 \u0437\u0430\u0434\u0430\u0432\u0430\u0442\u044c BI-\u0432\u043e\u043f\u0440\u043e\u0441\u044b \u201c\u043f\u043e-\u0447\u0435\u043b\u043e\u0432\u0435\u0447\u0435\u0441\u043a\u0438\u201d: \u043f\u0440\u0438\u0431\u044b\u043b\u044c \u0437\u0430 2 \u0434\u043d\u044f, \u043c\u0430\u0440\u0436\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0441\u0442\u044c \u0437\u0430 \u043c\u0435\u0441\u044f\u0446, \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u0447\u0435\u043a, \u043e\u0441\u0442\u0430\u0442\u043a\u0438  </p> <p>\u0426\u0435\u043b\u044c:</p> <p>\u0410\u0433\u0435\u043d\u0442, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u043f\u043e\u043d\u0438\u043c\u0430\u0435\u0442 NL-\u0437\u0430\u043f\u0440\u043e\u0441\u044b, \u0433\u0435\u043d\u0435\u0440\u0438\u0440\u0443\u0435\u0442 \u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u044b\u0439 SQL, \u0443\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u0442 \u0431\u0438\u0437\u043d\u0435\u0441-\u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0438 \u0432\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u044b\u0435 \u0430\u0433\u0440\u0435\u0433\u0430\u0442\u044b \u0441 \u043f\u043e\u044f\u0441\u043d\u0435\u043d\u0438\u0435\u043c  </p> <p>\u041c\u0435\u0442\u0440\u0438\u043a\u0438:</p> <ul> <li>ML/AI: exact match/exec accuracy SQL (\u0431\u0435\u0437 \u043e\u0448\u0438\u0431\u043e\u043a), rate \u043a\u043e\u0440\u0440\u0435\u043a\u0442\u043d\u044b\u0445 \u0430\u0433\u0440\u0435\u0433\u0430\u0442\u043e\u0432 \u043d\u0430 golden-queries, hallucination rate (\u2193)  </li> <li>\u0411\u0438\u0437\u043d\u0435\u0441: % \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432 \u0437\u0430\u043a\u0440\u044b\u0442\u044b self-service \u0431\u0435\u0437 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0430, \u0432\u0440\u0435\u043c\u044f \u043e\u0442\u0432\u0435\u0442\u0430, \u0441\u043d\u0438\u0436\u0435\u043d\u0438\u0435 \u043e\u0447\u0435\u0440\u0435\u0434\u0438 \u0432 BI  </li> <li>\u0418\u043d\u0436/\u0431\u0435\u0437\u043e\u043f\u0430\u0441\u043d\u043e\u0441\u0442\u044c: \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u0435 PII-\u0443\u0442\u0435\u0447\u0435\u043a, \u043b\u0438\u043c\u0438\u0442 \u0432\u0440\u0435\u043c\u0435\u043d\u0438/\u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u0438 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432  </li> </ul> <p>\u0420\u0435\u0444\u0435\u0440\u0435\u043d\u0441\u044b:</p> <ul> <li>\u201cText-to-SQL in the Wild\u201d (Yu et al., 2018, SPIDER)  </li> <li>\u201cRAG for Knowledge-Intensive NLP\u201d (Lewis et al., 2020)  </li> </ul>"},{"location":"hackathon/#send-simple-tool-call-request","title":"Send simple tool call request","text":"<pre><code>from openai import OpenAI\nimport json\nfrom datetime import datetime\n\nclient = OpenAI(base_url=\"https://a7657d0d699c.ngrok-free.app/v1\", api_key=\"dummy\")\n\n# --- Tool Function Definitions ---\n\ndef get_weather(location: str, unit: str = \"celsius\"):\n    \"\"\"Gets the current weather for a given location.\"\"\"\n    print(f\"--- Calling get_weather(location='{location}', unit='{unit}') ---\")\n    if \"astana\" in location.lower():\n        return f\"The weather in Astana is currently cool and clear at 8 degrees {unit}.\"\n    else:\n        return f\"The weather in {location} is currently 20 degrees {unit}.\"\n\ndef schedule_meeting(participants: list, title: str, time: str, location: str = \"Online\", duration_minutes: int = 30):\n    \"\"\"\n    Schedules a meeting with a list of participants at a specified time.\n    Time should be in ISO 8601 format, e.g., 2025-09-16T14:00:00.\n    \"\"\"\n    print(f\"--- Calling schedule_meeting() ---\")\n    participant_str = \", \".join(participants)\n    return (\n        f\"Meeting Scheduled Successfully!\\n\"\n        f\"\\tTitle: {title}\\n\"\n        f\"\\tTime: {time}\\n\"\n        f\"\\tParticipants: {participant_str}\\n\"\n        f\"\\tLocation: {location}\\n\"\n        f\"\\tDuration: {duration_minutes} minutes\"\n    )\n\n# --- Mapping tool names to the actual Python functions ---\ntool_functions = {\n    \"get_weather\": get_weather,\n    \"schedule_meeting\": schedule_meeting\n}\n\n# --- Tool Schemas for the Model ---\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\"type\": \"string\", \"description\": \"City and state, e.g., 'San Francisco, CA'\"},\n                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n                },\n                \"required\": [\"location\"] # Note: I made 'unit' optional here\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"schedule_meeting\",\n            \"description\": \"Schedules a calendar event or meeting.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"participants\": {\n                        \"type\": \"array\",\n                        \"items\": {\"type\": \"string\"},\n                        \"description\": \"A list of people to invite to the meeting.\"\n                    },\n                    \"title\": {\n                        \"type\": \"string\",\n                        \"description\": \"The title or subject of the meeting.\"\n                    },\n                    \"time\": {\n                        \"type\": \"string\",\n                        \"description\": \"The start time of the meeting in ISO 8601 format, e.g., YYYY-MM-DDTHH:MM:SS\"\n                    },\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The location of the meeting. Defaults to 'Online' if not specified.\"\n                    },\n                    \"duration_minutes\": {\n                        \"type\": \"integer\",\n                        \"description\": \"The duration of the meeting in minutes. Defaults to 30.\"\n                    }\n                },\n                \"required\": [\"participants\", \"title\", \"time\"]\n            }\n        }\n    }\n]\n\n# --- User Prompt ---\n# A natural language prompt that contains all the necessary info for the new tool.\n# Since today is September 15th, \"tomorrow at 2 PM\" should be resolved to Sept 16th.\nmessages = [\n    {\"role\": \"user\", \"content\": \"Hey, can you schedule a meeting for me and Vladimir about the 'Qwen3 Project Plan' for tomorrow at 2 PM? It should be 45 minutes long and take place in the main conference room.\"}\n]\n\nprint(\"Sending request to the model...\")\nresponse = client.chat.completions.create(\n    model=\"kita\", # Use your served model name\n    messages=messages,\n    tools=tools,\n    tool_choice=\"auto\"\n)\n\n# --- Robustly handle the response ---\nmessage = response.choices[0].message\n\nif message.tool_calls:\n    print(\"\u2705 Model decided to call a tool.\")\n    tool_call = message.tool_calls[0]\n    function_name = tool_call.function.name\n\n    if function_name in tool_functions:\n        arguments = json.loads(tool_call.function.arguments)\n        print(f\"Function to call: {function_name}\")\n        print(f\"Arguments: {arguments}\")\n\n        function_to_call = tool_functions[function_name]\n        result = function_to_call(**arguments)\n\n        print(\"\\n--- Result ---\")\n        print(result)\n    else:\n        print(f\"\u26a0\ufe0f Model tried to call an unknown function: {function_name}\")\n\nelse:\n    print(\"\u274c Model responded with text instead of a tool call.\")\n    print(f\"   Model Response: {message.content}\")\n</code></pre>"},{"location":"japanese-phrases/","title":"Japanese Phrases Collection \ud83c\uddef\ud83c\uddf5","text":"<p>Welcome to the Japanese phrases collection! Click on any section in the sidebar to explore specific categories.</p>"},{"location":"japanese-phrases/#available-sections","title":"\ud83d\udcda Available Sections","text":"<ul> <li>\ud83d\udde3\ufe0f Daily Life - Common everyday phrases</li> <li>\ud83d\udc4b Greetings - Morning, evening, and social greetings  </li> <li>\ud83d\ude0a Emotions - Expressing feelings and emotions</li> <li>\ud83c\udf8c Anime/Manga - Popular phrases from Japanese media</li> </ul>"},{"location":"japanese-phrases/#how-to-add-new-phrases","title":"\u2795 How to Add New Phrases","text":"<p>Use this simple markdown structure:</p> <pre><code>??? info \"Japanese phrase\"\n\n    **Romaji:** Romanized text\n\n    **Translation:** English translation\n\n    **Context:** When/where it's used\n\n    **Grammar / Vocab:**\n    - **word** \u2014 definition\n    - **word** \u2014 definition\n</code></pre>"},{"location":"japanese-phrases/anime-manga/","title":"\ud83c\udf8c Anime/Manga","text":"<p>Popular phrases from Japanese anime and manga.</p>"},{"location":"japanese-phrases/anime-manga/#determined-expressions","title":"Determined Expressions","text":"\u3084\u3063\u3066\u3084\u308b\u305c\uff01 <p>Romaji: Yatte yaru ze!</p> <p>Translation: I'll do it! / I'll show them!</p> <p>Context: Determined, masculine speech from anime/manga</p> <p>Grammar / Vocab: - \u3084\u3063\u3066 \u2014 doing (te-form) - \u3084\u308b \u2014 to do (casual/rough) - \u305c \u2014 masculine sentence ending particle</p>"},{"location":"japanese-phrases/anime-manga/#reactions","title":"Reactions","text":"\u4fe1\u3058\u3089\u308c\u306a\u3044 <p>Romaji: Shinjirarenai</p> <p>Translation: I can't believe it / Unbelievable</p> <p>Context: Common expression of disbelief</p> <p>Grammar / Vocab: - \u4fe1\u3058\u308b \u2014 to believe - \u3089\u308c\u306a\u3044 \u2014 potential negative form</p>"},{"location":"japanese-phrases/daily-life/","title":"\ud83d\udde3\ufe0f Daily Life","text":"<p>Common phrases used in everyday Japanese conversation.</p>"},{"location":"japanese-phrases/daily-life/#basic-phrases","title":"Basic Phrases","text":"\u8ab0\u304c\u30d0\u30ab\u3088\uff1f\u5b9f\u9a13\u53f0\u306b\u306a\u3063\u3066\u304b\u3089\u8a00\u3044\u306a\u3055\u3044\u3002\u6b21\u306f\u3084\u3055\u3057\u304f\u306d\u3002 <p>Romaji: Dare ga baka yo? Jikken-dai ni natte kara iinasai. Tsugi wa yasashiku ne.</p> <p>Translation: Who's the idiot? Say that after you become a test subject. Be gentle next time.</p> <p>Context: Playful/teasing, anime-style banter</p> <p>Grammar / Vocab: - \u3060\u308c \u2014 who - \u3070\u304b \u2014 idiot/fool - \u5b9f\u9a13\u53f0 \u2014 test subject - \u301c\u306a\u3063\u3066\u304b\u3089 \u2014 after becoming ~ - \u301c\u306a\u3055\u3044 \u2014 soft imperative 'do ~' - \u3084\u3055\u3057\u304f \u2014 gently/kindly</p>"},{"location":"japanese-phrases/daily-life/shopping/","title":"Shopping Phrases","text":"<p>Essential phrases for shopping in Japan.</p> \u3053\u308c\u306f\u3044\u304f\u3089\u3067\u3059\u304b\uff1f <p>Romaji: Kore wa ikura desu ka?</p> <p>Translation: How much is this?</p> <p>Context: Asking for price when shopping</p> <p>Grammar / Vocab: - \u3053\u308c \u2014 this - \u3044\u304f\u3089 \u2014 how much - \u3067\u3059\u304b \u2014 polite question form</p> \u5b89\u304f\u3057\u3066\u304f\u3060\u3055\u3044 <p>Romaji: Yasuku shite kudasai</p> <p>Translation: Please make it cheaper</p> <p>Context: Negotiating price (mainly at markets)</p> <p>Grammar / Vocab: - \u5b89\u304f \u2014 cheaply - \u3057\u3066\u304f\u3060\u3055\u3044 \u2014 please do</p>"},{"location":"japanese-phrases/emotions/","title":"\ud83d\ude0a Emotions","text":"<p>Expressing feelings and emotions in Japanese.</p>"},{"location":"japanese-phrases/emotions/#happy-emotions","title":"Happy Emotions","text":"\u5b09\u3057\u3044\u3067\u3059 <p>Romaji: Ureshii desu</p> <p>Translation: I'm happy</p> <p>Context: Expressing happiness or joy</p> <p>Grammar / Vocab: - \u5b09\u3057\u3044 \u2014 happy/glad - \u3067\u3059 \u2014 polite form ending</p>"},{"location":"japanese-phrases/emotions/#sad-emotions","title":"Sad Emotions","text":"\u60b2\u3057\u3044\u3067\u3059 <p>Romaji: Kanashii desu</p> <p>Translation: I'm sad</p> <p>Context: Expressing sadness</p> <p>Grammar / Vocab: - \u60b2\u3057\u3044 \u2014 sad - \u3067\u3059 \u2014 polite form ending</p>"},{"location":"japanese-phrases/greetings/","title":"\ud83d\udc4b Greetings","text":"<p>Standard Japanese greetings for different situations.</p>"},{"location":"japanese-phrases/greetings/#morning-greetings","title":"Morning Greetings","text":"\u304a\u306f\u3088\u3046\u3054\u3056\u3044\u307e\u3059 <p>Romaji: Ohayou gozaimasu</p> <p>Translation: Good morning (polite)</p> <p>Context: Standard morning greeting</p> <p>Grammar / Vocab: - \u304a\u306f\u3088\u3046 \u2014 good morning (casual) - \u3054\u3056\u3044\u307e\u3059 \u2014 polite form ending</p>"},{"location":"japanese-phrases/greetings/#worksocial-greetings","title":"Work/Social Greetings","text":"\u304a\u75b2\u308c\u69d8\u3067\u3057\u305f <p>Romaji: Otsukaresama deshita</p> <p>Translation: Thank you for your hard work / Good job</p> <p>Context: Used when leaving work or after completing tasks</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/fatigue (polite) - \u69d8 \u2014 honorific suffix - \u3067\u3057\u305f \u2014 past tense polite form</p>"},{"location":"japanese-phrases/greetings/casual/","title":"Casual Greetings","text":"<p>Informal greetings used with friends and family.</p> \u304a\u3063\u3059\uff01 <p>Romaji: Ossu!</p> <p>Translation: Hey! / What's up!</p> <p>Context: Very casual greeting, mainly used by males</p> <p>Grammar / Vocab: - \u304a\u3063\u3059 \u2014 casual \"hey\" (masculine)</p> \u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p>"},{"location":"japanese-phrases/greetings/casual/#c2","title":"C2","text":""},{"location":"japanese-phrases/greetings/casual/#sad","title":"sad","text":"\u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p> \u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p> \u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p>"},{"location":"japanese-phrases/greetings/casual/#sad_1","title":"sad","text":"<p>Informal greetings used with friends and family.</p> \u304a\u3063\u3059\uff01 <p>Romaji: Ossu!</p> <p>Translation: Hey! / What's up!</p> <p>Context: Very casual greeting, mainly used by males</p> <p>Grammar / Vocab: - \u304a\u3063\u3059 \u2014 casual \"hey\" (masculine)</p> \u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p>"},{"location":"japanese-phrases/greetings/casual/#c2_1","title":"C2","text":""},{"location":"japanese-phrases/greetings/casual/#sad_2","title":"sad","text":"\u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p> \u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p> \u304a\u75b2\u308c\uff01 <p>Romaji: Otsukare!</p> <p>Translation: Good work! / Thanks!</p> <p>Context: Casual version of \u304a\u75b2\u308c\u69d8</p> <p>Grammar / Vocab: - \u304a\u75b2\u308c \u2014 tiredness/effort (casual)</p>"},{"location":"japanese-phrases/greetings/casual/#sad_3","title":"sad","text":""},{"location":"llm/speculative_decoding/","title":"Speculative decoding MTP in Qwen3-Next","text":"<p>![Note] @AI_content writen by Gemini 2.5 Pro with context about Qwen3-Next MTP Context of implementation in vLLM 0.10.2 was provided + model architecture.</p>"},{"location":"llm/speculative_decoding/#technical-summary-speculative-decoding-in-qwen3-next-mtp-method","title":"Technical Summary: Speculative Decoding in Qwen3-Next (MTP Method)","text":"<p>Document Date: September 16, 2025</p>"},{"location":"llm/speculative_decoding/#1-high-level-concept-overcoming-latency","title":"1. High-Level Concept: Overcoming Latency","text":"<p>The primary goal of speculative decoding is to accelerate inference speed by reducing the number of slow, expensive forward passes required by the main model. Large Mixture-of-Experts (MoE) models like Qwen3-Next are powerful but have high per-token latency.</p> <p>The MTP method uses a \"Professor &amp; Student\" analogy:</p> <ul> <li>The Main Model (Professor): The full, deep (e.g., 48-layer) Qwen3-Next model. It is extremely accurate but slow.</li> <li>The MTP Drafter (Student): A specialized, fast predictor. It generates a \"draft\" of multiple future tokens which the Professor then proofreads all at once.</li> </ul>"},{"location":"llm/speculative_decoding/#2-architecture-of-the-mtp-drafter","title":"2. Architecture of the MTP Drafter","text":"<p>Based on the model's weight files, the MTP Drafter is not a simple MLP. It is a complete, self-contained <code>Qwen3NextDecoderLayer</code>.</p> <ul> <li>Structure: It has the identical architecture to a single layer of the main model, including:<ul> <li>A <code>self_attn</code> block (<code>Qwen3NextAttention</code>).</li> <li>An <code>mlp</code> block which is also a <code>Qwen3NextSparseMoeBlock</code> (with its own router, experts, and shared expert).</li> <li>Standard <code>input_layernorm</code> and <code>post_attention_layernorm</code>.</li> </ul> </li> <li>Why it's Fast: Its speed comes from its extreme shallowness. The generation of a single draft token requires a pass through only one of these complex layers, whereas the main model requires a pass through 48 of them.</li> </ul>"},{"location":"llm/speculative_decoding/#3-the-end-to-end-generation-pipeline","title":"3. The End-to-End Generation Pipeline","text":"<p>This is the complete process for generating a draft of <code>k</code> tokens (e.g., <code>k=2</code>).</p> <p>Phase 1: Initial Context Generation (1 Expensive Pass)</p> <ol> <li>Input: The initial prompt (e.g., <code>\"The cat sat\"</code>).</li> <li>Process: The prompt is fed through the Main Model (all 48 layers).</li> <li>Output: The model produces the final hidden state from its last layer for the last token (\"sat\"). We'll call this <code>H_main</code>. This is the \"thought vector\" or rich context for what comes next.</li> </ol> <p>Phase 2: MTP Drafting Loop (Fast Iterative Process)</p> <p>This phase is executed by the MTP Drafter and is computationally cheap.</p> <ol> <li> <p>Draft Token 1:</p> <ul> <li>Input: The original <code>H_main</code> is concatenated with the embedding of the last known token (\"sat\"). Shape <code>[1, hidden_size * 2]</code>.</li> <li>Projection: This double-sized tensor is passed through the <code>mtp.fc</code> linear layer to project it back down to <code>hidden_size</code>.</li> <li>Process: The result is passed through the single <code>mtp.layers.0</code> block.</li> <li>Output: A new hidden state, <code>H_spec_1</code>.</li> <li>Sampling: The system calculates logits (<code>lm_head(H_spec_1)</code>) and samples <code>token_1</code> (e.g., <code>\"on\"</code>).</li> </ul> </li> <li> <p>Draft Token 2:</p> <ul> <li>Input: The original <code>H_main</code> is concatenated with the embedding of the newly drafted <code>token_1</code> (\"on\").</li> <li>Projection &amp; Process: The same projection and <code>mtp.layers.0</code> block are used again.</li> <li>Output: A new hidden state, <code>H_spec_2</code>.</li> <li>Sampling: The system calculates logits (<code>lm_head(H_spec_2)</code>) and samples <code>token_2</code> (e.g., <code>\"the\"</code>).</li> </ul> </li> <li> <p>Result of Drafting: The phase ends with a candidate sequence: <code>Draft = {\"on\", \"the\"}</code>.</p> </li> </ol> <p>Phase 3: Verification &amp; Acceptance (1 Expensive Pass)</p> <ol> <li>Input: The full sequence <code>prompt + Draft</code> (e.g., <code>\"The cat sat on the\"</code>) is fed into the Main Model.</li> <li>Process: The Main Model runs a single forward pass, calculating the \"correct\" hidden states and logits for every token position in parallel.</li> <li>Comparison: The system compares its draft with the main model's verified outputs:<ul> <li>It checks if <code>token_1</code> (\"on\") is what the main model would have generated after \"sat\".</li> <li>If yes, it continues. It checks if <code>token_2</code> (\"the\") is what the main model would have generated after \"on\".</li> </ul> </li> <li>Outcome:<ul> <li>Full Acceptance: If all tokens match, the new sequence becomes <code>\"The cat sat on the\"</code>.</li> <li>Partial Acceptance: If only <code>token_1</code> matched, the new sequence becomes <code>\"The cat sat on\"</code>.</li> <li>Full Rejection: If not even <code>token_1</code> matched, the system discards the entire draft. It salvages the correct first token (which it calculated during this verification pass anyway) and proceeds.</li> </ul> </li> </ol>"},{"location":"llm/speculative_decoding/#4-cost-benefit-analysis-the-profit","title":"4. Cost-Benefit Analysis: The \"Profit\"","text":"<ul> <li>Standard Method Cost: To generate <code>N</code> tokens, it requires <code>N</code> expensive passes.</li> <li> <p>Speculative Method Cost: To generate <code>N</code> tokens (assuming full acceptance), it requires <code>2</code> expensive passes (1 Initial + 1 Verification).</p> </li> <li> <p>Profit (Passes Saved): <code>Profit = N - 2</code></p> <ul> <li>For 10 accepted tokens, the profit is <code>10 - 2 = 8</code> saved passes.</li> </ul> </li> <li> <p>Worst-Case Cost (0 Accepted Tokens):</p> <ul> <li>The standard method would have taken 1 pass to get the one correct token.</li> <li>The speculative method took 2 passes to get that same one correct token.</li> <li>Loss (Wasted Passes): <code>Loss = 1</code> pass.</li> </ul> </li> </ul> <p>The system is profitable as long as the average number of accepted tokens per cycle is greater than 1.</p>"},{"location":"slurm/","title":"Slurm info","text":""},{"location":"slurm/#comands-to-check-status","title":"Comands to check status","text":"<p>How to check running jobs</p> <pre><code>squeue \n</code></pre> <p>How to check avaliable recources</p> <pre><code>sinfo \n</code></pre> <p>sHow to stop the job</p> <pre><code>scancel &lt;JOB_ID&gt;  # example `scancel 35512`\n</code></pre>"},{"location":"slurm/#salloc-how-to-take-resources-on-the-node-interactvie-mode","title":"Salloc How to take resources on the node (interactvie mode):","text":"<p>You can name this file as salloc_res.sh </p> <p>and run with <code>bash salloc_res.sh</code></p> <pre><code>SESSION_NAME=\"2GPUs_int_yolo_trian\"\n\nif ! tmux has-session -t $SESSION_NAME 2&gt;/dev/null; then\n    tmux new-session -d -s $SESSION_NAME\n\n    tmux send-keys -t $SESSION_NAME \"salloc \\\n      --partition=defq \\\n      --time=167:00:00 \\\n      --nodelist=node006 \\\n      --nodes=1 \\\n      --mem=1 \\\n      --job-name='2GPUs_int_yolo_trian'\" C-m\nfi\n# --gres=gpu:2 \\\ntmux attach-session -t $SESSION_NAME\n</code></pre> <p>After you received the resources you can do:</p> <pre><code>ssh node006\n</code></pre> <p>And work as you were working usually on H100.</p>"},{"location":"slurm/#sbatch-only-send-1-request-and-wait-until-it-finished-or-crashed","title":"Sbatch Only send 1 request and wait until it finished or crashed","text":"<p>From Head_node run <code>sbatch &lt;your_bash_script.sh&gt;</code> in terminal it will create a job for the node.</p> <p><code>your_bash_script.sh</code> <pre><code>#!/bin/bash\n#SBATCH --job-name=qwen-train                           # job name\n#SBATCH --nodes=1                                       # amount of nodes to take\n#SBATCH --ntasks-per-node=1 \n#SBATCH --cpus-per-task=64                              # amount of cpus to take\n#SBATCH --nodelist=node00[6]                          # which nodes you want to take\n#SBATCH --gres=gpu:1                                    # amount of gpus to take\n#SBATCH --time=168:00:00                                # max time until job\n#SBATCH --mem=1000000M                                  # amount of RAM to take\n#SBATCH --partition=defq\n#SBATCH --output=logs_internvl/slurm-%N.%j.out          # where out logs will be writen\n#SBATCH --error=logs_internvl/slurm-%N.%j.err           # where error logs will be writen\n\n# Conda initialization\neval \"$(conda shell.bash hook)\"\nconda activate base\n\nexport CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\nexport OMP_NUM_THREADS=1\n\n# # Setting master address and port\n# MASTER_ADDR=$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)\n# MASTER_PORT=$(( RANDOM % (50000 - 30000 + 1 ) + 30000 ))\n\nsrun torchrun \\\n    --nnodes 1 \\\n    --nproc_per_node 8 \\\n    -m test_train.py\n</code></pre></p>"},{"location":"tool_calls/","title":"Tool Calls","text":""},{"location":"tool_calls/#simple-call-from-openrouter-to-qwen","title":"Simple call from OpenRouter to Qwen","text":"<pre><code>import json\nfrom openai import OpenAI\n\n# It's good practice to use environment variables for API keys\n# For this example, we'll use a placeholder.\n# from dotenv import load_dotenv\n# import os\n# load_dotenv()\n# OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n\nclient = OpenAI(\n  base_url=\"https://openrouter.ai/api/v1\",\n  api_key=\"\", # Replace with your key\n)\n\n# --- Step 1: Define your tool and the function it calls ---\n\n# This is the actual Python function that will be executed.\ndef get_current_weather(location, unit=\"celsius\"):\n    \"\"\"Get the current weather in a given location.\"\"\"\n    # In a real application, you would call a weather API here.\n    # For this example, we'll return mock data.\n    if \"astana\" in location.lower():\n        weather_info = {\n            \"location\": location,\n            \"temperature\": \"-15\",\n            \"unit\": unit,\n            \"forecast\": [\"snowy\", \"windy\", \"cold\"],\n        }\n    else:\n        weather_info = {\n            \"location\": location,\n            \"temperature\": \"22\",\n            \"unit\": unit,\n            \"forecast\": [\"sunny\", \"mild\"],\n        }\n    return json.dumps(weather_info)\n\n# --- Step 2: Make the first API call with the tools defined ---\n\n# The user's prompt that should trigger the tool.\nuser_prompt = \"What's the weather like in Astana?\"\nprint(f\"\ud83d\udc64 User: {user_prompt}\\n\")\n\nmessages = [{\"role\": \"user\", \"content\": user_prompt}]\n\n# Describe your tool in the JSON schema format the model expects.\ntools = [\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"get_current_weather\",\n            \"description\": \"Get the current weather in a given location\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"location\": {\n                        \"type\": \"string\",\n                        \"description\": \"The city and state, e.g., San Francisco, CA\",\n                    },\n                    \"unit\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"celsius\", \"fahrenheit\"]\n                    },\n                },\n                \"required\": [\"location\"],\n            },\n        },\n    }\n]\n\n# First API call\ncompletion = client.chat.completions.create(\n  model=\"qwen/qwen3-235b-a22b-2507\",\n  messages=messages,\n  tools=tools,\n  tool_choice=\"auto\", # 'auto' lets the model decide, or you can force a tool call.\n)\n\nresponse_message = completion.choices[0].message\ntool_calls = response_message.tool_calls\n\n# --- Step 3: Check if the model wants to call a tool and execute it ---\n\nif tool_calls:\n    print(\"\ud83e\udd16 Model wants to call a tool...\")\n    print(f\"Tool calls: {tool_calls}\\n\")\n\n    # Append the assistant's message with tool calls to the conversation history\n    messages.append(response_message)\n\n    # In this example, we'll use a mapping to find the correct function.\n    available_functions = {\n        \"get_current_weather\": get_current_weather,\n    }\n\n    # Loop through each tool call the model requested\n    for tool_call in tool_calls:\n        function_name = tool_call.function.name\n        function_to_call = available_functions[function_name]\n        function_args = json.loads(tool_call.function.arguments)\n\n        print(f\"Executing function: {function_name}({function_args})\")\n\n        # Call the actual function with the arguments provided by the model\n        function_response = function_to_call(\n            location=function_args.get(\"location\"),\n            unit=function_args.get(\"unit\"),\n        )\n\n        print(f\"Function response: {function_response}\\n\")\n\n        # --- Step 4: Send the function's response back to the model in a second call ---\n\n        # Append the tool's response to the conversation history\n        messages.append(\n            {\n                \"tool_call_id\": tool_call.id,\n                \"role\": \"tool\",\n                \"name\": function_name,\n                \"content\": function_response,\n            }\n        )\n\n    print(\"\ud83d\udce2 Sending tool response back to the model...\")\n\n    # Second API call\n    second_response = client.chat.completions.create(\n        model=\"qwen/qwen3-235b-a22b-2507\",\n        messages=messages, # Send the whole conversation history\n    )\n\n    final_message = second_response.choices[0].message.content\n    print(f\"\\n\u2705 Final Model Response:\\n{final_message}\")\n\nelse:\n    # If the model didn't call a tool, just print its response\n    print(f\"\ud83e\udd16 Model (no tool call):\\n{response_message.content}\")\n</code></pre>"},{"location":"vllm/bash_vllm_serve/","title":"Bash online serve","text":""},{"location":"vllm/bash_vllm_serve/#default","title":"Default","text":"<pre><code>export VLLM_USE_V1=1\n# Model and environment settings\nexport CUDA_VISIBLE_DEVICES=0,1,2,3\n\nMODEL=\"models/Oylan2_5-MLP-sft-10k\"\n\nexport WHISPER_MODEL_PATH=\"$MODEL\"\nMODEL_SERVED_NAME=\"kita\" \nPORT=6655\nHOST=\"0.0.0.0\"\nSEED=0\n\n# vLLM configuration parameters\nGPU_MEMORY_UTILIZATION=0.90 # 80 is fine\nTENSOR_PARALLEL_SIZE=4 # changable\nDTYPE=\"bfloat16\"\nMAX_NUM_BATCHED_TOKENS=32768 # 32768 vs 4096\nMAX_MODEL_LEN=4096\nKV_CACHE_DTYPE=\"auto\"\nBLOCK_SIZE=32 \nSWAP_SPACE=0\nMAX_NUM_SEQS=5\n\n\nCMD=\"vllm serve $MODEL \\\n  --tokenizer \"$MODEL\" \\\n  --host $HOST \\\n  --port $PORT \\\n  --served-model-name $MODEL_SERVED_NAME \\\n  --gpu-memory-utilization $GPU_MEMORY_UTILIZATION \\\n  --max-num-batched-tokens $MAX_NUM_BATCHED_TOKENS \\\n  --max-model-len $MAX_MODEL_LEN \\\n  --trust-remote-code \\\n  --dtype $DTYPE \\\n  --tensor-parallel-size $TENSOR_PARALLEL_SIZE \\\n  --swap-space $SWAP_SPACE \\\n  --block-size $BLOCK_SIZE \\\n  --kv-cache-dtype $KV_CACHE_DTYPE \\\n  --max-num-seqs $MAX_NUM_SEQS \\\n  --enable-prefix-caching \\\n  --enable-chunked-prefill \\\n  --seed $SEED\"\n\n# Execute the command\neval $CMD 2&gt;&amp;1 | grep -v -E \"'WhisperTokenizer'|'Qwen2Tokenizer'|unexpected tokenization|'WhisperTokenizerFast'\"\n</code></pre>"},{"location":"vllm/bash_vllm_serve/#with-lora-adapter-docker-run","title":"With lora adapter Docker run","text":"<pre><code>### Model and environment settings\nMODEL=\"/home/vladimir_albrekht/projects/digital_bridge/models/Oylan2_5-MLP-sft-10k\"\n\nMODEL_SERVED_NAME=\"oylan_a_v_t_2_5\" \nPORT=6655\nHOST=\"0.0.0.0\"\nSEED=0\n\n# vLLM configuration parameters\nGPU_MEMORY_UTILIZATION=0.90 # 80 is fine\nTENSOR_PARALLEL_SIZE=4 # changable\nDTYPE=\"bfloat16\"\nMAX_NUM_BATCHED_TOKENS=32768 # 32768 vs 4096\nMAX_MODEL_LEN=4096\nKV_CACHE_DTYPE=\"auto\"\nBLOCK_SIZE=32 \nSWAP_SPACE=0\nMAX_NUM_SEQS=5\nTP_SIZE=4  # correspond to the len(CUDA_VISIBLE_DEVICES) that you want to use for 1 instance\n\n# LORA\nADAPTER_1=\"/home/vladimir_albrekht/projects/digital_bridge/models/adapters/remaped/vision_lora_3k_Oylan2-5_MLP_SFT_10k_remap\"\n# --enable-lora \\\nLORA_DTYPE=\"bfloat16\"\nMAX_LORA_RANK=256\nMAX_LORAS=2\n\n\nIMAGE_NAME=\"vllm-oylan-2_5:0.9.2-cu124\"\ndocker run --gpus all \\\n  -v \"$MODEL\":\"$MODEL\":ro \\\n  -v \"$ADAPTER_1\":\"$ADAPTER_1\":ro \\\n  --env WHISPER_MODEL_PATH=\"$MODEL\" \\\n  -e VLLM_USE_V1=1 \\\n  -e CUDA_VISIBLE_DEVICES=4,5,6,7 \\\n  -p ${PORT}:8000 \\\n  --name $MODEL_SERVED_NAME \\\n  --ipc=host \\\n  $IMAGE_NAME \\\n  --model \"$MODEL\" \\\n  --tokenizer \"$MODEL\" \\\n  --enable-lora \\\n  --lora-modules oylan_2_5_vision_lora=$ADAPTER_1 \\\n  --lora-dtype $LORA_DTYPE \\\n  --max_lora_rank $MAX_LORA_RANK \\\n  --max-loras $MAX_LORAS \\\n  --tensor-parallel-size $TP_SIZE \\\n  --swap-space $SWAP_SPACE \\\n  --gpu-memory-utilization $GPU_MEMORY_UTILIZATION \\\n  --max-num-batched-tokens $MAX_NUM_BATCHED_TOKENS \\\n  --max-model-len $MAX_MODEL_LEN \\\n  --block-size $BLOCK_SIZE \\\n  --seed 0 \\\n  --dtype $DTYPE \\\n  --max-num-seqs $MAX_NUM_SEQS \\\n  --trust-remote-code \\\n  --enable-auto-tool-choice \\\n  --tool-call-parser llama3_json \\\n  --served-model-name $MODEL_SERVED_NAME\n\n# -d if you want to run in the background\n</code></pre>"},{"location":"vllm/open_ai_vllm_example_a_v_t/","title":"OpenAI example inference for vLLM Online serve","text":""},{"location":"vllm/open_ai_vllm_example_a_v_t/#text-inference","title":"Text inference","text":"<pre><code>import time\nimport openai\n\nclient = openai.Client(\n    base_url=\"http://localhost:6655/v1\", api_key=\"EMPTY\"\n)\nMODEL = \"ki_oylan_a_v_t_2_5\"\n\n\nstart_time = time.perf_counter()\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\"role\": \"system\", \"content\": \"\u0422\u044b \u043b\u044e\u0431\u0438\u0448\u044c \u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u0430\u043d\u0438\u043c\u0435. /no_think\"},\n        {\"role\": \"user\", \"content\": \"\u041a\u0430\u043a\u043e\u0435 \u0443 \u0442\u0435\u0431\u044f \u043b\u044e\u0431\u0438\u043c\u043e\u0435 \u0430\u043d\u0438\u043c\u0435?\"},\n    ],\n    temperature=0,\n    max_tokens=256,\n    stream=True\n)\n\nfirst_token_time = None\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        if first_token_time is None:  # first token arrived\n            first_token_time = time.perf_counter()\n            ttft = first_token_time - start_time\n            print(f\"\\n\\nTTFT: {ttft:.3f} seconds\\n\")\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"},{"location":"vllm/open_ai_vllm_example_a_v_t/#image-inference","title":"Image inference","text":"<pre><code># Image inference\nimport time\nfrom pathlib import Path\nimport openai\nimport base64\nclient = openai.Client(\n    base_url=\"http://localhost:6655/v1\", api_key=\"EMPTY\"\n)\nMODEL = \"ki_l\"\n\n# ########################################################################\n# ################ if you want to use local file #########################\n# ########################################################################\n# IMAGE_PATH = Path(\"/home/vladimir_albrekht/projects/digital_bridge/vllm/1_vladimir_utils/utils/benchs_perf/assets/cute_girl.jpg\")\n\n\n# def guess_mime(path: Path) -&gt; str:\n#     \"\"\"Guess MIME type from file extension\"\"\"\n#     ext = path.suffix.lower()\n#     if ext in [\".jpg\", \".jpeg\"]:\n#         return \"image/jpeg\"\n#     elif ext == \".png\":\n#         return \"image/png\"\n#     elif ext == \".webp\":\n#         return \"image/webp\"\n#     elif ext == \".gif\":\n#         return \"image/gif\"\n#     else:\n#         return \"image/jpeg\"  # Fallback\n\n# def encode_image(image_path: Path) -&gt; str:\n#     \"\"\"Encode image file to base64 string\"\"\"\n#     return base64.b64encode(image_path.read_bytes()).decode(\"utf-8\")\n\n# base64_image = encode_image(IMAGE_PATH)\n# mime_type = guess_mime(IMAGE_PATH)\n# # data:{mime_type};base64,{base64_image}\n# ########################################################################\n\n\n\nstart_time = time.perf_counter()\nresponse = client.chat.completions.create(\n  model=MODEL,\n  messages=[\n    {\n      \"role\": \"user\",\n      \"content\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"Describe this image in detail\", # make sure to inclue &lt;image&gt; otherwise it will crush.\n        },\n        {\n          \"type\": \"image_url\",\n          \"image_url\": {\n            \"url\":  f\"https://huggingface.co/datasets/CCRss/kv_brain/resolve/main/Xnip2025-08-24_15-02-37.jpg\" # for local path `data:{mime_type};base64,{base64_image}`\n          },\n        },\n      ],\n    }\n  ],\n  max_tokens=256,\n  stream=True,\n  temperature=0.1\n)\n\n\nfirst_token_time = None\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        if first_token_time is None:  # first token arrived\n            first_token_time = time.perf_counter()\n            ttft = first_token_time - start_time\n            print(f\"\\n\\nTTFT: {ttft:.3f} seconds\\n\")\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"},{"location":"vllm/open_ai_vllm_example_a_v_t/#audio-inference","title":"Audio inference","text":"<pre><code># Audio request\nimport base64\nimport io\nimport time\nfrom openai import OpenAI\nimport torch\nfrom pathlib import Path\nimport requests\nimport soundfile as sf\nimport numpy as np\nimport torchaudio\n\n\nclient = OpenAI(base_url=\"http://localhost:6655/v1\", api_key=\"EMPTY\")\nMODEL = \"ki_oylan_a_v_t_2_5\"\n\n\ndef encode_audio_from_url(url: str) -&gt; str:\n    resp = requests.get(url)\n    resp.raise_for_status()\n    data, sr = sf.read(io.BytesIO(resp.content))  # reads MP3/WAV/etc.\n    if data.ndim &gt; 1:\n        data = np.mean(data, axis=1, keepdims=True)  # mono\n    if sr != 16000:\n        import torchaudio\n        tensor = torchaudio.functional.resample(torch.from_numpy(data.T), sr, 16000)\n        sr = 16000\n        data = tensor.numpy().T\n    with io.BytesIO() as buffer:\n        sf.write(buffer, data, sr, format=\"WAV\")\n        return base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n\n########################################################################\n# ################ if you want to use local file #########################\n# ########################################################################\n# AUDIO_PATH = Path(\"assets/test_3.mp3\")\n# def convert_to_wav_bytes(path: Path) -&gt; bytes:\n#     ext = path.suffix.lower()\n#     if ext == \".wav\":\n#         return path.read_bytes()  # Already in WAV format\n\n#     elif ext == \".mp3\":\n#         # Load MP3, convert to WAV format (16kHz mono float32)\n#         waveform, sr = torchaudio.load(path)\n#         if waveform.shape[0] &gt; 1:\n#             waveform = waveform.mean(dim=0, keepdim=True)  # Make mono\n#         if sr != 16000:\n#             resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n#             waveform = resampler(waveform)\n\n#         # Save to in-memory WAV\n#         with io.BytesIO() as buffer:\n#             torchaudio.save(buffer, waveform, 16000, format=\"wav\")\n#             return buffer.getvalue()\n\n#     else:\n#         raise ValueError(f\"Unsupported audio format: {ext}\")\n\n# def encode_audio(audio_path: Path) -&gt; str:\n#     \"\"\"Encode audio file to base64 string\"\"\"\n#     wav_bytes = convert_to_wav_bytes(audio_path)\n#     return base64.b64encode(wav_bytes).decode(\"utf-8\")\n\n# base64_audio = encode_audio(AUDIO_PATH)\n\n#######################################################################\n\nstart_time = time.perf_counter()\nresponse = client.chat.completions.create(\n    model=MODEL,\n    messages=[\n        {\n            \"role\": \"system\",\n            \"content\": \"Your name is Oylan, you are a useful multi-modal large language model developed by ISSAI, Kazakhstan. /no_think\"\n        },\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\n                    \"type\": \"text\",\n                    \"text\": \"Transcribe the audio\"\n                },\n                {\n                    \"type\": \"audio_url\",\n                    \"audio_url\": {\n                        \"url\": f\"data:audio/wav;base64,{base64_audio}\"\n                        # or \n                        # \"url\": f\"data:audio/wav;base64,{encode_audio_from_url('https://huggingface.co/datasets/CCRss/kv_brain/resolve/main/yes_my_lord.mp3')}\"\n                    }\n                }\n            ]\n        }\n    ],\n    max_tokens=512,\n    temperature=0.1,\n    stream=True\n)\n\n# if stream=false\n# print(\"\u2705 Response:\", response.choices[0].message.content)\nfirst_token_time = None\nfor chunk in response:\n    if chunk.choices[0].delta.content is not None:\n        if first_token_time is None:  # first token arrived\n            first_token_time = time.perf_counter()\n            ttft = first_token_time - start_time\n            print(f\"\\n\\nTTFT: {ttft:.3f} seconds\\n\")\n        print(chunk.choices[0].delta.content, end=\"\", flush=True)\n</code></pre>"},{"location":"vllm/benchmarks/performance_eval/","title":"Performnace evaluation TTFT, etc.","text":"<p>If you want to evaluatin Image or Audio make sure to download Assets first.</p>"},{"location":"vllm/benchmarks/performance_eval/#assets-can-be-download-with","title":"Assets can be download with:","text":"<pre><code>from huggingface_hub import hf_hub_download, login, HfApi\nimport dotenv, os, shutil\n\ndotenv.load_dotenv()\nlogin(token=os.environ[\"HF_TOKEN\"])\n\nREPO_ID = \"CCRss/kv_brain\"\nPATH_IN_REPO = \"performance_eval_vllm/assets\"\n\nLOCAL_DIR = \"/scratch/vladimir_albrekht/projects/10_09_2025_MOe/debug_inf/assets\"\n\ndef download_all_files(repo_id, path_in_repo, local_dir):\n    print(f\"Downloading files from {repo_id}/{path_in_repo} ...\")\n    os.makedirs(local_dir, exist_ok=True)\n\n    api = HfApi()\n    repo_type = \"dataset\"\n    files = api.list_repo_files(repo_id, repo_type=repo_type)\n\n    # only files inside PATH_IN_REPO\n    data_files = [f for f in files if f.startswith(path_in_repo)]\n\n    for filename in data_files:\n        try:\n            # download to cache (don\u2019t enforce local_dir mirroring!)\n            cached_file = hf_hub_download(\n                repo_id=repo_id,\n                filename=filename,\n                repo_type=repo_type,\n                local_dir=None,\n                local_dir_use_symlinks=False\n            )\n\n            # strip \"performance_eval_vllm/assets/\" prefix\n            short_name = filename.replace(path_in_repo + \"/\", \"\")\n            target_path = os.path.join(local_dir, short_name)\n\n            # ensure parent dirs exist if short_name had subfolders\n            os.makedirs(os.path.dirname(target_path), exist_ok=True)\n            shutil.copy(cached_file, target_path)\n\n            print(f\"Saved {short_name} \u2192 {target_path}\")\n        except Exception as e:\n            print(f\"Error downloading {filename}: {e}\")\n\ndownload_all_files(REPO_ID, PATH_IN_REPO, LOCAL_DIR)\n</code></pre>"},{"location":"vllm/benchmarks/performance_eval/#audio-evaluation","title":"Audio evaluation","text":"<pre><code>import json\nimport time\nimport math\nimport base64\nimport io\nfrom pathlib import Path\nfrom typing import List, Tuple\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\n\nimport requests\nimport torchaudio\nfrom tqdm import tqdm\n\n# ----- Config -----\nSERVED_NAME = \"kita\"\nBASE_URL = \"http://localhost:6655/v1\"\nAPI_URL = f\"{BASE_URL}/chat/completions\"\nHEADERS = {\"Authorization\": \"Bearer empty\", \"Content-Type\": \"application/json\"}\n\n# \u0422\u0435\u043a\u0441\u0442\u043e\u0432\u0430\u044f \u0447\u0430\u0441\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0441\u0430 (\u0442\u043e, \u0447\u0442\u043e \u0431\u0443\u0434\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u0442\u044c)\nSYSTEM_PROMPT = (\n    \"Your name is Oylan, you are a useful multi-modal large language model \"\n    \"developed by ISSAI, Kazakhstan.\"\n)\nUSER_TEXT = \"&lt;audio&gt;\\nWhat was said in this audio\"\n\n# \u041f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u044b \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438\nMAX_TOKENS = 512\nTEMPERATURE = 0.1\n\n# ----- Tokenizer API (/tokenize) -----\ndef _api_root_from_base(base_url: str) -&gt; str:\n    return base_url.split(\"/v1\")[0] if \"/v1\" in base_url else base_url\n\n_API_ROOT = _api_root_from_base(BASE_URL)\n_TOKENIZE_ENDPOINT = f\"{_API_ROOT}/tokenize\"\n\ndef _token_count(text: str) -&gt; int:\n    try:\n        r = requests.post(_TOKENIZE_ENDPOINT, json={\"prompt\": text}, timeout=30)\n        r.raise_for_status()\n        data = r.json()\n        if isinstance(data.get(\"count\"), int):\n            return data[\"count\"]\n        if isinstance(data.get(\"tokens\"), list):\n            return len(data[\"tokens\"])\n    except Exception:\n        pass\n    return 0\n\n# ----- \u0410\u0443\u0434\u0438\u043e helpers -----\ndef _to_wav_bytes(path: Path) -&gt; bytes:\n    ext = path.suffix.lower()\n    if ext == \".wav\":\n        return path.read_bytes()\n    waveform, sr = torchaudio.load(str(path))\n    if waveform.shape[0] &gt; 1:\n        waveform = waveform.mean(dim=0, keepdim=True)  # mono\n    if sr != 16000:\n        resampler = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000)\n        waveform = resampler(waveform)\n    buf = io.BytesIO()\n    torchaudio.save(buf, waveform, 16000, format=\"wav\")\n    return buf.getvalue()\n\ndef load_audio_base64_wav(path: Path) -&gt; str:\n    wav_bytes = _to_wav_bytes(path)\n    return base64.b64encode(wav_bytes).decode(\"utf-8\")\n\n# ----- \u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u0438\u0435 payload (\u0431\u0435\u0437 OpenAI SDK, \u0447\u0442\u043e\u0431\u044b \u0441\u0432\u043e\u0431\u043e\u0434\u043d\u043e \u043a\u043b\u0430\u0441\u0442\u044c audio_url) -----\ndef _build_payload(audio_b64: str) -&gt; dict:\n    return {\n        \"model\": SERVED_NAME,\n        \"messages\": [\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": USER_TEXT},\n                    {\"type\": \"audio_url\", \"audio_url\": {\"url\": f\"data:audio/wav;base64,{audio_b64}\"}},\n                ],\n            },\n        ],\n        \"max_tokens\": MAX_TOKENS,\n        \"temperature\": TEMPERATURE,\n    }\n\n# ---------- \u041c\u0435\u0442\u0440\u0438\u043a\u0438 ----------\ndef _percentile(sorted_vals: List[float], p: float):\n    if not sorted_vals:\n        return float(\"nan\")\n    if len(sorted_vals) == 1:\n        return sorted_vals[0]\n    k = (len(sorted_vals) - 1) * (p / 100.0)\n    f = math.floor(k); c = math.ceil(k)\n    if f == c:\n        return sorted_vals[int(k)]\n    return sorted_vals[f] * (c - k) + sorted_vals[c] * (k - f)\n\n# ---------- \u0417\u0430\u043f\u0440\u043e\u0441 \u043a \u0441\u0435\u0440\u0432\u0435\u0440\u0443 ----------\ndef _one_request(audio_name: str, audio_b64: str) -&gt; dict:\n    start = time.perf_counter()\n    try:\n        payload = _build_payload(audio_b64)\n        resp = requests.post(API_URL, headers=HEADERS, json=payload, timeout=120)\n        latency_ms = (time.perf_counter() - start) * 1000.0\n\n        if resp.status_code != 200:\n            preview = (resp.text or \"\")[:160].replace(\"\\n\", \" \")\n            return {\n                \"audio_name\": audio_name, \"ok\": False, \"status_code\": resp.status_code,\n                \"latency_ms\": latency_ms, \"error\": f\"http {resp.status_code}\", \"preview\": preview,\n                \"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0,\n            }\n\n        data = resp.json()\n        text = None\n        try:\n            text = data[\"choices\"][0][\"message\"][\"content\"]\n        except Exception:\n            text = None\n        usage = data.get(\"usage\") or {}\n        pt = usage.get(\"prompt_tokens\") or 0\n        ct = usage.get(\"completion_tokens\") or 0\n        tt = usage.get(\"total_tokens\") or (pt + ct)\n\n        return {\n            \"audio_name\": audio_name, \"ok\": True, \"status_code\": resp.status_code,\n            \"latency_ms\": latency_ms, \"gemma_translation\": text,\n            \"preview\": (text or \"\")[:160].replace(\"\\n\", \" \"),\n            \"prompt_tokens\": int(pt), \"completion_tokens\": int(ct), \"total_tokens\": int(tt),\n        }\n\n    except Exception as e:\n        latency_ms = (time.perf_counter() - start) * 1000.0\n        return {\n            \"audio_name\": audio_name, \"ok\": False, \"status_code\": 0,\n            \"latency_ms\": latency_ms, \"error\": str(e), \"preview\": None,\n            \"prompt_tokens\": 0, \"completion_tokens\": 0, \"total_tokens\": 0,\n        }\n\n# ---------- \u041e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0444\u0443\u043d\u043a\u0446\u0438\u044f: \u0441\u043f\u0438\u0441\u043e\u043a \u0430\u0443\u0434\u0438\u043e -&gt; JSONL + \u043c\u0435\u0442\u0440\u0438\u043a\u0438 ----------\ndef evaluate_audio_list_to_jsonl(\n    audio_paths: List[Path],\n    out_jsonl_path: str,\n    total_requests: int = 100,\n    max_workers: int = 32,\n):\n    \"\"\"\n    \u0411\u0435\u0440\u0451\u0442 \u0441\u043f\u0438\u0441\u043e\u043a \u043f\u0443\u0442\u0435\u0439 \u043a \u0430\u0443\u0434\u0438\u043e, \u043f\u043e\u0432\u0442\u043e\u0440\u044f\u0435\u0442 \u0438\u0445 \u043f\u043e \u043a\u0440\u0443\u0433\u0443 \u0434\u043e total_requests,\n    \u0448\u043b\u0451\u0442 \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u043e, \u043f\u0438\u0448\u0435\u0442 JSONL \u0438 \u043f\u0435\u0447\u0430\u0442\u0430\u0435\u0442 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 (RPS, \u043f\u0435\u0440\u0446\u0435\u043d\u0442\u0438\u043b\u0438).\n    \"\"\"\n    # \u041f\u0440\u0435\u0434\u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0430\u0443\u0434\u0438\u043e \u0432 \u043f\u0430\u043c\u044f\u0442\u044c, \u0447\u0442\u043e\u0431\u044b CPU/IO \u043d\u0435 \u043c\u0435\u0448\u0430\u043b\u0438 \u043c\u0435\u0442\u0440\u0438\u043a\u0430\u043c\n    preloaded: List[Tuple[str, str]] = []\n    for p in audio_paths:\n        if not p.exists():\n            raise FileNotFoundError(f\"Audio not found: {p}\")\n        b64 = load_audio_base64_wav(p)\n        preloaded.append((p.name, b64))\n    if not preloaded:\n        raise RuntimeError(\"Empty audio list.\")\n\n    seq = [preloaded[i % len(preloaded)] for i in range(total_requests)]\n\n    t0 = time.perf_counter()\n    futures = []\n    with ThreadPoolExecutor(max_workers=max_workers) as ex, \\\n         open(out_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n\n        for idx, (name, b64) in enumerate(seq):\n            futures.append(ex.submit(_one_request, name, b64))\n\n        latencies = []\n        ok_cnt = 0\n        total_pt = 0\n        total_ct = 0\n\n        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Processing\"):\n            rec = fut.result()\n            # \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u0443\u044e \u0447\u0430\u0441\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0441\u0430 \u0434\u043b\u044f \u0442\u043e\u043a\u0435\u043d-\u0430\u0433\u0440\u0435\u0433\u0430\u0446\u0438\u0438 (\u043a\u0430\u043a \u0443 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0433\u043e \u0441\u043a\u0440\u0438\u043f\u0442\u0430)\n            rec[\"original_text\"] = USER_TEXT  # \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u043c \u043f\u043e\u043b\u0435, \u043a\u0430\u043a \u0432 \"\u043f\u0435\u0440\u0432\u043e\u043c \u0444\u0430\u0439\u043b\u0435\"\n            # \u043f\u043e\u0442\u043e\u043a\u043e\u0432\u0430\u044f \u0437\u0430\u043f\u0438\u0441\u044c\n            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\n            if rec.get(\"ok\"):\n                ok_cnt += 1\n                lat = rec.get(\"latency_ms\")\n                if isinstance(lat, (int, float)):\n                    latencies.append(lat)\n                total_pt += int(rec.get(\"prompt_tokens\", 0))\n                total_ct += int(rec.get(\"completion_tokens\", 0))\n\n    wall = time.perf_counter() - t0\n    latencies.sort()\n    eff_rps = (ok_cnt / wall) if wall &gt; 0 else float(\"inf\")\n\n    metrics = {\n        \"total_requests\": total_requests,\n        \"successful\": ok_cnt,\n        \"failed\": total_requests - ok_cnt,\n        \"total_time_s\": wall,\n        \"effective_rps\": eff_rps,\n        \"latency_ms\": {\n            \"avg\": (sum(latencies) / len(latencies)) if latencies else float(\"nan\"),\n            \"min\": latencies[0] if latencies else float(\"nan\"),\n            \"max\": latencies[-1] if latencies else float(\"nan\"),\n            \"p50\": _percentile(latencies, 50) if latencies else float(\"nan\"),\n            \"p90\": _percentile(latencies, 90) if latencies else float(\"nan\"),\n            \"p99\": _percentile(latencies, 99) if latencies else float(\"nan\"),\n            \"count\": len(latencies),\n        },\n        \"max_workers\": max_workers,\n        # \u0415\u0441\u043b\u0438 \u0441\u0435\u0440\u0432\u0435\u0440 \u0432\u0435\u0440\u043d\u0443\u043b usage \u2014 \u0443\u0436\u0435 \u0430\u0433\u0440\u0435\u0433\u0438\u0440\u043e\u0432\u0430\u043d\u043e\n        \"usage_sum\": {\n            \"total_input_tokens\": total_pt,\n            \"total_generated_tokens\": total_ct,\n            \"total_tokens\": total_pt + total_ct,\n            \"input_tokens_per_sec\": (total_pt / wall) if wall &gt; 0 else float(\"inf\"),\n            \"output_tokens_per_sec\": (total_ct / wall) if wall &gt; 0 else float(\"inf\"),\n            \"total_throughput_tokens_per_sec\": ((total_pt + total_ct) / wall) if wall &gt; 0 else float(\"inf\"),\n        },\n    }\n\n    print(f\"\\nDone. Wrote JSONL to: {out_jsonl_path}\")\n    print(f\"Total Time: {metrics['total_time_s']:.2f} seconds\")\n    print(f\"Effective RPS: {metrics['effective_rps']:.2f} req/s\\n\")\n    print(\"Request Latency (ms):\")\n    lm = metrics[\"latency_ms\"]\n    print(f\"  Avg: {lm['avg']:.1f}, Min: {lm['min']:.1f}, Max: {lm['max']:.1f}\")\n    print(f\"  P50: {lm['p50']:.1f}, P90: {lm['p90']:.1f}, P99: {lm['p99']:.1f}\")\n\n    # \u0415\u0441\u043b\u0438 usage \u043d\u0435 \u043f\u0440\u0438\u0448\u0451\u043b, \u043c\u043e\u0436\u043d\u043e \u0434\u043e\u0431\u0438\u0442\u044c \u0442\u043e\u043a\u0435\u043d\u044b \u0447\u0435\u0440\u0435\u0437 /tokenize \u043f\u043e \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 (system+user)\n    if metrics[\"usage_sum\"][\"total_tokens\"] == 0:\n        summary = summarize_tokens_with_tokenizer_api(\n            jsonl_path=out_jsonl_path,\n            total_time_s=wall,\n        )\n        metrics[\"tokenize_sum\"] = summary\n\n    # \u041f\u0435\u0447\u0430\u0442\u044c \u0442\u043e\u043a\u0435\u043d\u043e\u0432 (usage \u0435\u0441\u043b\u0438 \u0435\u0441\u0442\u044c, \u0438\u043d\u0430\u0447\u0435 tokenize)\n    use = metrics[\"usage_sum\"] if metrics[\"usage_sum\"][\"total_tokens\"] &gt; 0 else metrics.get(\"tokenize_sum\", {})\n    if use:\n        print(\"\\nTokens:\")\n        print(f\"  Total Input Tokens      : {use.get('total_input_tokens')}\")\n        print(f\"  Total Generated Tokens  : {use.get('total_generated_tokens')}\")\n        print(f\"  Input (tokens/s)        : {use.get('input_tokens_per_sec'):.2f}\")\n        print(f\"  Output (tokens/s)       : {use.get('output_tokens_per_sec'):.2f}\")\n        print(f\"  Total Throughput        : {use.get('total_throughput_tokens_per_sec'):.2f} tokens/s\")\n\n    return metrics\n\n# --- \u0421\u0443\u043c\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u043e\u043a\u0435\u043d\u043e\u0432 \u0447\u0435\u0440\u0435\u0437 /tokenize \u043f\u043e JSONL (\u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u0430\u044f \u0447\u0430\u0441\u0442\u044c \u0437\u0430\u043f\u0440\u043e\u0441\u0430 \u0438 \u043e\u0442\u0432\u0435\u0442) ---\ndef summarize_tokens_with_tokenizer_api(jsonl_path: str, total_time_s: float):\n    total_input_tokens = 0\n    total_output_tokens = 0\n    ok = 0\n    for line in open(jsonl_path, \"r\", encoding=\"utf-8\"):\n        if not line.strip():\n            continue\n        rec = json.loads(line)\n        if not rec.get(\"ok\"):\n            continue\n        ok += 1\n        # \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u044b \u0442\u043e\u043b\u044c\u043a\u043e \u0434\u043b\u044f \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0439 \u0447\u0430\u0441\u0442\u0438 (\u0431\u0435\u0437 audio_url)\n        # system + user_text\n        text_prompt = f\"{SYSTEM_PROMPT}\\n\\n{rec.get('original_text','')}\"\n        total_input_tokens += _token_count(text_prompt)\n        total_output_tokens += _token_count(rec.get(\"gemma_translation\") or \"\")\n\n    input_tps = (total_input_tokens / total_time_s) if total_time_s &gt; 0 else float(\"inf\")\n    output_tps = (total_output_tokens / total_time_s) if total_time_s &gt; 0 else float(\"inf\")\n    total_tps = ((total_input_tokens + total_output_tokens) / total_time_s) if total_time_s &gt; 0 else float(\"inf\")\n\n    return {\n        \"successful\": ok,\n        \"total_input_tokens\": total_input_tokens,\n        \"total_generated_tokens\": total_output_tokens,\n        \"input_tokens_per_sec\": input_tps,\n        \"output_tokens_per_sec\": output_tps,\n        \"total_throughput_tokens_per_sec\": total_tps,\n    }\n\n# --------- Example run with AUDIO LIST ---------\nif __name__ == \"__main__\":\n    AUDIO_FILES = [\n        Path(\"assets/test_1.mp3\"),\n        Path(\"assets/test_3.mp3\"),\n        Path(\"assets/rustem_1.wav\"),\n        Path(\"assets/rustem_2.wav\"),\n    ]\n\n    N = 100          # \u0440\u043e\u0432\u043d\u043e \u0441\u0442\u043e\u043b\u044c\u043a\u043e \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432 \u043e\u0442\u043f\u0440\u0430\u0432\u0438\u043c (\u0446\u0438\u043a\u043b\u043e\u043c \u043f\u043e \u0441\u043f\u0438\u0441\u043a\u0443)\n    OUT = \"audio_eval_results.jsonl\"\n\n    evaluate_audio_list_to_jsonl(\n        audio_paths=AUDIO_FILES,\n        out_jsonl_path=OUT,\n        total_requests=N,\n        max_workers=32,\n    )\n</code></pre>"},{"location":"vllm/benchmarks/performance_eval/#image-evaluation","title":"Image evaluation","text":"<pre><code>import json\nimport time\nimport math\nimport base64\nfrom pathlib import Path\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom openai import OpenAI\nfrom tqdm import tqdm\nimport requests\n\n# ----- Your existing config -----\nMODELS = [\"ki_l\", \"ki_oylan_a_v_t_2_5\"]\nBASE_URL = \"http://localhost:6655/v1\"\n\nIMAGES_DIR = Path(\"assets\")\nIMAGE_NAMES = [\"cute_girl.jpeg\"] + [f\"{i}.jpg\" for i in range(1, 10)]  # total 10 names\n\nSYSTEM_PROMPT = \"\u0422\u044b \u2014 \u043d\u0430\u0431\u043b\u044e\u0434\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0439 \u0430\u0441\u0441\u0438\u0441\u0442\u0435\u043d\u0442. \u041e\u0442\u0432\u0435\u0447\u0430\u0439 \u043a\u0440\u0430\u0442\u043a\u043e \u0438 \u043f\u043e \u0434\u0435\u043b\u0443.\"\n# All prompts MUST include \"&lt;image&gt;\" first (your requirement)\nIMAGE_TASKS = [\n    \"&lt;image&gt; Describe this image in detail.\",\n    \"&lt;image&gt; List the main objects and their colors.\",\n    \"&lt;image&gt; Summarize the scene (who/what/where).\",\n    \"&lt;image&gt; What is the mood and style? Explain briefly.\",\n    \"&lt;image&gt; Identify notable attributes (clothes, pose, background).\",\n    \"&lt;image&gt; Give 3 tags that describe this image.\",\n    \"&lt;image&gt; Describe composition (foreground/background, framing).\",\n    \"&lt;image&gt; What stands out first to the viewer? Why?\",\n    \"&lt;image&gt; Describe lighting and color palette.\",\n    \"&lt;image&gt; One-sentence caption, then 3 bullet facts.\",\n]\n\ndef _api_root_from_base(base_url: str) -&gt; str:\n    return base_url.split(\"/v1\")[0] if \"/v1\" in base_url else base_url\n\n_API_ROOT = _api_root_from_base(BASE_URL)\n_TOKENIZE_ENDPOINT = f\"{_API_ROOT}/tokenize\"\n\ndef _token_count(text: str) -&gt; int:\n    try:\n        r = requests.post(_TOKENIZE_ENDPOINT, json={\"prompt\": text}, timeout=30)\n        r.raise_for_status()\n        data = r.json()\n        if \"count\" in data and isinstance(data[\"count\"], int):\n            return data[\"count\"]\n        if \"tokens\" in data and isinstance(data[\"tokens\"], list):\n            return len(data[\"tokens\"])\n    except Exception:\n        pass\n    return 0\n\ndef guess_mime(path: Path) -&gt; str:\n    ext = path.suffix.lower()\n    if ext in [\".jpg\", \".jpeg\"]:\n        return \"image/jpeg\"\n    if ext == \".png\":\n        return \"image/png\"\n    if ext == \".webp\":\n        return \"image/webp\"\n    if ext == \".gif\":\n        return \"image/gif\"\n    # Fallback to JPEG\n    return \"image/jpeg\"\n\ndef encode_image_b64(path: Path) -&gt; str:\n    data = path.read_bytes()\n    return base64.b64encode(data).decode(\"utf-8\")\n\ndef get_image_prediction(image_path: Path, prompt_text: str, model_name: str):\n    client = OpenAI(api_key=\"empty\", base_url=BASE_URL)  # local client per call (thread-safe)\n\n    mime = guess_mime(image_path)\n    b64_image = encode_image_b64(image_path)\n\n    resp = client.chat.completions.create(\n        model=model_name,\n        messages=[\n            {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n            {\n                \"role\": \"user\",\n                \"content\": [\n                    {\"type\": \"text\", \"text\": prompt_text},\n                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:{mime};base64,{b64_image}\"}},\n                ],\n            },\n        ],\n        temperature=0.0,\n        max_tokens=50,\n    )\n    return resp.choices[0].message.content, resp.usage\n\n# ---------- NEW: \u043e\u0446\u0435\u043d\u043a\u0430 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u043f\u043e \u0441\u043f\u0438\u0441\u043a\u0443 ----------\ndef _percentile(sorted_vals, p):\n    if not sorted_vals:\n        return float(\"nan\")\n    if len(sorted_vals) == 1:\n        return sorted_vals[0]\n    k = (len(sorted_vals) - 1) * (p / 100.0)\n    f = math.floor(k)\n    c = math.ceil(k)\n    if f == c:\n        return sorted_vals[int(k)]\n    return sorted_vals[f] * (c - k) + sorted_vals[c] * (k - f)\n\ndef evaluate_images_to_jsonl(\n    image_paths_list,\n    prompts_list,\n    out_jsonl_path,\n    total_requests=100,\n    max_workers=32,\n    max_retries=3,\n    retry_backoff=0.7,\n):\n    \"\"\"\n    \u0411\u0435\u0440\u0451\u0442 \u0421\u041f\u0418\u0421\u041e\u041a \u043f\u0443\u0442\u0435\u0439 \u043a \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u044f\u043c \u0438 \u043f\u0440\u043e\u043c\u043f\u0442\u043e\u0432, \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442 \u0434\u043e `max_workers` \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u044b\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432,\n    \u0438 \u043f\u0438\u0448\u0435\u0442 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b (\u0438 \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0437\u0430\u043f\u0440\u043e\u0441\u0430) \u0432 JSONL:\n        {\"index\": ..., \"image_name\": ..., \"prompt_text\": ..., \"response\": ..., \"latency_ms\": ..., \"ok\": ...}\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0441\u043e \u0441\u0442\u0435\u043d\u0434\u043e\u0432\u044b\u043c \u0432\u0440\u0435\u043c\u0435\u043d\u0435\u043c (wall time).\n    \"\"\"\n\n    def worker(idx, image_path, prompt, model_name):\n        last_err = None\n        for attempt in range(1, max_retries + 1):\n            start = time.perf_counter()\n            try:\n                response, usage = get_image_prediction(image_path, prompt, model_name)\n                latency_ms = (time.perf_counter() - start) * 1000.0\n                return {\n                    \"index\": idx, \n                    \"image_name\": image_path.name,\n                    \"prompt_text\": prompt,\n                    \"model_name\": model_name,\n                    \"gemma_translation\": response, \n                    \"latency_ms\": latency_ms, \n                    \"ok\": True,\n                    \"prompt_tokens\": usage.prompt_tokens if usage else 0,\n                    \"completion_tokens\": usage.completion_tokens if usage else 0,\n                    \"total_tokens\": usage.total_tokens if usage else 0,\n                }\n            except Exception as e:\n                last_err = e\n                time.sleep(retry_backoff * attempt)\n        latency_ms = (time.perf_counter() - start) * 1000.0  # \u0432\u0440\u0435\u043c\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0439 \u043f\u043e\u043f\u044b\u0442\u043a\u0438\n        return {\n            \"index\": idx, \n            \"image_name\": image_path.name,\n            \"prompt_text\": prompt,\n            \"model_name\": model_name,\n            \"error\": str(last_err),\n            \"latency_ms\": latency_ms, \n            \"ok\": False,\n            \"prompt_tokens\": 0,\n            \"completion_tokens\": 0,\n            \"total_tokens\": 0,\n        }\n\n    # Build sequence for total_requests (round-robin images &amp; prompts, alternating models)\n    sequence = []\n    for i in range(total_requests):\n        image_path = image_paths_list[i % len(image_paths_list)]\n        prompt_text = prompts_list[i % len(prompts_list)]\n        model_name = MODELS[i % len(MODELS)]  # Alternate between models\n        sequence.append((image_path, prompt_text, model_name))\n\n    futures = []\n    t0 = time.perf_counter()\n    with ThreadPoolExecutor(max_workers=max_workers) as ex, \\\n         open(out_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n\n        for idx, (image_path, prompt_text, model_name) in enumerate(sequence):\n            futures.append(ex.submit(worker, idx, image_path, prompt_text, model_name))\n\n        latencies = []\n        ok_cnt = 0\n        total_pt = 0\n        total_ct = 0\n\n        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Processing\"):\n            rec = fut.result()\n            if rec.get(\"ok\"):\n                ok_cnt += 1\n                if rec.get(\"latency_ms\") is not None:\n                    latencies.append(rec[\"latency_ms\"])\n                total_pt += int(rec.get(\"prompt_tokens\", 0))\n                total_ct += int(rec.get(\"completion_tokens\", 0))\n            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\n    total_time_s = time.perf_counter() - t0\n    eff_rps = (ok_cnt / total_time_s) if total_time_s &gt; 0 else float(\"inf\")\n    latencies.sort()\n\n    metrics = {\n        \"total_requests\": total_requests,\n        \"successful\": ok_cnt,\n        \"failed\": total_requests - ok_cnt,\n        \"total_time_s\": total_time_s,\n        \"effective_rps\": eff_rps,\n        \"latency_ms\": {\n            \"avg\": (sum(latencies) / len(latencies)) if latencies else float(\"nan\"),\n            \"min\": latencies[0] if latencies else float(\"nan\"),\n            \"max\": latencies[-1] if latencies else float(\"nan\"),\n            \"p50\": _percentile(latencies, 50) if latencies else float(\"nan\"),\n            \"p90\": _percentile(latencies, 90) if latencies else float(\"nan\"),\n            \"p99\": _percentile(latencies, 99) if latencies else float(\"nan\"),\n            \"count\": len(latencies),\n        },\n        \"max_workers\": max_workers,\n        # Actual usage from server responses\n        \"usage_sum\": {\n            \"total_input_tokens\": total_pt,\n            \"total_generated_tokens\": total_ct,\n            \"total_tokens\": total_pt + total_ct,\n            \"input_tokens_per_sec\": (total_pt / total_time_s) if total_time_s &gt; 0 else float(\"inf\"),\n            \"output_tokens_per_sec\": (total_ct / total_time_s) if total_time_s &gt; 0 else float(\"inf\"),\n            \"total_throughput_tokens_per_sec\": ((total_pt + total_ct) / total_time_s) if total_time_s &gt; 0 else float(\"inf\"),\n        },\n    }\n\n    print(f\"\\nDone. Wrote JSONL to: {out_jsonl_path}\")\n    print(f\"Total Time: {metrics['total_time_s']:.2f} seconds\")\n    print(f\"Effective RPS: {metrics['effective_rps']:.2f} req/s\\n\")\n    print(\"Request Latency (ms):\")\n    lm = metrics[\"latency_ms\"]\n    print(f\"  Avg: {lm['avg']:.1f}, Min: {lm['min']:.1f}, Max: {lm['max']:.1f}\")\n    print(f\"  P50: {lm['p50']:.1f}, P90: {lm['p90']:.1f}, P99: {lm['p99']:.1f}\")\n\n    # Print actual token usage from server responses (includes image tokens!)\n    if metrics[\"usage_sum\"][\"total_tokens\"] &gt; 0:\n        use = metrics[\"usage_sum\"]\n        print(\"\\nTokens (from API usage):\")\n        print(f\"  Total Input Tokens      : {use['total_input_tokens']}\")\n        print(f\"  Total Generated Tokens  : {use['total_generated_tokens']}\")\n        print(f\"  Input (tokens/s)        : {use['input_tokens_per_sec']:.2f}\")\n        print(f\"  Output (tokens/s)       : {use['output_tokens_per_sec']:.2f}\")\n        print(f\"  Total Throughput        : {use['total_throughput_tokens_per_sec']:.2f} tokens/s\")\n\n    return metrics\n\n# --------- NEW: \u043f\u043e\u0441\u043b\u0435 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u2014 \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u044b \u0447\u0435\u0440\u0435\u0437 /tokenize ---------\ndef summarize_tokens_with_tokenizer_api(\n    jsonl_path: str,\n    total_time_s: float,\n):\n    total_input_tokens = 0\n    total_output_tokens = 0\n    ok = 0\n    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            if not line.strip():\n                continue\n            rec = json.loads(line)\n            if not rec.get(\"ok\"):\n                continue\n            ok += 1\n            prompt_text = rec.get(\"prompt_text\") or \"\"\n            output = rec.get(\"gemma_translation\") or \"\"\n            # \u0412\u041e\u0421\u0421\u0422\u0410\u041d\u0410\u0412\u041b\u0418\u0412\u0410\u0415\u041c \u0440\u043e\u0432\u043d\u043e \u0442\u043e\u0442 \u0442\u0435\u043a\u0441\u0442-\u043f\u0440\u043e\u043c\u043f\u0442, \u0447\u0442\u043e \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u043b\u0438:\n            full_prompt = f\"{SYSTEM_PROMPT}\\n\\n{prompt_text}\"\n            total_input_tokens += _token_count(full_prompt)\n            total_output_tokens += _token_count(output)\n\n    input_tps = (total_input_tokens / total_time_s) if total_time_s &gt; 0 else float(\"inf\")\n    output_tps = (total_output_tokens / total_time_s) if total_time_s &gt; 0 else float(\"inf\")\n    total_tps = ((total_input_tokens + total_output_tokens) / total_time_s) if total_time_s &gt; 0 else float(\"inf\")\n\n    print(\"\\nTokens (via /tokenize):\")\n    print(f\"  Successful requests     : {ok}\")\n    print(f\"  Total Input Tokens      : {total_input_tokens}\")\n    print(f\"  Total Generated Tokens  : {total_output_tokens}\")\n    print(f\"  Input (tokens/s)        : {input_tps:.2f}\")\n    print(f\"  Output (tokens/s)       : {output_tps:.2f}\")\n    print(f\"  Total Throughput        : {total_tps:.2f} tokens/s\")\n\n    return {\n        \"successful\": ok,\n        \"total_input_tokens\": total_input_tokens,\n        \"total_generated_tokens\": total_output_tokens,\n        \"input_tokens_per_sec\": input_tps,\n        \"output_tokens_per_sec\": output_tps,\n        \"total_throughput_tokens_per_sec\": total_tps,\n    }\n\n# --------- Example run with IMAGE LIST ---------\nif __name__ == \"__main__\":\n    # Preload available images\n    image_files = []\n    for name in IMAGE_NAMES:\n        p = IMAGES_DIR / name\n        if p.exists():\n            image_files.append(p)\n        else:\n            print(f\"\u26a0 Skipping missing image: {p}\")\n\n    if not image_files:\n        raise RuntimeError(\"No images found to benchmark.\")\n\n    N = 100\n    OUT = \"image_eval_results.jsonl\"\n\n    metrics = evaluate_images_to_jsonl(\n        image_files, \n        IMAGE_TASKS, \n        OUT, \n        total_requests=N, \n        max_workers=128\n    )\n\n    # If server didn't provide usage tokens, fall back to tokenizer API for text-only estimation\n    if metrics[\"usage_sum\"][\"total_tokens\"] == 0:\n        print(\"\\nNo usage tokens from server, falling back to text-only tokenizer estimation:\")\n        summarize_tokens_with_tokenizer_api(\n            jsonl_path=OUT,\n            total_time_s=metrics[\"total_time_s\"],\n        )\n</code></pre>"},{"location":"vllm/benchmarks/performance_eval/#text-evaluation","title":"Text evaluation","text":"<pre><code>import json\nimport time\nimport math\nfrom concurrent.futures import ThreadPoolExecutor, as_completed\nfrom openai import OpenAI\nfrom tqdm import tqdm\nimport requests\n\n# ----- Your existing config -----\nSERVED_NAME = \"kita\"\nBASE_URL = \"http://localhost:6655/v1\"\n\nall_langs = {\n    \"kk\": \"Kazakh\",\n    \"en\": \"English\",\n    \"ru\": \"Russian\",\n}\n\ndef _api_root_from_base(base_url: str) -&gt; str:\n    return base_url.split(\"/v1\")[0] if \"/v1\" in base_url else base_url\n\n_API_ROOT = _api_root_from_base(BASE_URL)\n_TOKENIZE_ENDPOINT = f\"{_API_ROOT}/tokenize\"\n\ndef _token_count(text: str) -&gt; int:\n    try:\n        r = requests.post(_TOKENIZE_ENDPOINT, json={\"prompt\": text}, timeout=30)\n        r.raise_for_status()\n        data = r.json()\n        if \"count\" in data and isinstance(data[\"count\"], int):\n            return data[\"count\"]\n        if \"tokens\" in data and isinstance(data[\"tokens\"], list):\n            return len(data[\"tokens\"])\n    except Exception:\n        pass\n    return 0\n\ndef _make_user_prompt(input_text: str, tgt_lang: str, src_lang=None, tgt_mode='text') -&gt; str:\n    instruction = f'Translate the following text into {all_langs[tgt_lang]}.'\n    NEWLINE = '\\n'\n    if NEWLINE in input_text:\n        instruction += f' Preserve every {NEWLINE} token\u2014same count.'\n    if tgt_mode == 'speech':\n        instruction = instruction + \" Transcribe all numbers as read.\"\n    if tgt_mode == 'speech' and src_lang and src_lang == tgt_lang:\n        instruction = f\"Do not translate or change this {all_langs[src_lang]} text, only transcribe all numbers as read.\"\n    return f\"{instruction}\\n\\n{input_text}\"\n\ndef get_prediction(input_text, tgt_lang, src_lang=None, tgt_mode='text'):\n    client = OpenAI(api_key=\"empty\", base_url=BASE_URL)  # local client per call (thread-safe)\n    _input = _make_user_prompt(input_text, tgt_lang, src_lang, tgt_mode)\n    resp = client.chat.completions.create(\n        model=SERVED_NAME,\n        messages=[{\"role\": \"user\", \"content\": _input}],\n        temperature=0.05,\n        top_p=0.95,\n        max_tokens=2048,\n        frequency_penalty=0.3,\n    )\n    return resp.choices[0].message.content\n\n# ---------- NEW: \u043e\u0446\u0435\u043d\u043a\u0430 \u0441\u043a\u043e\u0440\u043e\u0441\u0442\u0438 \u043f\u043e \u0441\u043f\u0438\u0441\u043a\u0443 ----------\ndef _percentile(sorted_vals, p):\n    if not sorted_vals:\n        return float(\"nan\")\n    if len(sorted_vals) == 1:\n        return sorted_vals[0]\n    k = (len(sorted_vals) - 1) * (p / 100.0)\n    f = math.floor(k)\n    c = math.ceil(k)\n    if f == c:\n        return sorted_vals[int(k)]\n    return sorted_vals[f] * (c - k) + sorted_vals[c] * (k - f)\n\ndef evaluate_list_to_jsonl(\n    texts_list,\n    out_jsonl_path,\n    tgt_lang=\"kk\",\n    max_workers=100,\n    max_retries=3,\n    retry_backoff=0.7,\n):\n    \"\"\"\n    \u0411\u0435\u0440\u0451\u0442 \u0421\u041f\u0418\u0421\u041e\u041a \u0441\u0442\u0440\u043e\u043a `texts_list`, \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u0435\u0442 \u0434\u043e `max_workers` \u043f\u0430\u0440\u0430\u043b\u043b\u0435\u043b\u044c\u043d\u044b\u0445 \u0437\u0430\u043f\u0440\u043e\u0441\u043e\u0432,\n    \u0438 \u043f\u0438\u0448\u0435\u0442 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b (\u0438 \u0432\u0440\u0435\u043c\u044f \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0438 \u0437\u0430\u043f\u0440\u043e\u0441\u0430) \u0432 JSONL:\n        {\"index\": ..., \"original_text\": ..., \"gemma_translation\": ..., \"latency_ms\": ..., \"ok\": ...}\n    \u0412\u043e\u0437\u0432\u0440\u0430\u0449\u0430\u0435\u0442 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0441\u043e \u0441\u0442\u0435\u043d\u0434\u043e\u0432\u044b\u043c \u0432\u0440\u0435\u043c\u0435\u043d\u0435\u043c (wall time).\n    \"\"\"\n\n    def worker(idx, src_text):\n        last_err = None\n        for attempt in range(1, max_retries + 1):\n            start = time.perf_counter()\n            try:\n                tr = get_prediction(src_text, tgt_lang)\n                latency_ms = (time.perf_counter() - start) * 1000.0\n                return {\"index\": idx, \"original_text\": src_text,\n                        \"gemma_translation\": tr, \"latency_ms\": latency_ms, \"ok\": True}\n            except Exception as e:\n                last_err = e\n                time.sleep(retry_backoff * attempt)\n        latency_ms = (time.perf_counter() - start) * 1000.0  # \u0432\u0440\u0435\u043c\u044f \u043f\u043e\u0441\u043b\u0435\u0434\u043d\u0435\u0439 \u043f\u043e\u043f\u044b\u0442\u043a\u0438\n        return {\"index\": idx, \"original_text\": src_text, \"error\": str(last_err),\n                \"latency_ms\": latency_ms, \"ok\": False}\n\n    futures = []\n    t0 = time.perf_counter()\n    with ThreadPoolExecutor(max_workers=max_workers) as ex, \\\n         open(out_jsonl_path, \"w\", encoding=\"utf-8\") as fout:\n\n        for idx, src_text in enumerate(texts_list):\n            futures.append(ex.submit(worker, idx, src_text))\n\n        latencies = []\n        ok_cnt = 0\n\n        for fut in tqdm(as_completed(futures), total=len(futures), desc=\"Processing\"):\n            rec = fut.result()\n            if rec.get(\"ok\"):\n                ok_cnt += 1\n                if rec.get(\"latency_ms\") is not None:\n                    latencies.append(rec[\"latency_ms\"])\n            fout.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n\n    total_time_s = time.perf_counter() - t0\n    eff_rps = (ok_cnt / total_time_s) if total_time_s &gt; 0 else float(\"inf\")\n    latencies.sort()\n\n    metrics = {\n        \"total_requests\": len(texts_list),\n        \"successful\": ok_cnt,\n        \"failed\": len(texts_list) - ok_cnt,\n        \"total_time_s\": total_time_s,\n        \"effective_rps\": eff_rps,\n        \"latency_ms\": {\n            \"avg\": (sum(latencies) / len(latencies)) if latencies else float(\"nan\"),\n            \"min\": latencies[0] if latencies else float(\"nan\"),\n            \"max\": latencies[-1] if latencies else float(\"nan\"),\n            \"p50\": _percentile(latencies, 50) if latencies else float(\"nan\"),\n            \"p90\": _percentile(latencies, 90) if latencies else float(\"nan\"),\n            \"p99\": _percentile(latencies, 99) if latencies else float(\"nan\"),\n            \"count\": len(latencies),\n        },\n        \"max_workers\": max_workers,\n    }\n\n    print(f\"\\nDone. Wrote JSONL to: {out_jsonl_path}\")\n    print(f\"Total Time: {metrics['total_time_s']:.2f} seconds\")\n    print(f\"Effective RPS: {metrics['effective_rps']:.2f} req/s\\n\")\n    print(\"Request Latency (ms):\")\n    lm = metrics[\"latency_ms\"]\n    print(f\"  Avg: {lm['avg']:.1f}, Min: {lm['min']:.1f}, Max: {lm['max']:.1f}\")\n    print(f\"  P50: {lm['p50']:.1f}, P90: {lm['p90']:.1f}, P99: {lm['p99']:.1f}\")\n\n    return metrics\n\n# --------- NEW: \u043f\u043e\u0441\u043b\u0435 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u2014 \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u044b \u0447\u0435\u0440\u0435\u0437 /tokenize ---------\ndef summarize_tokens_with_tokenizer_api(\n    jsonl_path: str,\n    tgt_lang: str,\n    total_time_s: float,\n    src_lang=None,\n    tgt_mode='text',\n):\n    total_input_tokens = 0\n    total_output_tokens = 0\n    ok = 0\n    with open(jsonl_path, \"r\", encoding=\"utf-8\") as f:\n        for line in f:\n            if not line.strip():\n                continue\n            rec = json.loads(line)\n            if not rec.get(\"ok\"):\n                continue\n            ok += 1\n            original = rec.get(\"original_text\") or \"\"\n            output = rec.get(\"gemma_translation\") or \"\"\n            # \u0412\u041e\u0421\u0421\u0422\u0410\u041d\u0410\u0412\u041b\u0418\u0412\u0410\u0415\u041c \u0440\u043e\u0432\u043d\u043e \u0442\u043e\u0442 user-\u043f\u043e\u0434\u0441\u043a\u0430\u0437, \u0447\u0442\u043e \u043e\u0442\u043f\u0440\u0430\u0432\u043b\u044f\u043b\u0438:\n            prompt_text = _make_user_prompt(original, tgt_lang, src_lang, tgt_mode)\n            total_input_tokens += _token_count(prompt_text)\n            total_output_tokens += _token_count(output)\n\n    input_tps = (total_input_tokens / total_time_s) if total_time_s &gt; 0 else float(\"inf\")\n    output_tps = (total_output_tokens / total_time_s) if total_time_s &gt; 0 else float(\"inf\")\n    total_tps = ((total_input_tokens + total_output_tokens) / total_time_s) if total_time_s &gt; 0 else float(\"inf\")\n\n    print(\"\\nTokens (via /tokenize):\")\n    print(f\"  Successful requests     : {ok}\")\n    print(f\"  Total Input Tokens      : {total_input_tokens}\")\n    print(f\"  Total Generated Tokens  : {total_output_tokens}\")\n    print(f\"  Input (tokens/s)        : {input_tps:.2f}\")\n    print(f\"  Output (tokens/s)       : {output_tps:.2f}\")\n    print(f\"  Total Throughput        : {total_tps:.2f} tokens/s\")\n\n    return {\n        \"successful\": ok,\n        \"total_input_tokens\": total_input_tokens,\n        \"total_generated_tokens\": total_output_tokens,\n        \"input_tokens_per_sec\": input_tps,\n        \"output_tokens_per_sec\": output_tps,\n        \"total_throughput_tokens_per_sec\": total_tps,\n    }\n\n# --------- Example run with LIST ---------\nif __name__ == \"__main__\":\n    QUESTIONS = [\n        \"\u041a\u0430\u043a\u043e\u0435 \u0443 \u0442\u0435\u0431\u044f \u043b\u044e\u0431\u0438\u043c\u043e\u0435 \u0430\u043d\u0438\u043c\u0435? /no_think\",\n        \"\u0427\u0442\u043e \u0432\u0430\u0436\u043d\u0435\u0435 \u0432 \u0430\u043d\u0438\u043c\u0435: \u0441\u044e\u0436\u0435\u0442 \u0438\u043b\u0438 \u0432\u0438\u0437\u0443\u0430\u043b? /no_think\",\n        \"\u041a\u0430\u043a\u043e\u0435 \u0430\u043d\u0438\u043c\u0435 \u0442\u044b \u0431\u044b \u043f\u043e\u0440\u0435\u043a\u043e\u043c\u0435\u043d\u0434\u043e\u0432\u0430\u043b \u043d\u043e\u0432\u0438\u0447\u043a\u0443? /no_think\",\n        \"\u041a\u0430\u043a\u0430\u044f \u043e\u043f\u0435\u043d\u0438\u043d\u0433-\u043f\u0435\u0441\u043d\u044f \u0443 \u0442\u0435\u0431\u044f \u0432 \u0442\u043e\u043f\u0435? /no_think\",\n        \"\u041a\u0430\u043a\u043e\u0439 \u043f\u0435\u0440\u0441\u043e\u043d\u0430\u0436 \u0442\u0435\u0431\u044f \u0432\u0434\u043e\u0445\u043d\u043e\u0432\u043b\u044f\u0435\u0442? /no_think\",\n    ]\n\n    N = 100\n    TEXTS = (QUESTIONS * math.ceil(N / len(QUESTIONS)))[:N]\n    OUT = \"translations_from_list.jsonl\"\n\n    metrics = evaluate_list_to_jsonl(TEXTS, OUT, tgt_lang=\"kk\", max_workers=128)\n\n    # \u041f\u043e\u0441\u043b\u0435 \u0433\u0435\u043d\u0435\u0440\u0430\u0446\u0438\u0438 \u0441\u0447\u0438\u0442\u0430\u0435\u043c \u0442\u043e\u043a\u0435\u043d\u044b \u0447\u0435\u0440\u0435\u0437 /tokenize \u0438 \u043f\u0435\u0447\u0430\u0442\u0430\u0435\u043c \u0441\u0432\u043e\u0434\u043a\u0443\n    summarize_tokens_with_tokenizer_api(\n        jsonl_path=OUT,\n        tgt_lang=\"kk\",\n        total_time_s=metrics[\"total_time_s\"],\n)\n</code></pre>"}]}